<!DOCTYPE html>
<html lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="UTF-8">

    

    

    <title>AI-深度学习 | Coderss</title>
    <meta name="author" content="coder">
    <meta name="version" content="1.0.0">
    <meta name="keywords" content="">
    <meta name="description" content="深度学习相关见解资料摘录
简介概述
什么是人工智能人工智能的概念：机器模拟人的意识和思维
重要人物：艾伦·麦席森·图灵（Alan Mathison Turing）
人物简介：1912 年 6 月 23 日－1954 年 6 月 7 日，英国数学家、逻辑学家，被称为计算机科学之父，人工智能之父。
相关事件：

1950 年在论文《机器能思考吗？》中提出了图灵测试，一种用于判定机器是否具有智能的试验方法：提问者和回答者隔开，提问者通过一些装置（如键盘）向机器随意提问。多次测试，如果有超过 30%的提">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    <meta name="baidu-site-verification" content="F0CXvmUgA9">

    
    
    <link rel="icon" href="/favicon.png">
    

    <link rel="stylesheet" href="/css/style.css">
</head>
<body>

    <div class="app">
        <header class="header clearfix">
    <div id="nav" class="nav">
    <button id="open-panel" class="open-panel"><i class="icon-library"></i></button>

    <nav class="nav-inner">

        
        
        <li class="nav-item">
            <a class="nav-link" href="/">首页</a>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/categories/back-end">Java前后端</a>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/categories/cpp">Cpp嵌入式</a>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/categories/go">Go云原生</a>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/categories/cloud">Linux安全</a>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/categories/reverse">Win安全</a>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/categories/data">数据与算法</a>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/categories/work">工作相关</a>
        </li>
        
        
        
        <li class="nav-item nav-item-tag">
            <a id="nav-tag" class="nav-link" href="#">文章标签</a>
            <div id="nav-tags" class="nav-tag-wrap">
                <i class="nav-tag-arrow"></i>
                
  <div class="widget-wrap">
    <h3 class="widget-title">
        <i class="icon-tag vm"></i>
        <span class="vm">Tags</span>
    </h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/">AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Boost库/">Boost库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Collection/">Collection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cpp编程/">Cpp编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Fescar/">Fescar</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gc/">Gc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/K8s/">K8s</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MapReduce/">MapReduce</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/">Math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Net/">Net</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nosql/">Nosql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python计算库/">Python计算库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Rust/">Rust</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sharding-jdbc/">Sharding-jdbc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SkyWalking/">SkyWalking</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TensorFlow/">TensorFlow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Turi/">Turi</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Windows系统/">Windows系统</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Windows驱动/">Windows驱动</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Yarn/">Yarn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/assembly/">assembly</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/c-cpp语言/">c/cpp语言</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/debug/">debug</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/design/">design</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/">docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dubbo/">dubbo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/eth/">eth</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/go/">go</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/go-kernel/">go-kernel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/io/">io</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/juc/">juc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kubernetes/">kubernetes</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/map/">map</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mfc/">mfc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/microservice/">microservice</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mybatis/">mybatis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/netty/">netty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python-book/">python-book</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/qt/">qt</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sentinel/">sentinel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/skycoin/">skycoin</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spring/">spring</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spring-cloud/">spring-cloud</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/stl/">stl</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tomcat/">tomcat</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/x86-Windows系统总结/">x86 Windows系统总结</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/中台/">中台</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/分布式文件系统/">分布式文件系统</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/多线程编程/">多线程编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/嵌入式/">嵌入式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/架构/">架构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/消息队列/">消息队列</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/网络编程/">网络编程</a></li></ul>
    </div>
  </div>


            </div>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/archives">历史归档</a>
        </li>
        
        
        

    </nav>
</div>

    <aside id="aside" class="aside">
    <div id="aside-mask" class="aside-mask"></div>
    <div id="aside-inner" class="aside-inner">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit"><i class="icon-search-stroke"></i></button><input type="hidden" name="sitesearch" value="http://www.coderss.cn"></form>

        
        
        
        
        

        
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#简介"><span class="toc-number">1.</span> <span class="toc-text">简介</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#概述"><span class="toc-number">1.1.</span> <span class="toc-text">概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#什么是人工智能"><span class="toc-number">1.1.1.</span> <span class="toc-text">什么是人工智能</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#什么是机器学习"><span class="toc-number">1.1.2.</span> <span class="toc-text">什么是机器学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#什么是深度学习"><span class="toc-number">1.1.3.</span> <span class="toc-text">什么是深度学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#人工智能-Vs-机器学习-Vs-深度学习"><span class="toc-number">1.1.4.</span> <span class="toc-text">人工智能 Vs 机器学习 Vs 深度学习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络的发展历史（三起两落）"><span class="toc-number">1.2.</span> <span class="toc-text">神经网络的发展历史（三起两落）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#机器学习的典型应用"><span class="toc-number">1.3.</span> <span class="toc-text">机器学习的典型应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#应用领域"><span class="toc-number">1.3.1.</span> <span class="toc-text">应用领域</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#主流应用"><span class="toc-number">1.3.2.</span> <span class="toc-text">主流应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">1.4.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Python及Shell"><span class="toc-number">2.</span> <span class="toc-text">Python及Shell</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Linux指令、Hello-World"><span class="toc-number">2.1.</span> <span class="toc-text">Linux指令、Hello World</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#常用指令"><span class="toc-number">2.1.1.</span> <span class="toc-text">常用指令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#常用基础语法点"><span class="toc-number">2.1.2.</span> <span class="toc-text">常用基础语法点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#列表、元组、字典"><span class="toc-number">2.2.</span> <span class="toc-text">列表、元组、字典</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#列表"><span class="toc-number">2.2.1.</span> <span class="toc-text">列表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#元组"><span class="toc-number">2.2.2.</span> <span class="toc-text">元组</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#字典"><span class="toc-number">2.2.3.</span> <span class="toc-text">字典</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#条件语句"><span class="toc-number">2.3.</span> <span class="toc-text">条件语句</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#vim-编辑器"><span class="toc-number">2.3.1.</span> <span class="toc-text">vim 编辑器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#条件语句-1"><span class="toc-number">2.3.2.</span> <span class="toc-text">条件语句</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#python-语句代码层次"><span class="toc-number">2.3.3.</span> <span class="toc-text">python 语句代码层次</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#逻辑关系"><span class="toc-number">2.3.4.</span> <span class="toc-text">逻辑关系</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#循环语句"><span class="toc-number">2.4.</span> <span class="toc-text">循环语句</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#turtle-模块"><span class="toc-number">2.4.1.</span> <span class="toc-text">turtle 模块</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#函数、模块、包"><span class="toc-number">2.5.</span> <span class="toc-text">函数、模块、包</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#函数"><span class="toc-number">2.5.1.</span> <span class="toc-text">函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模块"><span class="toc-number">2.5.2.</span> <span class="toc-text">模块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#三、包"><span class="toc-number">2.5.3.</span> <span class="toc-text">三、包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#四、变量作用域"><span class="toc-number">2.5.4.</span> <span class="toc-text">四、变量作用域</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#类、对象、面向对象的编程"><span class="toc-number">2.6.</span> <span class="toc-text">类、对象、面向对象的编程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#文件操作"><span class="toc-number">2.7.</span> <span class="toc-text">文件操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#文件写操作"><span class="toc-number">2.7.1.</span> <span class="toc-text">文件写操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#文件读操作"><span class="toc-number">2.7.2.</span> <span class="toc-text">文件读操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#总结-1"><span class="toc-number">2.7.3.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#搭建神经网络，总结搭建八股"><span class="toc-number">3.</span> <span class="toc-text">搭建神经网络，总结搭建八股</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#张量、计算图、会话"><span class="toc-number">3.1.</span> <span class="toc-text">张量、计算图、会话</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基本概念"><span class="toc-number">3.1.1.</span> <span class="toc-text">基本概念</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#前向传播"><span class="toc-number">3.2.</span> <span class="toc-text">前向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#神经网络的参数"><span class="toc-number">3.2.1.</span> <span class="toc-text">神经网络的参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#神经网络的搭建"><span class="toc-number">3.2.2.</span> <span class="toc-text">神经网络的搭建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#前向传播-1"><span class="toc-number">3.2.3.</span> <span class="toc-text">前向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#√-推导"><span class="toc-number">3.2.3.1.</span> <span class="toc-text">√ 推导</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#第一层"><span class="toc-number">3.2.3.1.1.</span> <span class="toc-text">第一层</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#第二层"><span class="toc-number">3.2.3.1.2.</span> <span class="toc-text">第二层</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#反向传播"><span class="toc-number">3.3.</span> <span class="toc-text">反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#反向传播-1"><span class="toc-number">3.3.1.</span> <span class="toc-text">反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#进阶：反向传播参数更新推导过程"><span class="toc-number">3.3.1.1.</span> <span class="toc-text">进阶：反向传播参数更新推导过程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#搭建神经网络的八股"><span class="toc-number">3.3.2.</span> <span class="toc-text">搭建神经网络的八股</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#神经网络优化"><span class="toc-number">4.</span> <span class="toc-text">神经网络优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#损失函数"><span class="toc-number">4.1.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#学习率"><span class="toc-number">4.2.</span> <span class="toc-text">学习率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#滑动平均"><span class="toc-number">4.3.</span> <span class="toc-text">滑动平均</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#正则化"><span class="toc-number">4.4.</span> <span class="toc-text">正则化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#搭建模块化神经网络八股"><span class="toc-number">4.5.</span> <span class="toc-text">搭建模块化神经网络八股</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#搭建神经网络-mnist-数据集上训练模型-输出手写数字识别准确率"><span class="toc-number">5.</span> <span class="toc-text">搭建神经网络(mnist 数据集上训练模型,输出手写数字识别准确率)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#MNIST数据集"><span class="toc-number">5.1.</span> <span class="toc-text">MNIST数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模块化搭建神经网络"><span class="toc-number">5.2.</span> <span class="toc-text">模块化搭建神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#手写数字识别准确率输出"><span class="toc-number">5.3.</span> <span class="toc-text">手写数字识别准确率输出</span></a></li></ol></li></ol>
        
    </div>
</aside>

</header>

        <div id="content" class="content"><article class="article" itemscope="" itemprop="blogPost">
    
    <header class="article-header">
        
        <h1 itemprop="name">
           AI-深度学习
        </h1>
        
        <div class="article-meta clearfix">
            <a class="article-date" href="/2018/01/06/ai-dl/">
    
    <i class="icon-calendar"></i>
    
    <time datetime="2018-01-06T01:50:37.000Z" itemprop="datePublished">2018-01-06</time>
</a>

            
<div class="article-tag-list">
    <i class="icon-tag"></i>
    <a class="article-tag-link" href="/tags/TensorFlow/">TensorFlow</a>
</div>


        </div>
    </header>
    
    <section class="article-body markdown-body">
        
        <p>深度学习相关见解资料摘录<br><a id="more"></a></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p><img src="/2018/01/06/ai-dl/image-110.png" width="700px"></p>
<h3 id="什么是人工智能"><a href="#什么是人工智能" class="headerlink" title="什么是人工智能"></a>什么是人工智能</h3><p><strong>人工智能的概念</strong>：机器模拟人的意识和思维</p>
<p><strong>重要人物</strong>：艾伦·麦席森·图灵（Alan Mathison Turing）</p>
<p><strong>人物简介</strong>：1912 年 6 月 23 日－1954 年 6 月 7 日，英国数学家、逻辑学家，被称为计算机科学之父，人工智能之父。</p>
<p><strong>相关事件</strong>：</p>
<ol>
<li>1950 年在论文《机器能思考吗？》中提出了图灵测试，一种用于判定机器是否具有智能的试验方法：提问者和回答者隔开，提问者通过一些装置（如键盘）向机器随意提问。多次测试，如果有超过 30%的提问者认为回答问题的是人而不是机器，那么这台机器就通过测试，具有了人工智能。也就是工智能的概念：“用机器模拟人的意识和思维”。  </li>
<li>图灵在论文中预测：在 2000 年，会出现通过图灵测试具备人工智能的机器。然而直到 2014 年 6 月，英国雷丁大学的聊天程序才成功冒充了 13 岁男孩，通过了图灵测试。这一事件比图灵的预测晚了 14 年。  </li>
<li>在 2015 年 11 月 science 杂志封面新闻报道，机器人已经可以依据从未见过的文字中的一个字符，写出同样风格的字符，说明机器已经具备了迅速学习陌生文字的创造能力。</li>
</ol>
<p><strong>消费级人工智能产品</strong>：</p>
<ul>
<li>国外<ol>
<li>谷歌 Assistant</li>
<li>微软 Cortana</li>
<li>苹果 Siri</li>
<li>亚马逊 Alexa</li>
</ol>
</li>
<li>国内<ol>
<li>阿里的天猫精灵</li>
<li>小米的小爱同学</li>
</ol>
</li>
</ul>
<p><strong>人工智能先锋</strong>：</p>
<ol>
<li>Geoffrey Hinton：多伦多大学的教授，谷歌大脑多伦多分布负责人，是人工智能领域的鼻祖，他发表了许多让神经网络得以应用的论文，激活了整个人工智能领域。他还培养了许多人工智能的大家。比如 LeCun 就是他的博士后。</li>
<li>Yann LeCun：纽约大学的教授，Facebook 人工智能研究室负责人，他改进了卷积神经网路算法，使卷积神经网络具有了工程应用价值，现在卷积神经网络依旧是计算机视觉领域最有效的模型之一。</li>
<li>Yoshua Bengio：蒙特利尔大学的教授，现任微软公司战略顾问，他推动了循环神经网路算法的发展，使循环神经网络得到工程应用，用循环神经网络解决了自然语言处理中的问题。</li>
</ol>
<h3 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a>什么是机器学习</h3><p><strong>机器学习的概念</strong>：机器学习是一种统计学方法，计算机利用已有数据得出某种模型，再利用此模型预测结果。</p>
<p><img src="/2018/01/06/ai-dl/Image_001.png" alt=""></p>
<p><strong>特点</strong>：随经验的增加，效果会变好。 </p>
<p><strong>简单模型举例</strong>：决策树模型</p>
<p>预测班车到达时间问题描述： 每天早上七点半，班车从 A 地发往 B 地，到达 B地的时间如何准确预测？</p>
<p>如果你第一次乘坐班车，你的预测通常不太准。<br>一周之后，你大概能预测出班车 8:00 左右到达 B 地；<br>一个月之后，随着经验的增加，你还会知道，周一常堵车，会晚 10 分钟，下雨常堵车，会晚 20 分钟。<br>于是你画出了如下的一张树状图，<br>如果是周一，还下了雨，班车会 8:30 到达；<br>如果不是周一，也没有下雨，班车会 8:00 到达。</p>
<p><img src="/2018/01/06/ai-dl/Image_002.png" alt=""></p>
<p><strong>机器学习和传统计算机运算的区别</strong>：传统计算机是基于冯诺依曼结构，指令预先存储。运行时，CPU 从存储器里逐行读取指令，按部就班逐行执行预先安排好的<br>指令。其特点是，输出结果确定，因为先干什么，后干什么都已经提前写在指令里了。</p>
<p><img src="/2018/01/06/ai-dl/Image_003.png" alt=""></p>
<p><strong>机器学习三要素</strong>：数据、算法、算力</p>
<p><img src="/2018/01/06/ai-dl/Image_004.png" alt=""></p>
<h3 id="什么是深度学习"><a href="#什么是深度学习" class="headerlink" title="什么是深度学习"></a>什么是深度学习</h3><p><strong>深度学习的概念</strong>：深层次神经网络，源于对生物脑神经元结构的研究。 </p>
<p><strong>人脑神经网络</strong>：随着人的成长，脑神经网络是在渐渐变粗变壮。</p>
<p><img src="/2018/01/06/ai-dl/Image_005.jpg" alt=""></p>
<p><strong>生物学中的神经元</strong>：<br>下图左侧有许多支流汇总在一起，生物学中称这些支流叫做树突。树突具有接受刺激并将冲动传入细胞体的功能，是神经元的输入。这些树突汇总于细胞核又沿着一条轴突输出。<br>轴突的主要功能是将神经冲动由胞体传至其他神经元，是神经元的输出。人脑便是由 860 亿个这样的神经元组成，所有的思维意识，都以它为基本单元，连接成网络实现的。</p>
<p><img src="/2018/01/06/ai-dl/Image_006.jpg" alt=""></p>
<p><strong>计算机中的神经元模型</strong>：<br>1943 年，心理学家 McCulloch 和数学家 Pitts 参考了生物神经元的结构，发表了抽象的神经元模型 MP。神经元模型是一个包含输入，输出与计算功能的模型。输入可以类比为神经元的树突，输出可以类比为神经元的轴突，计算可以类比为细胞核。</p>
<p><img src="/2018/01/06/ai-dl/Image_007.jpg" alt=""></p>
<h3 id="人工智能-Vs-机器学习-Vs-深度学习"><a href="#人工智能-Vs-机器学习-Vs-深度学习" class="headerlink" title="人工智能 Vs 机器学习 Vs 深度学习"></a>人工智能 Vs 机器学习 Vs 深度学习</h3><p>人工智能，就是用机器模拟人的意识和思维。</p>
<p>机器学习，则是实现人工智能的一种方法，是人工智能的子集。</p>
<p>深度学习就是深层次神经网络，是机器学习的一种实现方法，是机器学习的子集。</p>
<p><img src="/2018/01/06/ai-dl/Image_008.png" alt=""></p>
<h2 id="神经网络的发展历史（三起两落）"><a href="#神经网络的发展历史（三起两落）" class="headerlink" title="神经网络的发展历史（三起两落）"></a>神经网络的发展历史（三起两落）</h2><p><img src="/2018/01/06/ai-dl/Image_009.jpg" alt=""></p>
<p><strong>第一次兴起</strong>：1958 年，人们把两层神经元首尾相接，组成单层神经网络，称做感知机。感知机成了首个可以学习的人工神经网络。引发了神经网络研究的第一次兴起。</p>
<p><strong>第一次寒冬</strong>：1969 年，这个领域的权威学者 Minsky 用数学公式证明了只有单层神经网络的感知机无法对异或逻辑进行分类，Minsky 还指出要想解决异或可分问题，需要把单层神经网络扩展到两层或者以上。然而在那个年代计算机的运算能力，是无法支撑这种运算量的。只有一层计算单元的感知机，暴露出他的天然缺陷，使得神经网络研究进入了第一个寒冬。</p>
<p><strong>第二次兴起</strong>：1986 年，Hinton 等人提出了反向传播方法，有效解决了两层神经网络的算力问题。引发了神经网络研究的第二次兴起。</p>
<p><strong>第二次寒冬</strong>：1995 年，支持向量机诞生。支持向量机可以免去神经网络需要调节参数的不足，还避免了神经网络中局部最优的问题。一举击败神经网络，成为当时人工智能领域的主流算法，使得神经网络进入了他的第二个冬季。</p>
<p><strong>第三次兴起</strong>：2006 年，深层次神经网络出现，2012 年，卷积神经网络在图像识别领域中的惊人表现，又引发了神经网络研究的再一次兴起。</p>
<h2 id="机器学习的典型应用"><a href="#机器学习的典型应用" class="headerlink" title="机器学习的典型应用"></a>机器学习的典型应用</h2><h3 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h3><p>计算机视觉、语音识别、自然语言处理</p>
<h3 id="主流应用"><a href="#主流应用" class="headerlink" title="主流应用"></a>主流应用</h3><p>（1） <strong>预测</strong>（对连续数据进行预测）</p>
<p>如，预测某小区 100 平米的房价卖多少钱。<br>根据以往数据（红色●），拟合出一条线，让它“穿过”所有的点，并且与各个点的距离尽可能的小。</p>
<p><img src="/2018/01/06/ai-dl/Image_010.png" alt=""></p>
<p>我们可以把以前的数据，输入神经网络，让他训练出一个模型，比如这张图中红色点表示了以往的数据，虚线表示了预测出的模型 Y = ax + b ，大量历史数据也就是面积 x 和房价 y 作为输入，训练出了模型的参数 a = 3.5, b = 150，则你家 100 平米的房价应该是 3.5 * 100 + 150 = 500 万。</p>
<p>我们发现，模型不一定全是直线，也可以是曲线；我们还发现，随着数据的增多，模型一般会更准确。</p>
<p>（2） <strong>分类</strong>（对离散数据进行分类）</p>
<p>如，根据肿瘤患者的年龄和肿瘤大小判断良性、恶性。<br> 红色样本为恶性，蓝色样本为良性，绿色分为哪类？</p>
<p><img src="/2018/01/06/ai-dl/Image_011.png" alt=""></p>
<p>假如让计算机判断肿瘤是良性还是恶性，先要把历史数据输入到神经网络进行建模，调节模型的参数，得到一条线把良性肿瘤和恶性肿瘤分开。比如输入患者的年龄、肿瘤的大小 还有对应的良性肿瘤还是恶性肿瘤，使用神经网络训练模型 调整参数，再输入新的患者年龄和肿瘤大小时，计算机会直接告诉你肿瘤是良性还是恶性。比如上图的绿色三角就属于良性肿瘤。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li>机器学习，就是在任务 T 上，随经验 E 的增加，效果 P 随之增加。</li>
<li>机器学习的过程是通过大量数据的输入，生成一个模型，再利用这个生成的模型，实现对结果的预测。 </li>
<li>庞大的神经网络是基于神经元结构的，是输入乘以权重，再求和，再过非线性函数的过程。</li>
</ol>
<p><br><br><br></p>
<h1 id="Python及Shell"><a href="#Python及Shell" class="headerlink" title="Python及Shell"></a>Python及Shell</h1><h2 id="Linux指令、Hello-World"><a href="#Linux指令、Hello-World" class="headerlink" title="Linux指令、Hello World"></a>Linux指令、Hello World</h2><h3 id="常用指令"><a href="#常用指令" class="headerlink" title="常用指令"></a>常用指令</h3><ul>
<li>√ 桌面点击右键 &gt; 选择 <code>Open Terminal</code>：<strong>打开终端</strong></li>
<li>√ <code>pwd</code>：<strong>打印当前在哪个目录</strong></li>
<li>√ <code>ls</code>：<strong>列出当前路径下的文件和目录</strong></li>
<li>√ <code>mkdir 目录名</code>：<strong>新建目录</strong></li>
<li>√ <code>cd 目录名</code>：<strong>进到指定目录</strong></li>
<li>√ <code>python</code>：<strong>运行 Python 解释器</strong></li>
<li>√ <code>print &quot;Hello World&quot;</code></li>
</ul>
<p>代码验证：</p>
<p><img src="/2018/01/06/ai-dl/Image_001.jpg" alt=""></p>
<p>补充：</p>
<p>使用 <code>pwd</code> 命令打印当前在哪个目录，打印的是绝对路径。</p>
<p>绝对路径：是以根目录(<code>/</code>)为起点的完整路径，以你所要到的目录为终点。</p>
<p>相对路径：是你当前的目录(<code>.</code>)为起点的路径，以你所要到的目录为终点。</p>
<p>使用 cd 目录名 进到指定目录，如果指定的”目录名”是</p>
<ul>
<li><code>.</code> 表示当前目录</li>
<li><code>..</code> 表示当前目录的上一级目录</li>
<li><code>-</code> 表示上一次所在目录</li>
<li><code>~</code> 表示当前用户的 home 目录(即刚 login 时所在的目录) </li>
</ul>
<p>比如：</p>
<ul>
<li><code>cd ..</code> 返回上级目录 <code>cd ../..</code> 返回上两级目录</li>
<li><code>cd ~</code> 进入用户主目录 home 目录</li>
</ul>
<h3 id="常用基础语法点"><a href="#常用基础语法点" class="headerlink" title="常用基础语法点"></a>常用基础语法点</h3><ul>
<li>√ <strong>运算符</strong>： <code>+ - * / %</code></li>
<li><p>√ <strong>运算顺序：先乘除、再加减、括号最优先</strong><br>代码验证：<br><img src="/2018/01/06/ai-dl/Image_002.jpg" alt=""></p>
</li>
<li><p>√ <strong>变量</strong>，就是一个标签，由非数字开头的字母、数字、下划线组成，它的内容可以是数值、字符串、列表、元组和字典。</p>
</li>
<li>√ <strong>数值</strong>，就是数字。 <code>a = 100</code></li>
<li>√ <strong>字符串</strong>，就是用一对儿双引号、或单引号引起来的内容，只要被引号引起来，就是字符串了。 <code>b = &quot;Hello World&quot;</code><br><code>100</code> 是数值 Vs <code>&quot;100&quot;</code> 是字符串。</li>
<li>√ <strong>转义字符</strong>： <ul>
<li><code>\t</code> 表示 tab</li>
<li><code>\n</code> 表示换行</li>
<li><code>\&quot;</code> 表示 <code>&quot;</code></li>
</ul>
</li>
<li><p>√ <code>%s</code> <strong>占位</strong>，用 <code>%</code> 后的变量替换<br>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="number">100</span></span><br><span class="line">b = <span class="string">" Hello World "</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">" point = %s \n \" %s \" "</span> % (a, b)</span><br></pre></td></tr></table></figure>
<p>打印出:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">point = <span class="number">100</span></span><br><span class="line"><span class="string">"  Hello World  "</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="列表、元组、字典"><a href="#列表、元组、字典" class="headerlink" title="列表、元组、字典"></a>列表、元组、字典</h2><h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><ul>
<li><p>√ <strong>列表</strong> <code>[ ]</code> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]</span><br><span class="line">d = [<span class="string">"张三"</span>,<span class="string">"李四"</span>,<span class="string">"王五"</span>]</span><br><span class="line">e = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="string">"4"</span>,<span class="string">"5"</span>,d]</span><br></pre></td></tr></table></figure>
</li>
<li><p>√ 用列表名 <code>[ 索引号 ]</code> <strong>索引</strong>列表中的元素<br><code>d[0]</code> 表示列表 <code>d</code> 中的第零个元素 <code>&quot;张三&quot;</code></p>
</li>
<li>√ 用列表名 <code>[起 : 止]</code> 表示<strong>切片</strong>，从列表中切出相应的元素 前闭后开<ul>
<li><code>c[0:2]</code> 切出 <code>[1,2]</code></li>
<li><code>c[ : ]</code> 切出 <code>[1,2,3,4,5,6,7]</code></li>
</ul>
</li>
<li>√ 用列表名 <code>[起 : 止: 步长]</code> <strong>带步长的切片</strong>，步长有方向。 <ul>
<li><code>c = [1,2,3,4,5,6,7]</code></li>
<li>切出 <code>[5,4,3,2]</code> 用 <code>c[4:0:-1]</code> </li>
<li>切出 <code>[5,4,3,2,1]</code> 用 <code>c[4::-1]</code> </li>
<li>切出 <code>[6,4,2]</code> 用 <code>c[-2::-2]</code> 从倒数第二个开始一直切到头，步长 <code>-2</code></li>
</ul>
</li>
<li>√ <strong>修改</strong>：<code>列表名[ 索引号 ] = 新值</code></li>
<li>√ <strong>删除</strong>：<code>del 列表名[ 索引号 ]</code></li>
<li>√ <strong>插入</strong>：<code>列表名.insert()插入位置索引号,新元素)</code></li>
</ul>
<p>代码验证：</p>
<p><img src="/2018/01/06/ai-dl/Image_013.jpg" alt=""></p>
<h3 id="元组"><a href="#元组" class="headerlink" title="元组"></a>元组</h3><ul>
<li>√ <strong>元组</strong> <code>( )</code> 誓言，一旦定义不能改变 <code>f=(1,2,3)</code></li>
</ul>
<h3 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h3><ul>
<li>√ <strong>字典</strong> <code>{ }</code></li>
<li>√ 字典里放着 <code>{键:值, 键:值, 键:值}</code> n 个<strong>键值对</strong><br><code>dic={1:&quot;123&quot;,&quot;name&quot;:&quot;zhangsan&quot;,&quot;height&quot;:180}</code></li>
<li>√ 用字典名 <code>[ 键 ]</code> <strong>索引</strong>字典中的值<br><code>dic[&quot;name&quot;]</code> 表示字典 <code>dic</code> 中键 <code>&quot;name&quot;</code> 对应的值 <code>&quot;zhangsan&quot;</code></li>
<li>√ <strong>修改</strong>：<code>字典名[ 键 ] = 新值</code></li>
<li>√ <strong>删除</strong>：<code>del 字典名[ 键 ]</code></li>
<li>√ <strong>插入</strong>：<code>字典名[ 新键 ] = 新值</code> </li>
</ul>
<p>代码验证：</p>
<p><img src="/2018/01/06/ai-dl/Image_014.jpg" alt=""></p>
<h2 id="条件语句"><a href="#条件语句" class="headerlink" title="条件语句"></a>条件语句</h2><h3 id="vim-编辑器"><a href="#vim-编辑器" class="headerlink" title="vim 编辑器"></a>vim 编辑器</h3><ul>
<li>√ <code>vim 文件名</code> <strong>打开或新建文本</strong></li>
<li>√ 在 vim 中 点击 <code>i</code> 进入<strong>插入模式</strong> 可往文本里写内容</li>
<li>√ <code>ESC</code> + <code>:q</code> <strong>退出 vim</strong></li>
<li>√ <code>ESC</code> + <code>:wq</code> <strong>保存更改退出 vim</strong></li>
<li>√ <code>ESC</code> + <code>:q!</code> <strong>不保存更改退出 vim</strong></li>
</ul>
<h3 id="条件语句-1"><a href="#条件语句-1" class="headerlink" title="条件语句"></a>条件语句</h3><ul>
<li><p>√ 1、</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> 条件成立:</span><br><span class="line">    执行任务</span><br></pre></td></tr></table></figure>
</li>
<li><p>√ 2、</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> 条件 <span class="number">1</span> 成立:</span><br><span class="line">    执行任务 <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    执行任务 <span class="number">2</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>√ 3、</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> 条件 <span class="number">1</span> 成立:</span><br><span class="line">    执行任务 <span class="number">1</span></span><br><span class="line"><span class="keyword">elif</span> 条件 <span class="number">2</span> 成立:</span><br><span class="line">    执行任务 <span class="number">2</span></span><br><span class="line">        ┊</span><br><span class="line"><span class="keyword">elif</span> 条件 n 成立:</span><br><span class="line">    执行任务 n</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    执行任务 n+<span class="number">1</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>代码验证：</p>
<p><img src="/2018/01/06/ai-dl/Image_018.jpg" alt=""><br><img src="/2018/01/06/ai-dl/Image_019.jpg" alt=""></p>
<p>其中 <code>#coding:utf-8</code> 以注释的形式加入来兼容中文输入；</p>
<p><code>age=input(&quot;输入你的年龄\n&quot;)</code> 中的<code>input()</code> 是一个函数，表示从屏幕接收内容括号里的字符串是向屏幕打印出的提示内容，可以增加程序和用户的交互。 </p>
<h3 id="python-语句代码层次"><a href="#python-语句代码层次" class="headerlink" title="python 语句代码层次"></a>python 语句代码层次</h3><p><img src="/2018/01/06/ai-dl/Image_020.png" alt=""></p>
<p>Python 代码是使用四个空格的缩进表示层次关系的，从缩进我们可以看出这段条件语句分了三个层次，第一个层次是黄色的 block1，然后是红色的 block2,最后是蓝色的 block3。</p>
<h3 id="逻辑关系"><a href="#逻辑关系" class="headerlink" title="逻辑关系"></a>逻辑关系</h3><ul>
<li><code>==</code> <strong>等于</strong></li>
<li><code>!=</code> <strong>不等于</strong></li>
<li><code>&gt;</code> <strong>大于</strong></li>
<li><code>&gt;=</code> <strong>大于等于</strong></li>
<li><code>&lt;</code> <strong>小于</strong></li>
<li><code>&lt;=</code> <strong>小于等于</strong></li>
<li><code>and</code> <strong>与</strong></li>
<li><code>or</code> <strong>或</strong></li>
</ul>
<p><img src="/2018/01/06/ai-dl/Image_021.jpg" alt=""></p>
<h2 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h2><p><strong>循环语句</strong></p>
<ul>
<li><p>√ 1、</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> 变量 <span class="keyword">in</span> range(开始值, 结束值): </span><br><span class="line">    执行某些任务</span><br></pre></td></tr></table></figure>
<p>其中的括号内的开始、结束值也为前闭后开区间 代码验证：<br><img src="/2018/01/06/ai-dl/Image_022.jpg" alt=""></p>
</li>
<li><p>√ 2、</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> 变量 <span class="keyword">in</span> 列表名: </span><br><span class="line">    执行某些任务</span><br></pre></td></tr></table></figure>
<p>代码验证：<br><img src="/2018/01/06/ai-dl/Image_023.jpg" alt=""></p>
<p>在第一个 for 循环中，<br>先打印出 i 对应的 abcd 中的 a，<br>然后执行第二个 for 循环，打印出 j 对应的 abcd；<br>再回到 i，然后打印出 i 对应的 abcd 中的 b，<br>再打印出第二个 for 循环 j 对应的 abcd<br>…</p>
</li>
<li><p>√ 3、</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> 条件:</span><br><span class="line">  执行某些任务</span><br></pre></td></tr></table></figure>
<p>代码验证：<br><img src="/2018/01/06/ai-dl/Image_024.jpg" alt=""></p>
</li>
<li><p>√ 4、终止循环用 <code>break</code></p>
</li>
</ul>
<h3 id="turtle-模块"><a href="#turtle-模块" class="headerlink" title="turtle 模块"></a><code>turtle</code> 模块</h3><ul>
<li>√ <code>turtle</code> 模块</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> turtle       <span class="comment"># 导入 turtle 模块</span></span><br><span class="line"></span><br><span class="line">t = turtle.Pen( )   <span class="comment"># 用 turtle 模块中的 Pen 类，实例化出一个叫做 t 的对象</span></span><br><span class="line">t.forward(像素点)    <span class="comment"># 让 t 向前走多少个像素点</span></span><br><span class="line">t.backward(像素点)   <span class="comment"># 让 t 向前走多少个像素点 </span></span><br><span class="line">t.left(角度)         <span class="comment"># 让 t 左转多少角度 </span></span><br><span class="line">t.right(角度)        <span class="comment"># 让 t 右转多少角度 </span></span><br><span class="line">t.reset( )           <span class="comment"># 让 t 复位</span></span><br></pre></td></tr></table></figure>
<p>代码验证：</p>
<p><img src="/2018/01/06/ai-dl/Image_025.jpg" alt=""></p>
<p>可以把刚才的重复工作用循环表示出来，<code>for</code> 循环一般用作循环次数已知的任务<br>代码验证：</p>
<p><img src="/2018/01/06/ai-dl/Image_026.jpg" alt=""></p>
<p>用 <code>while</code> 循环复现刚才的工作，<code>t.reset()</code> 先让海龟复位，<br>为了防止程序死循环我们用 <code>i</code> 做个计数器，到了指定次数强制退出循环。<br>给 <code>i</code> 赋初值 <code>0</code>，做个计数器让它每运行一遍循环自加一，把 <code>i=i+1</code> 放到和 <code>t.forward</code> 和 <code>t.left</code> 一个层次，如果 <code>i=4</code> 要执行 <code>break</code> 操作，也就是停止循环。</p>
<p>代码验证：</p>
<p><img src="/2018/01/06/ai-dl/Image_027.jpg" alt=""></p>
<h2 id="函数、模块、包"><a href="#函数、模块、包" class="headerlink" title="函数、模块、包"></a>函数、模块、包</h2><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><ul>
<li><strong>√ 1、函数(function)：组织好的，可重复使用的，用来实现单一或相关联功能的代码段。</strong><br>比如之前用过的 <code>input()</code> 就是一个函数,可以直接使用函数，不用每次重新定义如何从控制台接收输入，所以函数是帮助实现代码复用的。</li>
<li><p><strong>√ 2、定义函数：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> 函数名<span class="params">(参数表)</span>:</span>  </span><br><span class="line">    函数体</span><br></pre></td></tr></table></figure>
<p>括号里的参数如果不需要可以为空</p>
</li>
<li><p><strong>√ 3、使用函数：</strong><code>函数名(参数表)</code><br>如：<code>input(&quot;please input your class number:&quot;)</code><br>定义：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hi_name</span><span class="params">(yourname)</span>:</span>  </span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Hello %s"</span> %yourname</span><br></pre></td></tr></table></figure>
<p>使用：<code>hi_name(&quot;zhangsan&quot;)</code><br>会输出：<code>Hello zhangsan</code><br>代码验证：<br><img src="/2018/01/06/ai-dl/Image_028.jpg" alt=""></p>
</li>
<li><p><strong>√ 4、函数返回值：</strong><code>return</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(a,b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> a+b</span><br><span class="line"></span><br><span class="line">c=add(a,b) <span class="comment"># c 被赋值为 add 的返回值 11</span></span><br></pre></td></tr></table></figure>
<p>代码验证：<br><img src="/2018/01/06/ai-dl/Image_029.jpg" alt=""></p>
</li>
<li><strong>√ 5、内建函数：python 解释器自带的函数</strong><br><code>abs(-10)</code> 返回 10</li>
</ul>
<h3 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h3><ul>
<li><p><strong>√ 模块(module)：是一个 Python 文件，以 <code>.py</code> 结尾，包含了 Python 函数等语句。先导入，再使用，用 <code>模块.函数名</code> 调用。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time </span><br><span class="line">time.asctime()</span><br></pre></td></tr></table></figure>
<p>输出：<code>&#39;Tue Jan 16 21:51:06 2018&#39;</code></p>
</li>
</ul>
<h3 id="三、包"><a href="#三、包" class="headerlink" title="三、包"></a>三、包</h3><ul>
<li><strong>√ 包：包含有多个模块</strong><br><code>from PIL import Image</code></li>
</ul>
<h3 id="四、变量作用域"><a href="#四、变量作用域" class="headerlink" title="四、变量作用域"></a>四、变量作用域</h3><p>局部变量：在函数中定义的变量，只在函数中存在，函数执行结束不可再用。 </p>
<p>全局变量，在函数前定义的变量，一般在整个代码最前面定义，全局可用。</p>
<h2 id="类、对象、面向对象的编程"><a href="#类、对象、面向对象的编程" class="headerlink" title="类、对象、面向对象的编程"></a>类、对象、面向对象的编程</h2><p><strong>类、对象和面向对象的编程</strong></p>
<ul>
<li><strong>√1、类(class)</strong>：用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每个对象所共有的属性和方法。对象是类的实例。物以类聚人以群分，类是可实例化出对象的模具。</li>
<li><strong>√2、实例化</strong>：<code>对象 = 类()</code><br><code>t = turtle.Pen()</code></li>
<li><strong>√3、对象</strong>：是类实例化出的实体，对象实实在在存在，完成具体工作。</li>
<li><p><strong>√4、面向对象</strong>：程序员反复修改优化类，类实例化出对象，对象调用类里的函数执行具体的操作。  </p>
<p><img src="/2018/01/06/ai-dl/Image_030.png" alt=""></p>
<p>在上图中，有动物、哺乳动物和猫。 动物是一个类，他们具有共同的功能：呼吸、移动和吃东西。哺乳动物也是一个类，他们是动物类里的子类，是在动物类的基础上多了喂奶的功能。猫是哺乳动物类的一个子类，猫类在哺乳动物的基础上多了捉老鼠的功能。  </p>
<p>类命名时第一个字母常大写，比如 Animals、Mammals 和 Cats 的首字母都大写了。这些类的右侧列出了每个类具有的功能：比如呼吸、移动和吃东西是动物这个类具备的功能，在计算机中用函数的形式表示。喂奶是哺乳动物的功能，是哺乳动物这个类里的函数。捉老鼠是猫类具有的功能，是猫类的函数。  </p>
<ul>
<li><strong>√ 上面的类是下面类的父类；下面类是上面类的子类</strong></li>
<li><strong>√ 子类实例化出来的对象，可以使用自身和父类的函数与变量</strong></li>
</ul>
</li>
<li><p><strong>√5、类的定义</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> 类名<span class="params">(父类名)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>  <strong>如果有父类，写在类名后面的括号里；如果没有父类，可以不写括号了。用关键词 pass 占个位置，之后再用具体函数把类补充完整。</strong></p>
<p>  举例：<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Animals</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mammals</span><span class="params">(Animals)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cats</span><span class="params">(Mammals)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong>√6、类里定义函数时，语法规定第一个参数必须是 <code>self</code> 。</strong></li>
<li><p><strong>√7、<code>__init__</code> 函数，在新对象实例化时会自动运行，用于给新对象赋初值。</strong></p>
<p><img src="/2018/01/06/ai-dl/Image_031.png" alt=""></p>
<p>(1) 将猫类实例化出一个叫 kitty 的对象，kitty 有自己的特征属性，比如身上有 <code>10</code> 个斑点：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kitty = Cats(<span class="number">10</span>)    <span class="comment"># 实例化时运行 init 函数，给 spots 赋值，告知 kitty 有 10 个斑点</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"kitty.spots"</span> <span class="comment"># 打印出 10</span></span><br></pre></td></tr></table></figure>
<p>(2) kitty 可以做具体的工作，比如捉老鼠：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kitty.catch_mouse() <span class="comment"># 对象运行函数，必须用对象.函数名，调用类里的函数</span></span><br><span class="line">                    <span class="comment"># 会运行 print "catch mouse" 故打印出 catch mouse</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><strong>√8、对象调用类里的函数，用 <code>对象.函数名</code>；</strong></li>
<li><strong>√9、对象调用类里的变量，用 <code>对象.变量名</code>。</strong></li>
<li><p><strong>√10、类内定义函数时，如调用自身或父类的函数与变量，须用 <code>self.引导</code>，应写为 <code>self.函数名</code> 或 <code>self.变量名</code>。</strong></p>
<p><img src="/2018/01/06/ai-dl/Image_032.png" alt=""></p>
<p>代码验证：</p>
<p><img src="/2018/01/06/ai-dl/Image_033.jpg" alt=""></p>
<p><img src="/2018/01/06/ai-dl/Image_034.jpg" alt=""></p>
<p><img src="/2018/01/06/ai-dl/Image_035.jpg" alt="">  </p>
</li>
</ul>
<p>补充：</p>
<p>Python 中虽然没有访问控制的关键字，例如 <code>private`</code>、protected` 等等。<br>但是，在 Python 编码中，有一些约定来进行访问控制。</p>
<p>单下划线、双下划线、头尾双下划线说明：</p>
<ul>
<li><code>_foo</code>: 以单下划线开头的表示的是 <code>protected</code> 类型的变量，即保护类型只能允许其本身与子类进行访问，不能用于 <code>from module import *</code></li>
<li><code>__foo</code>: 双下划线的表示的是私有类型(<code>private</code>)的变量, 只能是允许这个类本身进行访问了。</li>
<li><code>__foo__</code>: 头尾双下划线定义的是特列方法，类似 <code>__init__()</code> 之类的。</li>
</ul>
<h2 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h2><h3 id="文件写操作"><a href="#文件写操作" class="headerlink" title="文件写操作"></a>文件写操作</h3><p><code>import pickle</code></p>
<p><img src="/2018/01/06/ai-dl/Image_036.jpg" alt=""></p>
<ul>
<li><strong>开：</strong><code>文件变量 = open(&quot;文件路径文件名&quot;, &quot;wb&quot;)</code> </li>
<li><strong>存：</strong><code>pickle.dump(待写入的变量，文件变量)</code>  </li>
<li><strong>关：</strong><code>文件变量.close()</code>  </li>
</ul>
<p>代码验证：</p>
<p><img src="/2018/01/06/ai-dl/Image_037.jpg" alt=""></p>
<h3 id="文件读操作"><a href="#文件读操作" class="headerlink" title="文件读操作"></a>文件读操作</h3><p><code>import pickle</code></p>
<p><img src="/2018/01/06/ai-dl/Image_038.jpg" alt=""></p>
<ul>
<li><strong>开：</strong><code>文件变量 = open(&quot;文件路径文件名&quot;, &quot;rb&quot;)</code>   </li>
<li><strong>取：</strong><code>放内容的变量 = pickle.load(文件变量)</code>  </li>
<li><strong>关：</strong><code>文件变量.close()</code>   </li>
</ul>
<p>代码验证：</p>
<p><img src="/2018/01/06/ai-dl/Image_039.jpg" alt=""></p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><ul>
<li>√ 1、ubuntu 终端的简单使用</li>
<li>√ 2、vim 编辑器的基本用法</li>
<li>√ 3、python 里的数据类型：数值、字符串、列表、元组和字典</li>
<li>√ 4、python 的条件语句和循环语句</li>
<li>√ 5、代码纵向对齐表层次关系</li>
<li>√ 6、函数、对象、类、模块、包还有面向对象的编程思想</li>
<li>√ 7、文件读写操作</li>
</ul>
<p><br><br><br></p>
<h1 id="搭建神经网络，总结搭建八股"><a href="#搭建神经网络，总结搭建八股" class="headerlink" title="搭建神经网络，总结搭建八股"></a>搭建神经网络，总结搭建八股</h1><h2 id="张量、计算图、会话"><a href="#张量、计算图、会话" class="headerlink" title="张量、计算图、会话"></a>张量、计算图、会话</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul>
<li>√ <strong>基于 Tensorflow 的 NN：用张量表示数据，用计算图搭建神经网络，用会话执行计算图，优化线上的权重(参数)，得到模型。</strong></li>
<li><p>√ <strong>张量：张量就是多维数组(列表)，用”阶”表示张量的维度。</strong>  </p>
<ul>
<li>0 阶张量称作标量，表示一个单独的数；<br>【<strong>举例</strong>】 <code>S=123</code> </li>
<li>1 阶张量称作向量，表示一个一维数组；<br>【<strong>举例</strong>】 <code>V=[1,2,3]</code></li>
<li>2 阶张量称作矩阵，表示一个二维数组，它可以有 <code>i</code> 行 <code>j</code> 列个元素，每个元素可以用行号和列号共同索引到；<br>【<strong>举例</strong>】 <code>m=[[1, 2, 3], [4, 5, 6], [7, 8, 9]]</code>  </li>
<li>判断张量是几阶的，就通过张量右边的方括号数，0 个是 0 阶，n 个是 n 阶，张量可以表示 0 阶到 n 阶数组(列表)；<br>【<strong>举例</strong>】 <code>t=[ [ [… ] ] ]</code> 为 3 阶。  </li>
</ul>
</li>
<li><p>√ <strong>数据类型：Tensorflow 的数据类型有 <code>tf.float32</code>、<code>tf.int32</code> 等。</strong><br>  【<strong>举例</strong>】<br>  我们实现 Tensorflow 的加法：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf     <span class="comment"># 引入模块</span></span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>]) <span class="comment"># 定义一个张量等于 [1.0,2.0] </span></span><br><span class="line">b = tf.constant([<span class="number">3.0</span>, <span class="number">4.0</span>]) <span class="comment"># 定义一个张量等于 [3.0,4.0] </span></span><br><span class="line">result = a+b                <span class="comment"># 实现 a 加 b 的加法</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> result                <span class="comment"># 打印出结果</span></span><br></pre></td></tr></table></figure>
<p>  可以打印出这样一句话：<br>  <code>Tensor(&quot;add:0&quot;, shape=(2, ), dtype=float32)</code>，<br>  意思为 <code>result</code> 是一个名称为 <code>add:0</code> 的张量，<br>  <code>shape=(2,)</code> 表示一维数组长度为 2，<br>  <code>dtype=float32</code> 表示数据类型为浮点型。</p>
</li>
<li><p>√ <strong>计算图(Graph)：搭建神经网络的计算过程，是承载一个或多个计算节点的一张图，只搭建网络，不运算。</strong><br>  【<strong>举例</strong>】<br>  在第一讲中我们曾提到过，神经网络的基本模型是神经元，神经元的基本模型其实就是数学中的乘、加运算。我们搭建如下的计算图：</p>
<p>  <img src="/2018/01/06/ai-dl/img1.svg" alt=""></p>
  <!-- $$
  \text{y}=\text{x}_1\text{w}_1+\text{x}_2\text{w}_2
  $$ -->
<p>  <code>x1</code>、<code>x2</code> 表示输入，<code>w1</code>、<code>w2</code> 分别是 <code>x1</code> 到 <code>y</code> 和 <code>x2</code> 到 <code>y</code> 的权重，<img src="/2018/01/06/ai-dl/in-eq1.svg" alt="&quot;$y=x_1w_1+x_2w_2$&quot;">。 我们实现上述计算图：  </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf         <span class="comment"># 引入模块</span></span><br><span class="line"></span><br><span class="line">x = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>]])   <span class="comment"># 定义一个 2 阶张量等于 [[1.0,2.0]]</span></span><br><span class="line">w = tf.constant([[<span class="number">3.0</span>], [<span class="number">4.0</span>]]) <span class="comment"># 定义一个 2 阶张量等于 [[3.0],[4.0]] </span></span><br><span class="line">y = tf.matmul(x, w)             <span class="comment"># 实现 xw 矩阵乘法</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> y                         <span class="comment"># 打印出结果</span></span><br></pre></td></tr></table></figure>
<p>  可以打印出这样一句话：<br>  <code>Tensor(&quot;matmul:0&quot;, shape(1,1), dtype=float32)</code>，<br>  从这里我们可以看出，<code>print</code> 的结果显示 <code>y</code> 是一个张量，只搭建承载计算过程的计算图，并没有运算，如果我们想得到运算结果就要用到”会话 <code>Session()</code>“了。</p>
</li>
<li><p>√ <strong>会话(Session)：执行计算图中的节点运算。</strong><br>  我们用 with 结构实现，语法如下： </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">print</span> sess.run(y)</span><br></pre></td></tr></table></figure>
<p>  【<strong>举例</strong>】：<br>  对于刚刚所述计算图，我们执行 <code>Session()</code> 会话可得到矩阵相乘结果：  </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf         <span class="comment"># 引入模块</span></span><br><span class="line"></span><br><span class="line">x = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>]])   <span class="comment"># 定义一个 2 阶张量等于 [[1.0,2.0]]</span></span><br><span class="line">w = tf.constant([[<span class="number">3.0</span>], [<span class="number">4.0</span>]]) <span class="comment"># 定义一个 2 阶张量等于 [[3.0],[4.0]] </span></span><br><span class="line">y = tf.matmul(x, w)             <span class="comment"># 实现 xw 矩阵乘法</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> y                         <span class="comment"># 打印出结果 </span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">print</span> sess.run(y)           <span class="comment"># 执行会话并打印出执行后的结果</span></span><br></pre></td></tr></table></figure>
<p>  可以打印出这样的结果：<br>  <code>Tensor(&quot;matmul:0&quot;, shape(1,1), dtype=float32) [[11.]]</code><br>  我们可以看到，运行 <code>Session()</code> 会话前只打印出 <code>y</code> 是个张量的提示，运行 <code>Session()</code> 会话后打印出了 <code>y</code> 的结果 <code>1.0*3.0 + 2.0*4.0 = 11.0</code>。</p>
</li>
</ul>
<p><strong>注①</strong>：我们以后会常用到 <code>vim</code> 编辑器，为了使用方便，我们可以更改 vim 的配置<br>文件，使 vim 的使用更加便捷。我们在 <code>vim ~/.vimrc</code><br>写入： <code>set ts=4</code> 表示使 <code>Tab</code> 键等效为 4 个空格</p>
<p><code>set nu</code> 表示使 <code>vim</code> 显示行号 <code>nu</code> 是 <code>number</code> 缩写</p>
<p><strong>注②</strong>：在 <code>vim</code> 编辑器中运行 <code>Session()</code> 会话时，有时会出现”提示 warning”，是因为有的电脑可以支持加速指令，但是运行代码时并没有启动这些指令。可以把这些”提示 warning”暂时屏蔽掉。屏蔽方法为进入主目录下的 <code>bashrc</code> 文件，在<br><code>bashrc</code> 文件中加入这样一句 <code>export TF_CPP_MIN_LOG_LEVEL=2</code>，从而把”提示 warning”等级降低。</p>
<p>这个命令可以控制 <code>python</code> 程序显示提示信息的等级，在 Tensorflow 里面一般设置成是<br>“<code>0</code>“(显示所有信息)或者”<code>1</code>“(不显示 info)，<br>“<code>2</code>“代表不显示 warning，<br>“<code>3</code>“代表不显示 error。一般不建议设置成 3。<br><code>source</code> 命令用于重新执行修改的初始化文件，使之立即生效，而不必注销并重新登录。</p>
<h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><h3 id="神经网络的参数"><a href="#神经网络的参数" class="headerlink" title="神经网络的参数"></a>神经网络的参数</h3><ul>
<li>√ <strong>神经网络的参数：是指神经元线上的权重 <code>w</code>，用变量表示，一般会先随机生成 这些参数。生成参数的方法是让 <code>w</code> 等于<code>tf.Variable</code>，把生成的方式写在括号里。</strong> </li>
</ul>
<p><strong>神经网络中常用的生成随机数/数组的函数有：</strong></p>
<ul>
<li><code>tf.random_normal()</code> 生成<strong>正态分布随机数</strong> </li>
<li><code>tf.truncated_normal()</code> 生成<strong>去掉过大偏离点的</strong>正态分布随机数 </li>
<li><code>tf.random_uniform()</code> 生成<strong>均匀分布随机数</strong></li>
<li><code>tf.zeros</code> 表示生成<strong>全 0 数组</strong></li>
<li><code>tf.ones</code> 表示生成<strong>全 1 数组</strong> </li>
<li><code>tf.fill</code> 表示生成<strong>全定值数组</strong></li>
<li><code>tf.constant</code> 表示生成<strong>直接给定值的数组</strong> </li>
</ul>
<p>【<strong>举例</strong>】</p>
<ul>
<li>① <code>w=tf.Variable(tf.random_normal([2,3],stddev=2, mean=0, seed=1))</code><br>  表示生成正态分布随机数，形状两行三列，标准差是 2，均值是 0，随机种子是 1。</li>
<li>② <code>w=tf.Variable(tf.Truncated_normal([2,3],stddev=2, mean=0, seed=1))</code><br>  表示去掉偏离过大的正态分布，也就是如果随机出来的数据偏离平均值超过两个 标准差，这个数据将重新生成。</li>
<li>③ <code>w=random_uniform(shape=7,minval=0,maxval=1,dtype=tf.int32，seed=1)</code><br>  表示从一个均匀分布 <code>[minval maxval)</code> 中随机采样，<br>  注意定义域是左闭右开，即包含 <code>minval</code>，不包含 <code>maxval</code>。</li>
<li>④ 除了生成随机数， 还可以生成常量。<ul>
<li><code>tf.zeros([3,2],int32)</code> 表示生成 <code>[[0,0],[0,0],[0,0]]</code>；</li>
<li><code>tf.ones([3,2],int32)</code> 表示生成 <code>[[1,1],[1,1],[1,1]</code>；</li>
<li><code>tf.fill([3,2],6)</code> 表示生成 <code>[[6,6],[6,6],[6,6]]</code>；</li>
<li><code>tf.constant([3,2,1])</code> 表示生成 <code>[3,2,1]</code>。</li>
</ul>
</li>
</ul>
<p><strong>注意</strong>：</p>
<ul>
<li>① 随机种子如果去掉每次生成的随机数将不一致。</li>
<li>② 如果没有特殊要求标准差、均值、随机种子是可以不写的。 </li>
</ul>
<h3 id="神经网络的搭建"><a href="#神经网络的搭建" class="headerlink" title="神经网络的搭建"></a>神经网络的搭建</h3><p>当我们知道张量、计算图、会话和参数后，我们可以讨论神经网络的实现过程了。</p>
<p><strong>√ 神经网络的实现过程：</strong></p>
<ol>
<li><strong>准备数据集，提取特征，作为输入喂给神经网络(Neural Network, NN)</strong></li>
<li><strong>搭建 NN 结构，从输入到输出(先搭建计算图，再用会话执行)</strong><br>( NN 前向传播算法 ==&gt;&gt; 计算输出 )</li>
<li><strong>大量特征数据喂给 NN，迭代优化 NN 参数</strong><br>( NN 反向传播算法 ==&gt;&gt; 优化参数训练模型 )</li>
<li><strong>使用训练好的模型预测和分类</strong>  </li>
</ol>
<p>由此可见，基于神经网络的机器学习主要分为两个过程，即训练过程和使用过程。 训练过程是第一步、第二步、第三步的循环迭代，使用过程是第四步，一旦参数 优化完成就可以固定这些参数，实现特定应用了。  </p>
<p>很多实际应用中，我们会先使用现有的成熟网络结构，喂入新的数据，训练相应模型，判断是否能对喂入的从未见过的新数据作出正确响应，再适当更改网络结构，反复迭代，让机器自动训练参数找出最优结构和参数，以固定专用模型。 </p>
<h3 id="前向传播-1"><a href="#前向传播-1" class="headerlink" title="前向传播"></a>前向传播</h3><ul>
<li>√ <strong>前向传播就是搭建模型的计算过程，让模型具有推理能力，可以针对一组输入给出相应的输出。</strong></li>
</ul>
<p>【<strong>举例</strong>】</p>
<p>假如生产一批零件，体积为 <code>x1</code>，重量为 <code>x2</code>，体积和重量就是我们选择的特征，把它们喂入神经网络，当体积和重量这组数据走过神经网络后会得到一个输出。</p>
<p>假如输入的特征值是：体积 0.7 重量 0.5</p>
<p><img src="/2018/01/06/ai-dl/img2.svg" alt=""></p>
<p>由搭建的神经网络可得，隐藏层节点 <img src="/2018/01/06/ai-dl/in-eq2.svg" alt="&quot;$a_{11}=x_1w_{1, 1}^{(1)}+x_2w_{1, 2}^{(1)}=0.14+0.15=0.29$&quot;">，同理算得节点 <img src="/2018/01/06/ai-dl/in-eq3.svg" alt="&quot;$a_{12}=0.32, a_{13}=0.38$&quot;">，最终计算得到输出层 <code>Y=-0.015</code>，这便实现了前向传播过程。</p>
<h4 id="√-推导"><a href="#√-推导" class="headerlink" title="√ 推导"></a>√ 推导</h4><h5 id="第一层"><a href="#第一层" class="headerlink" title="第一层"></a>第一层</h5><ul>
<li><p>√ <strong><code>X</code> 是输入为 1x2 矩阵</strong><br>  用 x 表示输入，是一个 1 行 2 列矩阵，表示一次输入一组特征，这组特征包含了 体积和重量两个元素。</p>
</li>
<li><p>√ <img src="/2018/01/06/ai-dl/in-sym1.svg" alt="&quot;$W_{i,j}^{(n)}$&quot;"> <strong>为待优化的参数</strong>  </p>
<blockquote>
<p><code>n</code> 为层数；<code>i</code> 为前节点编号；<code>j</code> 为后节点编号</p>
</blockquote>
<p>  对于第一层的 w 前面有两个节点，后面有三个节点 w 应该是个两行三列矩阵，我们这样表示：  </p>
  <!-- $$
  \text{w}^{\left( 2 \right)}=\left[ \begin{array}{c}
      \text{w}_{\text{1,}1}^{\left( 2 \right)}\\
      \text{w}_{\text{1,}2}^{\left( 2 \right)}\\
      \text{w}_{\text{1,}3}^{\left( 2 \right)}\\
  \end{array} \right] 
  $$ -->
<p>  <img src="/2018/01/06/ai-dl/eq1.svg" alt=""></p>
</li>
<li><p>√ <strong>神经网络共有几层(或当前是第几层网络) 都是指的计算层，输入不是计算层，所以 a 为第一层网络，a 是一个一行三列矩阵。</strong><br>  我们这样表示：  </p>
  <!-- $$
  a^{(1)}=\left[ a_{11},\ a_{12},\ a_{13} \right] =XW^{(1)}
  $$ -->
<p>  <img src="/2018/01/06/ai-dl/eq2.svg" alt=""></p>
</li>
</ul>
<h5 id="第二层"><a href="#第二层" class="headerlink" title="第二层"></a>第二层</h5><ul>
<li><p>√ <strong>参数要满足前面三个节点，后面一个节点，所以 <img src="/2018/01/06/ai-dl/sym1.svg" alt="">   是三行一列矩阵。</strong> </p>
<p>  我们这样表示：  </p>
  <!-- $$
  \text{w}^{(1)}=\left[ 
  \begin{matrix}
  \text{w}_{\text{1,}1}^{(1)}& \text{w}_{\text{1,}2}^{(1)}& \text{w}_{\text{1,}3}^{(1)} \\
  \text{w}_{\text{2,}1}^{(1)}& \text{w}_{\text{2,}2}^{(1)}& \text{w}_{\text{2,}3}^{(1)} \\
  \end{matrix} \right] 
  $$ -->
<p>  <img src="/2018/01/06/ai-dl/eq3.svg" alt=""></p>
<p>  我们把每层输入乘以线上的权重 <code>w</code>，这样用矩阵乘法可以计算出输出 <code>y</code> 了。 </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = tf.matmul(X, W1)</span><br><span class="line">y = tf.matmul(a, W2)</span><br></pre></td></tr></table></figure>
<p>  由于需要计算结果，就要用 <code>with</code> 结构实现，所有变量初始化过程、计算过程都要放到 <code>sess.run</code> 函数中。对于变量初始化，我们在 <code>sess.run</code> 中写入 <code>tf.global_variables_initializer</code> 实现对所有变量初始化，也就是赋初值。对于计算图中的运算，我们直接把运算节点填入 <code>sess.run</code> 即可，比如要计算输出 <code>y</code>，直接写 <code>sess.run(y)</code> 即可。</p>
<p>  在实际应用中，我们可以一次喂入一组或多组输入，让神经网络计算输出 <code>y</code>，可以先用 <code>tf.placeholder</code> 给输入占位。如果一次喂一组数据 <code>shape</code> 的第一维位置写 1，第二维位置看有几个输入特征；如果一次想喂多组数据，<code>shape</code> 的第一维位置可以写 <code>None</code> 表示先空着，第二维位置写有几个输入特征。这样在 <code>feed_dict</code> 中可以喂入若干组体积重量了。</p>
</li>
<li><p>√ <strong>前向传播过程的 tensorflow 描述：</strong></p>
</li>
<li><p>√ <strong>变量初始化、计算图节点运算都要用会话(with 结构)实现</strong> </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">sess.run()</span><br></pre></td></tr></table></figure>
</li>
<li><p>√ <strong>变量初始化：在 <code>sess.run</code> 函数中用 <code>tf.global_variables_initializer()</code> 汇总所有待优化变量。</strong>  </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line">sess.run(init_op)</span><br></pre></td></tr></table></figure>
</li>
<li><p>√ <strong>计算图节点运算：在 <code>sess.run</code> 函数中写入待运算的节点</strong><br>  <code>sess.run(y)</code></p>
</li>
<li><p>√ <strong>用 <code>tf.placeholder</code> 占位，在 <code>sess.run</code> 函数中用 <code>feed_dict</code> 喂数据</strong><br>  <strong>喂一组数据：</strong>  </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="number">1</span>, <span class="number">2</span>)) </span><br><span class="line">sess.run(y, feed_dict=&#123;x: [[<span class="number">0.5</span>,<span class="number">0.6</span>]]&#125;)</span><br></pre></td></tr></table></figure>
<p>  <strong>喂多组数据：</strong>  </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">sess.run(y, feed_dict=&#123;x: [[<span class="number">0.1</span>,<span class="number">0.2</span>],[<span class="number">0.2</span>,<span class="number">0.3</span>],[<span class="number">0.3</span>,<span class="number">0.4</span>],[<span class="number">0.4</span>,<span class="number">0.5</span>]]&#125;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>【<strong>举例</strong>】:<br>这是一个实现神经网络前向传播过程，网络可以自动推理出输出 <code>y</code> 的值。</p>
<ul>
<li><p>① 用 <code>placeholder</code> 实现输入定义(<code>sess.run</code> 中喂入一组数据)的情况   </p>
<p>  第一组喂体积 0.7、重量 0.5</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入和参数 </span></span><br><span class="line">x=tf.placeholder(tf.float32,shape=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">w1=tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>)) </span><br><span class="line">w2=tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播过程 </span></span><br><span class="line">a=tf.matmul(x,w1) </span><br><span class="line">y=tf.matmul(a,w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用会话计算结果</span></span><br><span class="line">withtf.Session() <span class="keyword">as</span> sess:     </span><br><span class="line">    init_op=tf.global_variables_initializer() </span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"y in tf3_3.py is:\n"</span>, sess.run(y,feed_dict=&#123;x:[[<span class="number">0.7</span>,<span class="number">0.5</span>]]&#125;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>② 用 <code>placeholder</code> 实现输入定义(<code>sess.run</code> 中喂入多组数据)的情况  </p>
<p>  第一组喂体积 0.7、重量 0.5，第二组喂体积 0.2、重量 0.3，第三组喂体积 0.3 、重量 0.4，第四组喂体积 0.4、重量 0.5。</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入和参数 </span></span><br><span class="line">x=tf.placeholder(tf.float32,shape=(<span class="keyword">None</span>,<span class="number">2</span>)) </span><br><span class="line">w1=tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>)) </span><br><span class="line">w2=tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播过程 </span></span><br><span class="line">a=tf.matmul(x,w1) </span><br><span class="line">y=tf.matmul(a,w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用会话计算结果</span></span><br><span class="line">withtf.Session() <span class="keyword">as</span> sess: </span><br><span class="line">    init_op=tf.global_variables_initializer() </span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"y in tf3_4.py is:\n"</span>, sess.run(y,feed_dict=&#123;x:[[<span class="number">0.7</span>,<span class="number">0.5</span>], [<span class="number">0.2</span>,<span class="number">0.3</span>],[<span class="number">0.3</span>,<span class="number">0.4</span>]，[<span class="number">0.4</span>,<span class="number">0.5</span>]]&#125;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><h3 id="反向传播-1"><a href="#反向传播-1" class="headerlink" title="反向传播"></a>反向传播</h3><ul>
<li>√ <strong>反向传播：训练模型参数，在所有参数上用梯度下降，使 NN 模型在训练数据上的损失函数最小。</strong></li>
<li>√ <strong>损失函数(loss)：计算得到的预测值 <code>y</code> 与已知答案 <code>y_</code> 的差距。</strong><br>损失函数的计算有很多方法，均方误差 MSE 是比较常用的方法之一。</li>
<li>√ <strong>均方误差 MSE：求前向传播计算结果与已知答案之差的平方再求平均。</strong>    <!-- $$
  \mathbf{MSE}\left( y_{\_},\ y \right) =\frac{\sum_{i=1}^n{\left( y-y_{\_} \right) ^2}}{n}
  $$ -->
  <img src="/2018/01/06/ai-dl/eq4.svg" alt=""><br>  用 tensorflow 函数表示为：<br>  <code>loss_mse = tf.reduce_mean(tf.square(y_ - y))</code></li>
<li><p>√ <strong>反向传播训练方法：以减小 loss 值为优化目标，有梯度下降、momentum 优化器、adam 优化器等优化方法。</strong>  </p>
<p>  这三种优化方法用 tensorflow 的函数可以表示为：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</span><br><span class="line">train_step=tf.train.MomentumOptimizer(learning_rate, momentum).minimize(loss) </span><br><span class="line">train_step=tf.train.AdamOptimizer(learning_rate).minimize(loss)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>三种优化方法区别如下：</p>
<ul>
<li><p>① <code>tf.train.GradientDescentOptimizer()</code> 使用随机梯度下降算法，使参数沿着梯度的反方向，即总损失减小的方向移动，实现更新参数。</p>
<p>  <img src="/2018/01/06/ai-dl/img3.svg" alt=""></p>
<p>  参数更新公式是  </p>
  <!-- $$
  \theta _{n+1}=\theta _n-\alpha \frac{\partial J\left( \theta _n \right)}{\partial \theta _n}
  $$ -->
<p>  <img src="/2018/01/06/ai-dl/eq5.svg" alt=""></p>
<p>  其中，<code>𝐽(𝜃)</code> 为损失函数，<code>𝜃</code> 为参数，<code>𝛼</code> 为学习率。</p>
</li>
<li><p>② <code>tf.train.MomentumOptimizer()</code> 在更新参数时，利用了超参数，参数更新公式是</p>
  <!-- $$
  d_i=\beta d_{i-1}+g\left( \theta _{i-1} \right) \\
  \theta_i=\theta_{i-1}-\alpha d_i 
  $$ -->
<p>  <img src="/2018/01/06/ai-dl/eq6.svg" alt=""></p>
<p>  其中，<code>𝛼</code> 为学习率，超参数为 <code>𝛽, 𝜃</code> 为参数，<img src="/2018/01/06/ai-dl/in-sym2.svg" alt="&quot;$g\left( \theta _i-1 \right)$&quot;"> 为损失函数的梯度。</p>
</li>
<li><p>③ <code>tf.train.AdamOptimizer()</code> 是利用自适应学习率的优化算法，Adam 算法和随机梯度下降算法不同。随机梯度下降算法保持单一的学习率更新所有的参数，学习率在训练过程中并不会改变。而 Adam 算法通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率。</p>
</li>
<li><p>√ <strong>学习率：决定每次参数更新的幅度。</strong><br>优化器中都需要一个叫做学习率的参数，使用时，如果学习率选择过大会出现震荡不收敛的情况，如果学习率选择过小，会出现收敛速度慢的情况。我们可以选个比较小的值填入，比如 0.01、0.001。</p>
</li>
</ul>
<h4 id="进阶：反向传播参数更新推导过程"><a href="#进阶：反向传播参数更新推导过程" class="headerlink" title="进阶：反向传播参数更新推导过程"></a>进阶：反向传播参数更新推导过程</h4><p><strong>符号说明：</strong></p>
<ul>
<li><img src="/2018/01/06/ai-dl/in-sym-zl.svg" alt="&quot;$\text{z}^l$&quot;"> 表示第 <code>l</code> 层隐藏层和输出层的输入值；</li>
<li><img src="/2018/01/06/ai-dl/in-sym-al.svg" alt="&quot;$\alpha^l$&quot;"> 表示第 <code>l</code> 层隐藏层和输出层的输值；</li>
<li><code>f(z)</code> 表示激活函数；</li>
<li>最后的输出层为第 <code>L</code> 层。 </li>
</ul>
<p><strong>推导过程：</strong></p>
<p>前向传播第 <code>L</code> 层输出：<br><!-- $$
\alpha ^l=f\left( z^l \right) =f\left( w^l\alpha ^{l-1}+b^l \right) 
$$ --><br><img src="/2018/01/06/ai-dl/eq7.svg" alt=""></p>
<p>损失函数：<br><!-- $$
J\left( w,\ b,\ x,\ y \right) =\frac{1}{2}\lVert \alpha ^l-y \rVert _{2}^{2}
$$ --><br><img src="/2018/01/06/ai-dl/eq8.svg" alt=""></p>
<p>对于输出层第 <code>L</code> 层：</p>
<p><img src="/2018/01/06/ai-dl/eq9.svg" alt=""></p>
<!-- $$
\begin{aligned}
\frac{\partial J\left( w,b,x,y \right)}{\partial w^L} 
&=\frac{\partial J\left( w,b,x,y \right)}{\partial z^L}\frac{\partial z^L}{\partial w^L} \\
&=\left( f\left( z^L \right) -y \right) f'\left( z^L \right) \alpha ^{L-1} \\
&=\left( \alpha ^L-y \right) \left( \alpha ^{L-1} \right) ^T\odot f'\left( z^L \right) \\

\frac{\partial J\left( w,b,x,y \right)}{\partial b^L}
&=\frac{\partial J\left( w,b,x,y \right)}{\partial z^L}\frac{\partial z^L}{\partial b^L} \\
&=\left( f\left( z^L \right) -y \right) f'\left( z^L \right) \\
&=\left( \alpha ^L-y \right) \odot f'\left( z^L \right) 
\end{aligned}
$$ -->
<p><img src="/2018/01/06/ai-dl/eq10.svg" alt=""></p>
<p>更新最后一层 <code>L</code> 层的参数 <img src="/2018/01/06/ai-dl/in-sym-w-L.svg" alt="&quot;$w^L$&quot;"><br>和<img src="/2018/01/06/ai-dl/in-sym-b-L.svg" alt="&quot;$b^L$&quot;"></p>
<p><img src="/2018/01/06/ai-dl/eq11.svg" alt=""></p>
<p>令<br><!-- $$
\begin{aligned}
\delta ^l
&=\frac{\partial J\left( w,b,x,y \right)}{\partial z^l} \\
&=\frac{\partial J\left( w,b,x,y \right)}{\partial z^L}\frac{\partial z^L}{\partial z^{L-1}}\frac{\partial z^{L-1}}{\partial z^{L-2}}\cdots \frac{\partial z^{l+1}}{\partial z^l}
\end{aligned}
$$ --><br><img src="/2018/01/06/ai-dl/eq12.svg" alt=""></p>
<p>则有<br><!-- $$
\frac{\partial J\left( w,b,x,y \right)}{\partial w^l}=\frac{\partial J\left( w,b,x,y \right)}{\partial z^l}\frac{\partial z^l}{\partial w^l}=\left( w^{l+1} \right) ^T\odot f'\left( z^l \right) 
\\
\frac{\partial J\left( w,b,x,y \right)}{\partial b^l}=\frac{\partial J\left( w,b,x,y \right)}{\partial z^l}\frac{\partial z^l}{\partial b^l}=\delta ^l
$$ --><br><img src="/2018/01/06/ai-dl/eq13.svg" alt=""></p>
<p><img src="/2018/01/06/ai-dl/in-sym-detl.svg" alt="&quot;$\delta^l$&quot;"> 可以用数学归纳法得到：<br><!-- $$
\delta ^l=\frac{\partial J\left( w,b,x,y \right)}{\partial z^l}=\frac{\partial J\left( w,b,x,y \right)}{\partial z^{l+1}}\frac{\partial z^{l+1}}{\partial z^l}=\delta ^{l+1}\frac{\partial z^{l+1}}{\partial z^l}
$$ --><br><img src="/2018/01/06/ai-dl/eq14.svg" alt=""></p>
<p>由于 <img src="/2018/01/06/ai-dl/eq15.svg" alt="&quot;$z^{l+1}=w^{l+1}a^l+b^l$&quot;"></p>
<p>所以 </p>
<p><img src="/2018/01/06/ai-dl/eq16.svg" alt=""></p>
<p>因此第 <code>l</code> 层参数 <img src="/2018/01/06/ai-dl/in-sym-wl.svg" alt="&quot;$w^l$&quot;"> 和<br><img src="/2018/01/06/ai-dl/in-sym-bl.svg" alt="&quot;$b^l$&quot;"> 更新公式为：<br><!-- $$
\begin{aligned}
w^l
&=w^l-\frac{\partial J\left( w,b,x,y \right)}{\partial w^l} \\
&=w^l-\delta ^l\left( a^{l-1} \right) ^T \\
&=w^l-\delta ^{l+1}\left( w^{l+1} \right) ^T\odot f'\left( z^l \right) \left( a^{l-1} \right) ^T \\
b^l
&=b^l-\frac{\partial J\left( w,b,x,y \right)}{\partial b^l} \\
&=b^l-\delta ^l \\
&=b^l-\delta ^{l+1}\left( w^{l+1} \right) ^T\odot f'\left( z^l \right) 
\end{aligned}
$$ --><br><img src="/2018/01/06/ai-dl/eq17.svg" alt=""></p>
<h3 id="搭建神经网络的八股"><a href="#搭建神经网络的八股" class="headerlink" title="搭建神经网络的八股"></a>搭建神经网络的八股</h3><p>我们最后梳理出神经网络搭建的八股，神经网络的搭建课分四步完成：准备工作、前向传播、反向传播和循环迭代。</p>
<ol>
<li>√ <strong>导入模块，生成模拟数据集</strong>；<br> <code>import</code><br> 常量定义<br> 生成数据集</li>
<li><p>√ <strong>前向传播：定义输入、参数和输出</strong>  </p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=    y_=</span><br><span class="line">w1=   w2=</span><br><span class="line">a=    y=</span><br></pre></td></tr></table></figure>
</li>
<li><p>√ <strong>反向传播：定义损失函数、反向传播方法</strong>  </p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss= </span><br><span class="line">train_step=</span><br></pre></td></tr></table></figure>
</li>
<li><p>√ <strong>生成会话，训练 STEPS 轮</strong> </p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.session() <span class="keyword">as</span> sess</span><br><span class="line">    Init_op=tf. global_variables_initializer()</span><br><span class="line">    sess_run(init_op)</span><br><span class="line">    STEPS=<span class="number">3000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start=</span><br><span class="line">        end=</span><br><span class="line">        sess.run(train_step, feed_dict:)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>【<strong>举例</strong>】</p>
<p>随机产生 32 组生产出的零件的体积和重量，训练 3000 轮，每 500 轮输出一次损失函数。下面我们通过源代码进一步理解神经网络的实现过程： </p>
<ol start="0">
<li><p><strong>导入模块，生成模拟数据集；</strong>  </p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 0导入模块，生成模拟数据集。</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">8</span></span><br><span class="line">SEED = <span class="number">23455</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#基于seed产生随机数</span></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机数返回32行2列的矩阵 表示32组 体积和重量 作为输入数据集</span></span><br><span class="line">X = rdm.rand(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从X这个32行2列的矩阵中 取出一行 </span></span><br><span class="line"><span class="comment"># 判断 如果和小于1 给Y赋值1；如果和不小于1 给Y赋值0 </span></span><br><span class="line"><span class="comment"># 作为输入数据集的标签（正确答案） </span></span><br><span class="line">Y_ = [[int(x0 + x1 &lt; <span class="number">1</span>)] <span class="keyword">for</span> (x0, x1) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">"X:\n"</span>,X</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Y_:\n"</span>,Y_</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>定义神经网络的输入、参数和输出，定义前向传播过程；</strong>  </p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1定义神经网络的输入、参数和输出,定义前向传播过程。</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">y_= tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">w1= tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">w2= tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>定义损失函数及反向传播方法</strong>  </p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2定义损失函数及反向传播方法。</span></span><br><span class="line">loss_mse = tf.reduce_mean(tf.square(y-y_)) </span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss_mse)</span><br><span class="line"><span class="comment"># train_step = tf.train.MomentumOptimizer(0.001,0.9).minimize(loss_mse)</span></span><br><span class="line"><span class="comment"># train_step = tf.train.AdamOptimizer(0.001).minimize(loss_mse)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>生成会话，训练 STEPS 轮</strong>  </p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3生成会话，训练STEPS轮</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="comment"># 输出目前（未经训练）的参数取值。</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w1:\n"</span>, sess.run(w1)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w2:\n"</span>, sess.run(w2)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"\n"</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练模型。</span></span><br><span class="line">    STEPS = <span class="number">3000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE) % <span class="number">32</span></span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x: X[start:end], y_: Y_[start:end]&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">            total_loss = sess.run(loss_mse, feed_dict=&#123;x: X, y_: Y_&#125;)</span><br><span class="line">            print(<span class="string">"After %d training step(s), loss_mse on all data is %g"</span> % (i, total_loss))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出训练后的参数取值。</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"\n"</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w1:\n"</span>, sess.run(w1)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w2:\n"</span>, sess.run(w2)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>由神经网络的实现结果，我们可以看出，总共训练 3000 轮，每轮从 X 的数据集 和 Y 的标签中抽取相对应的从 <code>start</code> 开始到 <code>end</code> 结束个特征值和标签，喂入神经网络，用 <code>sess.run</code> 求出 <code>loss</code>，每 500 轮打印一次 <code>loss</code> 值。经过 3000 轮后，我们打印出最终训练好的参数<code>w1</code>、<code>w2</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">After 0 training step(s), loss_mse on all data is 5.13118</span><br><span class="line">After 500 training step(s), loss_mse on all data is 0.429111</span><br><span class="line">After 1000 training step(s), loss_mse on all data is 0.409789</span><br><span class="line">After 1500 training step(s), loss_mse on all data is 0.399923</span><br><span class="line">After 2000 training step(s), loss_mse on all data is 0.394146</span><br><span class="line">After 2500 training step(s), loss_mse on all data is 0.390597</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">w1:</span><br><span class="line">[[-0.7000663   0.9136318   0.08953571]</span><br><span class="line"> [-2.3402493  -0.14641267  0.58823055]]</span><br><span class="line">w2:</span><br><span class="line">[[-0.06024267]</span><br><span class="line"> [ 0.91956186]</span><br><span class="line"> [-0.0682071 ]]</span><br></pre></td></tr></table></figure>
<p>这样四步就可以实现神经网络的搭建了。</p>
<h1 id="神经网络优化"><a href="#神经网络优化" class="headerlink" title="神经网络优化"></a>神经网络优化</h1><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><ul>
<li>√ <strong>神经元模型：用数学公式表示为：<img src="/2018/01/06/ai-dl/in-eq-01.svg" alt="&quot;$f\left( \sum_i{x_iw_i}+b \right)$&quot;">，<code>f</code> 为激活函数。神经网络是以神经元为基本单元构成的。</strong></li>
<li><p>√ <strong>激活函数：引入非线性激活因素，提高模型的表达力。</strong><br>  常用的激活函数有 <code>relu</code>、<code>sigmoid</code>、<code>tanh</code> 等。</p>
<ul>
<li><p>① <strong>激活函数 <code>relu</code>: 在 Tensorflow 中，用 <code>tf.nn.relu()</code> 表示</strong><br>  <code>relu()</code> 数学表达式  </p>
  <!-- $$
  \begin{aligned}
  f\left( x \right) 
  &=\max \left( x,0 \right) \\
  &=
  \begin{cases}
  \text{0,}&  x\le 0 \\
  x,&         x\ge 0 \\
  \end{cases}
  \end{aligned}
  $$ -->
<p>  <img src="/2018/01/06/ai-dl/eq-relu.svg" alt="&quot;relu() 数学表达式&quot;"><br>  <img src="/2018/01/06/ai-dl/ReLU.svg" alt="&quot;ReLU 函数图形&quot;"></p>
</li>
<li><p>② <strong>激活函数 <code>sigmoid</code>：在 Tensorflow 中，用 <code>tf.nn.sigmoid()</code> 表示</strong><br>  <code>sigmoid()</code> 数学表达式</p>
  <!-- $$
  f\left( x \right) =\frac{1}{1+e^x}
  $$ -->
<p>  <img src="/2018/01/06/ai-dl/eq-sigmod.svg" alt="&quot;sigmoid() 数学表达式&quot;"><br>  <img src="/2018/01/06/ai-dl/sigmod.svg" alt="&quot;sigmod 函数图形&quot;"></p>
</li>
<li><p>③ <strong>激活函数 <code>tanh</code>：在 Tensorflow 中，用 <code>tf.nn.tanh()</code> 表示</strong><br>  <code>tanh()</code> 数学表达式   </p>
  <!-- $$
  f\left( x \right) =\frac{1-e^{-2x}}{1+e^{-2x}}
  $$ -->
<p>  <img src="/2018/01/06/ai-dl/eq-tanh.svg" alt="&quot;tanh() 数学表达式&quot;"><br>  <img src="/2018/01/06/ai-dl/tanh.svg" alt="&quot;tanh 函数图形&quot;"></p>
</li>
</ul>
</li>
<li><p>√ <strong>神经网络的复杂度：可用神经网络的层数和神经网络中待优化参数个数表示</strong></p>
</li>
<li>√ <strong>神经网路的层数：一般不计入输入层，<code>层数 = n 个隐藏层 + 1 个输出层</code></strong></li>
<li><p>√ <strong>神经网路待优化的参数：神经网络中所有参数 w 的个数 + 所有参数 b 的个数</strong><br>  <strong>【例如】</strong>：</p>
<p>  <img src="/2018/01/06/ai-dl/img01.svg" alt=""></p>
<p>  在该神经网络中，包含 1 个输入层、1 个隐藏层和 1 个输出层，该神经网络的层数为 2 层。</p>
<p>  在该神经网络中，参数的个数是所有参数 w 的个数加上所有参数 b 的总数，第一层参数用三行四列的二阶张量表示(即 12 个线上的权重 w)再加上 4 个偏置 b；第二层参数是四行两列的二阶张量(即 8 个线上的权重 w)再加上 2 个偏置 b。<code>总参数 = 3*4+4+4*2+2 = 26</code>。</p>
</li>
<li><p>√ <strong>损失函数(loss)：用来表示预测值 <code>y</code> 与已知答案 <code>y_</code> 的差距。在训练神经网络时，通过不断改变神经网络中所有参数，使损失函数不断减小，从而训练出更高准确率的神经网络模型。</strong></p>
</li>
<li>√ <strong>常用的损失函数有均方误差、自定义和交叉熵等。</strong></li>
<li><p>√ <strong>均方误差 <code>MSE</code>：n 个样本的预测值 <code>y</code> 与已知答案 <code>y_</code> 之差的平方和，再求平均值。</strong><br>  <img src="/2018/01/06/ai-dl/eq1-MSE.svg" alt=""></p>
<p>  在 Tensorflow 中用 <code>loss_mse = tf.reduce_mean(tf.square(y_ - y))</code>  </p>
</li>
</ul>
<p><strong>【例如】</strong>：  </p>
<p>预测酸奶日销量 <code>y</code>，<code>x1</code> 和 <code>x2</code> 是影响日销量的两个因素。  </p>
<p>应提前采集的数据有：一段时间内，每日的 <code>x1</code> 因素、<code>x2</code> 因素和销量 <code>y_</code>。采集的数据尽量多。在本例中用销量预测产量，最优的产量应该等于销量。由于目前没有数据集，所以拟造了一套数据集。利用 Tensorflow 中函数随机生成 <code>x1</code>、 <code>x2</code>，制造标准答案 <code>y_ = x1 + x2</code>，为了更真实，求和后还加了正负 0.05 的随机噪声。</p>
<p>我们把这套自制的数据集喂入神经网络，构建一个一层的神经网络，拟合预测酸奶日销量的函数。</p>
<p>代码 <a href="./opt4_1.py"><code>opt4_1.py</code></a> 如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 预测多或预测少的影响一样</span></span><br><span class="line"><span class="comment"># 0 导入模块，生成数据集</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">BATCH_SIZE = <span class="number">8</span></span><br><span class="line">SEED = <span class="number">23455</span></span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">X = rdm.rand(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line">Y_ = [[x1+x2+(rdm.rand()/<span class="number">10.0</span><span class="number">-0.05</span>)] <span class="keyword">for</span> (x1, x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 定义神经网络的输入、参数和输出，定义前向传播过程。</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>))</span><br><span class="line">w1= tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">y = tf.matmul(x, w1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 定义损失函数及反向传播方法。</span></span><br><span class="line"><span class="comment">#   定义损失函数为 MSE,反向传播方法为梯度下降。</span></span><br><span class="line">loss_mse = tf.reduce_mean(tf.square(y_ - y))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss_mse)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 生成会话，训练 STEPS 轮</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = <span class="number">20000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE) % <span class="number">32</span></span><br><span class="line">        end = (i*BATCH_SIZE) % <span class="number">32</span> + BATCH_SIZE</span><br><span class="line">        sess.run(</span><br><span class="line">            train_step, </span><br><span class="line">            feed_dict=&#123;x: X[start:end], y_: Y_[start:end]&#125;</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"After %d training steps, w1 is: "</span> % (i)</span><br><span class="line">            <span class="keyword">print</span> sess.run(w1), <span class="string">"\n"</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Final w1 is: \n"</span>, sess.run(w1)</span><br></pre></td></tr></table></figure></p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">After 19500 training steps, w1 is: </span><br><span class="line">[[0.9777026]</span><br><span class="line"> [1.0181949]] </span><br><span class="line"></span><br><span class="line">Final w1 is: </span><br><span class="line">[[0.98019385]</span><br><span class="line"> [1.0159807 ]]</span><br></pre></td></tr></table></figure>
<p>由上述代码可知，本例中神经网络预测模型为 <code>y = w1*x1 + w2*x2</code>，损失函数采用均方误差。通过使损失函数值(loss)不断降低，神经网络模型得到最终参数 <code>w1=0.98</code>，<code>w2=1.02</code>，销量预测结果为 <code>y = 0.98*x1 + 1.02*x2</code>。由于在生成数据集时，标准答案为 <code>y = x1 + x2</code>，因此，销量预测结果和标准答案已非常接近，说明该神经网络预测酸奶日销量正确。</p>
<ul>
<li><strong>√ 自定义损失函数：根据问题的实际情况，定制合理的损失函数。</strong></li>
</ul>
<p><strong>【例如】</strong>：</p>
<p>对于预测酸奶日销量问题，如果预测销量大于实际销量则会损失成本；如果预测销量小于实际销量则会损失利润。在实际生活中，往往制造一盒酸奶的成本和销售一盒酸奶的利润是不等价的。因此，需要使用符合该问题的自定义损失函数。</p>
<p>自定义损失函数为：<img src="/2018/01/06/ai-dl/in-eq-02.svg" alt="&quot;$\text{loss}=\sum_n{f\left( y_{\_},y \right)}$&quot;"><br>其中，损失定义成分段函数：</p>
<!--
$$
f\left( y_{\_},y \right) =
\begin{cases}
    PROFIT\cdot \left( y_{\_}-y \right)&    y<y_{\_}\\
    COST\cdot \left( y_{\_}-y \right)&      y\ge y_{\_}\\
\end{cases}
$$ 
-->
<p><img src="/2018/01/06/ai-dl/eq-01.svg" alt=""></p>
<p>损失函数表示，若预测结果 <code>y</code> 小于标准答案 <code>y_</code>，损失函数为利润乘以预测结果 <code>y</code> 与标准答案 <code>y_</code> 之差；若预测结果 <code>y</code> 大于标准答案 <code>y_</code>，损失函数为成本乘以预测结果 <code>y</code> 与标准答案 <code>y_</code> 之差。</p>
<p>用 Tensorflow 函数表示为：</p>
<p><code>loss = tf.reduce_sum(tf.where(tf.greater(y,y_),COST(y-y_),PROFIT(y_-y)))</code></p>
<ul>
<li><p>① 若酸奶成本为 1 元，酸奶销售利润为 9 元，则制造成本小于酸奶利润，因此希望预测的结果 <code>y</code> 多一些。采用上述的自定义损失函数，训练神经网络模型。</p>
<p>  代码 <a href="./opt4_2.py"><code>opt4_2.py</code></a> 如下：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 酸奶成本 1元， 酸奶利润 9元</span></span><br><span class="line"><span class="comment"># 预测少了损失大，故不要预测少，故生成的模型会多预测一些</span></span><br><span class="line"><span class="comment"># 0 导入模块，生成数据集</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">BATCH_SIZE = <span class="number">8</span></span><br><span class="line">SEED = <span class="number">23455</span></span><br><span class="line">COST = <span class="number">1</span></span><br><span class="line">PROFIT = <span class="number">9</span></span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">X = rdm.rand(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line">Y = [[x1+x2+(rdm.rand()/<span class="number">10.0</span><span class="number">-0.05</span>)] <span class="keyword">for</span> (x1, x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 定义神经网络的输入、参数和输出，定义前向传播过程。</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>))</span><br><span class="line">w1= tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">y = tf.matmul(x, w1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 定义损失函数及反向传播方法。</span></span><br><span class="line"><span class="comment">#   定义损失函数使得预测少了的损失大，于是模型应该偏向多的方向预测。</span></span><br><span class="line">loss = tf.reduce_sum(</span><br><span class="line">    tf.where(</span><br><span class="line">        tf.greater(y, y_), </span><br><span class="line">        (y - y_)*COST, </span><br><span class="line">        (y_ - y)*PROFIT</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 生成会话，训练STEPS轮。</span></span><br><span class="line"><span class="comment"># ....</span></span><br></pre></td></tr></table></figure>
<p>  运行结果如下：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">After 2000 training steps, w1 is: </span><br><span class="line">[[1.0179386]</span><br><span class="line">[1.0412899]] </span><br><span class="line"></span><br><span class="line">After 2500 training steps, w1 is: </span><br><span class="line">[[1.0205938]</span><br><span class="line">[1.0390677]] </span><br><span class="line"></span><br><span class="line">Final w1 is: </span><br><span class="line">[[1.0296593]</span><br><span class="line">[1.0484432]]</span><br></pre></td></tr></table></figure>
<p>  由代码执行结果可知，神经网络最终参数为 <code>w1=1.03</code>， <code>w2=1.05</code>，销量预测结果为 <code>y = 1.03*x1+1.05*x2</code>。由此可见，采用自定义损失函数预测的结果大于采用均方误差预测的结果，更符合实际需求。</p>
</li>
<li><p>② 若酸奶成本为 9 元，酸奶销售利润为 1 元，则制造成本大于酸奶利润，因此希望预测结果 <code>y</code> 小一些。采用上述的自定义损失函数，训练神经网络模型。</p>
<p>  代码 <a href="./opt4_3.py"><code>opt4_3.py</code></a> 如下：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 酸奶成本9元， 酸奶利润1元</span></span><br><span class="line"><span class="comment"># 预测多了损失大，故不要预测多，故生成的模型会少预测一些</span></span><br><span class="line"><span class="comment"># 0 导入模块，生成数据集</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">BATCH_SIZE = <span class="number">8</span></span><br><span class="line">SEED = <span class="number">23455</span></span><br><span class="line">COST = <span class="number">9</span></span><br><span class="line">PROFIT = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">X = rdm.rand(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line">Y = [[x1+x2+(rdm.rand()/<span class="number">10.0</span><span class="number">-0.05</span>)] <span class="keyword">for</span> (x1, x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 定义神经网络的输入、参数和输出，定义前向传播过程。</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>))</span><br><span class="line">w1= tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">y = tf.matmul(x, w1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 定义损失函数及反向传播方法。</span></span><br><span class="line"><span class="comment">#   重新定义损失函数，使得预测多了的损失大，于是模型应该偏向少的方向预测。</span></span><br><span class="line">loss = tf.reduce_sum(</span><br><span class="line">    tf.where(</span><br><span class="line">        tf.greater(y, y_), </span><br><span class="line">        (y - y_)*COST, </span><br><span class="line">        (y_ - y)*PROFIT</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 生成会话，训练STEPS轮。</span></span><br><span class="line"><span class="comment"># ....</span></span><br></pre></td></tr></table></figure>
<p>  运行结果如下：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">After 2000 training steps, w1 is: </span><br><span class="line">[[0.9602475]</span><br><span class="line">[0.9742084]] </span><br><span class="line"></span><br><span class="line">After 2500 training steps, w1 is: </span><br><span class="line">[[0.96100295]</span><br><span class="line">[0.96993417]] </span><br><span class="line"></span><br><span class="line">Final w1 is: </span><br><span class="line">[[0.9600407 ]</span><br><span class="line">[0.97334176]]</span><br></pre></td></tr></table></figure>
<p>  由执行结果可知，神经网络最终参数为 <code>w1=0.96</code>，<code>w2=0.97</code>，销量预测结果为 <code>y = 0.96*x1 + 0.97*x2</code>。</p>
</li>
</ul>
<p>因此，采用自定义损失函数预测的结果小于采用均方误差预测的结果，更符合实际需求。</p>
<ul>
<li><p>√ <strong>交叉熵(Cross Entropy)：表示两个概率分布之间的距离。</strong><br>  <strong>交叉熵越大，两个概率分布距离越远，两个概率分布越相异；</strong><br>  <strong>交叉熵越小，两个概率分布距离越近，两个概率分布越相似。</strong><br>  <strong>交叉熵计算公式</strong>：<img src="/2018/01/06/ai-dl/in-eq-03.svg" alt="&quot;$H\left( y_{\_}, y \right) =-\sum{y_{\_}\cdot \log y}$&quot;"><br>  用 Tensorflow 函数表示为<br>  <code>ce= -tf.reduce_mean(y_* tf.log(tf.clip_by_value(y, 1e-12, 1.0)))</code> </p>
<p>  <strong>【例如】</strong>：  </p>
<p>  两个神经网络模型解决二分类问题中，已知标准答案为 <code>y_=(1, 0)</code>，第一个神经网络模型预测结果为 <code>y1=(0.6, 0.4)</code>，第二个神经网络模型预测结果为 <code>y2=(0.8, 0.2)</code>，判断哪个神经网络模型预测的结果更接近标准答案。</p>
<p>  根据交叉熵的计算公式得：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">H1((1,0),(0.6,0.4)) = -(1*log0.6 + 0*log0.4) ≈ -(-0.222 + 0) = 0.222</span><br><span class="line">H2((1,0),(0.8,0.2)) = -(1*log0.8 + 0*log0.2) ≈ -(-0.097 + 0) = 0.097</span><br></pre></td></tr></table></figure>
<p>  由于 <code>0.222 &gt; 0.097</code>，所以预测结果 <code>y2</code> 与标准答案 <code>y_</code> 更接近，<code>y2</code> 预测更准确。</p>
</li>
<li><p><strong>√ <code>softmax</code> 函数：将 <code>n</code> 分类的 <code>n</code> 个输出 (<code>y1, y2, ..., yn</code>) 变为满足以下概率分布要求的函数。</strong></p>
  <!-- 
  $$
  \forall x\,\,P\left( X=x \right) \in \left[ \text{0,}1 \right] \,\,and\,\,\sum{P_x\left( X=x \right)}=1
  $$ 
  -->
<p>  <img src="/2018/01/06/ai-dl/eq-02.svg" alt=""></p>
<p>  <strong><code>softmax</code> 函数表示为</strong>：<img src="/2018/01/06/ai-dl/in-eq-04.svg" alt="&quot;$\mathbf{softmax}\left( y_i \right) =\frac{e^{yi}}{\sum_{j=1}^n{e^{yj}}}$&quot;"></p>
<p>  <code>softmax</code> 函数应用：在 <code>n</code> 分类中，模型会有 <code>n</code> 个输出，即 <code>y1, y2, ..., yn</code>，其中 <code>yi</code> 表示第 <code>i</code> 种情况出现的可能性大小。将 <code>n</code> 个输出经过 <code>softmax</code> 函数，可得到符合概率分布的分类结果。</p>
</li>
<li><p><strong>√ 在 Tensorflow 中，一般让模型的输出经过 <code>sofemax</code> 函数，以获得输出分类的概率分布，再与标准答案对比，求出交叉熵，得到损失函数，用如下函数实现：</strong>  </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ce = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">    logits=y, </span><br><span class="line">    labels=tf.argmax(y_, <span class="number">1</span>)</span><br><span class="line">) </span><br><span class="line">cem = tf.reduce_mean(ce)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2><ul>
<li><strong>√ 学习率 learning_rate：表示了每次参数更新的幅度大小。</strong><br>  <strong>学习率过大，会导致待优化的参数在最小值附近波动，不收敛；</strong><br>  <strong>学习率过小，会导致待优化的参数收敛缓慢。在训练过程中，参数的更新向着损失函数梯度下降的方向。</strong>  </li>
</ul>
<p><strong>参数的更新公式为</strong>：</p>
<!-- 
$$
w_{n+1}=w_n-learning\_rate\nabla 
$$ 
-->
<p><img src="/2018/01/06/ai-dl/eq-03.svg" alt=""></p>
<p>假设损失函数为 <img src="/2018/01/06/ai-dl/in-eq-loss.svg" alt="&quot;$loss=\left( w+1 \right) ^2$&quot;">。梯度是损失函数 <code>loss</code> 的导数为 <code>∇=2w+2</code>。如参数初值为 5，学习率为 0.2，则参数和损失函数更新如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1 次参数 w: 5          5 - 0.2 * (2 * 5 + 2) = 2.6</span><br><span class="line">2 次参数 w: 2.6      2.6 - 0.2 * (2 * 2.6 + 2) = 1.16</span><br><span class="line">3 次参数 w: 1.16    1.16 - 0.2 * (2 * 1.16 + 2) = 0.296</span><br><span class="line">4 次参数 w: 0.296</span><br></pre></td></tr></table></figure>
<p>损失函数 <img src="/2018/01/06/ai-dl/in-eq-loss.svg" alt="&quot;$loss=\left( w+1 \right) ^2$&quot;"> 的图像为：</p>
<p><img src="/2018/01/06/ai-dl/loss.svg" alt=""></p>
<p>由图可知，损失函数 <code>loss</code> 的最小值会在 <code>(-1, 0)</code> 处得到，此时损失函数的导数为 0,得到最终参数 <code>w = -1</code>。</p>
<p>代码 <a href="./opt4_4.py"><code>opt4_4.py</code></a> 如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 设损失函数 loss=(w+1)^2, 令 w 初值是常数 5。</span></span><br><span class="line"><span class="comment"># 反向传播就是求最优 w，即求最小 loss 对应的 w 值</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义待优化参数 w 初值赋 5</span></span><br><span class="line">w = tf.Variable(tf.constant(<span class="number">5</span>, dtype=tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数 loss</span></span><br><span class="line">loss = tf.square(w+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义反向传播方法</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.2</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成会话，训练 40 轮</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op=tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">40</span>):</span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        w_val = sess.run(w)</span><br><span class="line">        loss_val = sess.run(loss)</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"After %s steps: w is %f,   loss is %f."</span> % (i, w_val,loss_val)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">After 30 steps: w is -0.999999,   loss is 0.000000.</span><br><span class="line">After 31 steps: w is -1.000000,   loss is 0.000000.</span><br><span class="line">After 32 steps: w is -1.000000,   loss is 0.000000.</span><br><span class="line">After 33 steps: w is -1.000000,   loss is 0.000000.</span><br><span class="line">After 34 steps: w is -1.000000,   loss is 0.000000.</span><br><span class="line">After 35 steps: w is -1.000000,   loss is 0.000000.</span><br><span class="line">After 36 steps: w is -1.000000,   loss is 0.000000.</span><br><span class="line">After 37 steps: w is -1.000000,   loss is 0.000000.</span><br><span class="line">After 38 steps: w is -1.000000,   loss is 0.000000.</span><br><span class="line">After 39 steps: w is -1.000000,   loss is 0.000000.</span><br></pre></td></tr></table></figure>
<p>由结果可知，随着损失函数值的减小，<code>w</code> 无限趋近于 <code>-1</code>，模型计算推测出最优参数 <code>w = -1</code>。</p>
<ul>
<li><p><strong>√ 学习率的设置</strong><br>  <strong>学习率过大，会导致待优化的参数在最小值附近波动，不收敛；</strong><br>  <strong>学习率过小，会导致待优化的参数收敛缓慢。</strong>  </p>
<p>  <strong>【例如】</strong>：  </p>
<ul>
<li><p>① 对于上例的损失函数 <img src="/2018/01/06/ai-dl/in-eq-loss.svg" alt="&quot;$loss=\left( w+1 \right) ^2$&quot;">。则将上述代码中学习率修改为 <code>1</code>，其余内容不变。(<a href="./opt4_4-1.py"><code>opt4_4-1.py</code></a>)<br>  实验结果如下：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">After 11 steps: w is 5.000000,   loss is 36.000000.</span><br><span class="line">After 12 steps: w is -7.000000,   loss is 36.000000.</span><br><span class="line">After 13 steps: w is 5.000000,   loss is 36.000000.</span><br><span class="line">After 14 steps: w is -7.000000,   loss is 36.000000.</span><br><span class="line">After 15 steps: w is 5.000000,   loss is 36.000000.</span><br><span class="line">After 16 steps: w is -7.000000,   loss is 36.000000.</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>  由运行结果可知，损失函数 <code>loss</code> 值并没有收敛，而是在 <code>5</code> 和 <code>-7</code> 之间波动。</p>
</li>
<li><p>② 对于上例的损失函数 <img src="/2018/01/06/ai-dl/in-eq-loss.svg" alt="&quot;$loss=\left( w+1 \right) ^2$&quot;">。则将上述代码中学习率修改为 <code>0.0001</code>，其余内容不变。(<a href="./opt4_4-2.py"><code>opt4_4-2.py</code></a>)<br>  实验结果如下：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">After 31 steps: w is 4.961716,   loss is 35.542053.</span><br><span class="line">After 32 steps: w is 4.960523,   loss is 35.527836.</span><br><span class="line">After 33 steps: w is 4.959331,   loss is 35.513626.</span><br><span class="line">After 34 steps: w is 4.958139,   loss is 35.499420.</span><br><span class="line">After 35 steps: w is 4.956947,   loss is 35.485222.</span><br><span class="line">After 36 steps: w is 4.955756,   loss is 35.471027.</span><br><span class="line">After 37 steps: w is 4.954565,   loss is 35.456841.</span><br><span class="line">After 38 steps: w is 4.953373,   loss is 35.442654.</span><br><span class="line">After 39 steps: w is 4.952183,   loss is 35.428478.</span><br></pre></td></tr></table></figure>
<p>  由运行结果可知，损失函数 <code>loss</code> 值缓慢下降，<code>w</code> 值也在小幅度变化，收敛缓慢。</p>
</li>
</ul>
</li>
<li><p><strong>√ 指数衰减学习率：学习率随着训练轮数变化而动态更新学习率计算公式如下</strong>：</p>
</li>
</ul>
<!-- 
$$
\begin{aligned}
Learning\_rate
=&LEARNING\_RATE\_BASE \\ 
&\times LEARNING\_RATE\_DECAY \\
&\times \frac{global\_step}{LEARNING\_RATE\_BATCH\_SIZE}
\end{aligned}
$$ 
-->
<p><img src="/2018/01/06/ai-dl/eq-04.svg" alt=""></p>
<p>用 Tensorflow 的函数表示为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>) </span><br><span class="line">learning_rate = tf.train.exponential_decay( </span><br><span class="line">    LEARNING_RATE_BASE,</span><br><span class="line">    global_step,</span><br><span class="line">    LEARNING_RATE_STEP, LEARNING_RATE_DECAY,</span><br><span class="line">    staircase=<span class="keyword">True</span>/<span class="keyword">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>其中，<br><code>LEARNING_RATE_BASE</code> 为学习率初始值，<code>LEARNING_RATE_DECAY</code> 为学习率衰减率，<code>global_step</code> 记录了当前训练轮数，为不可训练型参数。</p>
<p>学习率 <code>learning_rate</code> 更新频率为输入数据集总样本数除以每次喂入样本数。</p>
<p>若 <code>staircase</code> 设置为 <code>True</code> 时，表示 <code>global_step/learning rate step</code> 取整数，学习率阶梯型衰减；<br>若 <code>staircase</code> 设置为 <code>false</code> 时，学习率会是一条平滑下降的曲线。</p>
<p><strong>【例如】</strong>：</p>
<p>在本例中，模型训练过程不设定固定的学习率，使用指数衰减学习率进行训练。其中，学习率初值设置为 0.1，学习率衰减率设置为 0.99，<code>BATCH_SIZE</code> 设置为 1。</p>
<p>代码 <a href="./opt4_5.py"><code>opt4_5.py</code></a> 如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 设损失函数 loss=(w+1)^2, 令 w 初值是常数 10。反向传播就是求最优 w，即求最小 loss 对应的 w 值</span></span><br><span class="line"><span class="comment"># 使用指数衰减的学习率，在迭代初期得到较高的下降速度，可以在较小的训练轮数下取得更有收敛度。</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.1</span>    <span class="comment"># 最初学习率</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span>  <span class="comment"># 学习率衰减率</span></span><br><span class="line">LEARNING_RATE_STEP = <span class="number">1</span>      <span class="comment"># 喂入多少轮 BATCH_SIZE 后，更新一次学习率，一般设为：总样本数/BATCH_SIZE</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行了几轮 BATCH_SIZE 的计数器，初值给 0, 设为不被训练</span></span><br><span class="line">global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># 定义指数下降学习率</span></span><br><span class="line">learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, LEARNING_RATE_STEP, LEARNING_RATE_DECAY, staircase=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 定义待优化参数，初值给 10</span></span><br><span class="line">w = tf.Variable(tf.constant(<span class="number">5</span>, dtype=tf.float32))</span><br><span class="line"><span class="comment"># 定义损失函数 loss</span></span><br><span class="line">loss = tf.square(w+<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 定义反向传播方法</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成会话，训练 40 轮</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op=tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">40</span>):</span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        learning_rate_val = sess.run(learning_rate)</span><br><span class="line">        global_step_val = sess.run(global_step)</span><br><span class="line">        w_val = sess.run(w)</span><br><span class="line">        loss_val = sess.run(loss)</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"After %s steps: global_step is %f, w is %f, learning rate is %f, loss is %f"</span> % (i, global_step_val, w_val, learning_rate_val, loss_val)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">After 35 steps: global_step is 36.000000, w is -0.992297, learning rate is 0.069641, loss is 0.000059</span><br><span class="line">After 36 steps: global_step is 37.000000, w is -0.993369, learning rate is 0.068945, loss is 0.000044</span><br><span class="line">After 37 steps: global_step is 38.000000, w is -0.994284, learning rate is 0.068255, loss is 0.000033</span><br><span class="line">After 38 steps: global_step is 39.000000, w is -0.995064, learning rate is 0.067573, loss is 0.000024</span><br><span class="line">After 39 steps: global_step is 40.000000, w is -0.995731, learning rate is 0.066897, loss is 0.000018</span><br></pre></td></tr></table></figure>
<p>由结果可以看出，随着训练轮数增加学习率在不断减小。</p>
<h2 id="滑动平均"><a href="#滑动平均" class="headerlink" title="滑动平均"></a>滑动平均</h2><ul>
<li><p><strong>√ 滑动平均：记录了一段时间内模型中所有参数 <code>w</code> 和 <code>b</code> 各自的平均值。利用滑动平均值可以增强模型的泛化能力。</strong></p>
</li>
<li><p><strong>√ 滑动平均值(影子)计算公式</strong>：<br>  <code>影子 = 衰减率*影子 + (1-衰减率)*参数</code></p>
<p>  其中，</p>
  <!-- 
  $$
  Decay\_Rate=\min \left\{ MOVING_{AVERAGE\_DECAY}, \frac{1+Round}{10+Round} \right\} ,
  \\
  Shadow\_Initial\_Value=Parameters\_Initial\_Value
  $$ 
  -->
<p>  <img src="/2018/01/06/ai-dl/eq-05.svg" alt=""></p>
</li>
<li><p><strong>√ 用 Tesnsorflow 函数表示为</strong>：</p>
</li>
<li><p>√ <code>ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY，global_step)</code><br>  其中，<code>MOVING_AVERAGE_DECAY</code> 表示滑动平均衰减率，一般会赋接近 1 的值，<code>global_step</code> 表示当前训练了多少轮。</p>
</li>
<li><p>√ <code>ema_op = ema.apply(tf.trainable_variables())</code><br>  其中，<code>ema.apply()</code> 函数实现对括号内参数求滑动平均，  <code>tf.trainable_variables()</code> 函数实现把所有待训练参数汇总为列表。</p>
</li>
<li><p>√ </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">        train_op = tf.no_op(name=<span class="string">'train'</span>)</span><br><span class="line">    ```  </span><br><span class="line">    其中，该函数实现将滑动平均和训练过程同步运行。</span><br><span class="line"></span><br><span class="line">查看模型中参数的平均值，可以用 `ema.average()` 函数。 </span><br><span class="line"></span><br><span class="line">**【例如】**：</span><br><span class="line"></span><br><span class="line">在神经网络模型中，将 `MOVING_AVERAGE_DECAY` 设置为 <span class="number">0.99</span>，参数 `w1` 设置为 <span class="number">0</span>，`w1` 的滑动平均值设置为 <span class="number">0</span>。</span><br><span class="line"></span><br><span class="line">- ① 开始时，轮数 `global_step` 设置为 <span class="number">0</span>，参数 `w1` 更新为 <span class="number">1</span>，则 `w1` 的滑动平均值为：  </span><br><span class="line">    `w1 滑动平均值= min(<span class="number">0.99</span>,<span class="number">1</span>/<span class="number">10</span>)*<span class="number">0</span>+(<span class="number">1</span>–min(<span class="number">0.99</span>,<span class="number">1</span>/<span class="number">10</span>)*<span class="number">1</span>= <span class="number">0.9</span>`</span><br><span class="line">- ② 当轮数 `global_step` 设置为 <span class="number">100</span> 时，参数 `w1` 更新为 <span class="number">10</span>，以下代码 `global_step` 保持为 <span class="number">100</span>，每次执行滑动平均操作影子值更新，则滑动平均值变为：  </span><br><span class="line">    `w1 滑动平均值= min(<span class="number">0.99</span>,<span class="number">101</span>/<span class="number">110</span>)*<span class="number">0.9</span>+(<span class="number">1</span>–min(<span class="number">0.99</span>,<span class="number">101</span>/<span class="number">110</span>)*<span class="number">10</span>= <span class="number">0.826</span>+<span class="number">0.818</span>= <span class="number">1.644</span>`</span><br><span class="line">- ③ 再次运行，参数 `w1` 更新为 <span class="number">1.644</span>，则滑动平均值变为：  </span><br><span class="line">    `w1 滑动平均值= min(<span class="number">0.99</span>,<span class="number">101</span>/<span class="number">110</span>)*<span class="number">1.644</span>+(<span class="number">1</span>–min(<span class="number">0.99</span>,<span class="number">101</span>/<span class="number">110</span>)*<span class="number">10</span>= <span class="number">2.328</span>`</span><br><span class="line">- ④ 再次运行，参数 `w1` 更新为 <span class="number">2.328</span>，则滑动平均值： `w1 滑动平均值=<span class="number">2.956</span>`</span><br><span class="line"></span><br><span class="line">代码 [`opt4_6.py`](./opt4_6.py) 如下：  </span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 定义变量及滑动平均类</span></span><br><span class="line"><span class="comment">#    定义一个 32 位浮点变量，初始值为 0.0  这个代码就是不断更新 w1 参数，优化 w1 参数，滑动平均做了个 w1 的影子</span></span><br><span class="line">w1 = tf.Variable(<span class="number">0</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 num_updates(NN的迭代轮数),初始值为 0，不可被优化(训练)，这个参数不训练</span></span><br><span class="line">global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化滑动平均类，给衰减率为0.99，当前轮数global_step</span></span><br><span class="line">MOVING_AVERAGE_DECAY = <span class="number">0.99</span></span><br><span class="line">ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ema.apply 后的括号里是更新列表，每次运行 sess.run(ema_op) 时，对更新列表中的元素求滑动平均值。</span></span><br><span class="line"><span class="comment"># 在实际应用中会使用 tf.trainable_variables() 自动将所有待训练的参数汇总为列表</span></span><br><span class="line"><span class="comment"># ema_op = ema.apply([w1])</span></span><br><span class="line">ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 查看不同迭代中变量取值的变化。</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 用 ema.average(w1) 获取 w1 滑动平均值 (要运行多个节点，作为列表中的元素列出，写在 sess.run 中)</span></span><br><span class="line">    <span class="comment"># 打印出当前参数 w1 和 w1 滑动平均值</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current global_step:"</span>, sess.run(global_step)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current w1"</span>, sess.run([w1, ema.average(w1)]) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 参数 w1 的值赋为 1</span></span><br><span class="line">    sess.run(tf.assign(w1, <span class="number">1</span>))</span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current global_step:"</span>, sess.run(global_step)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current w1"</span>, sess.run([w1, ema.average(w1)]) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新 global_step 和 w1 的值,模拟出轮数为 100时，参数 w1 变为 10, 以下代码 global_step 保持为 100，每次执行滑动平均操作，影子值会更新 </span></span><br><span class="line">    sess.run(tf.assign(global_step, <span class="number">100</span>))  </span><br><span class="line">    sess.run(tf.assign(w1, <span class="number">10</span>))</span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current global_step:"</span>, sess.run(global_step)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current w1:"</span>, sess.run([w1, ema.average(w1)])       </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 每次 sess.run 会更新一次 w1 的滑动平均值</span></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current global_step:"</span> , sess.run(global_step)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current w1:"</span>, sess.run([w1, ema.average(w1)])</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current global_step:"</span> , sess.run(global_step)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current w1:"</span>, sess.run([w1, ema.average(w1)])</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current global_step:"</span> , sess.run(global_step)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current w1:"</span>, sess.run([w1, ema.average(w1)])</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current global_step:"</span> , sess.run(global_step)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current w1:"</span>, sess.run([w1, ema.average(w1)])</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current global_step:"</span> , sess.run(global_step)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current w1:"</span>, sess.run([w1, ema.average(w1)])</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current global_step:"</span> , sess.run(global_step)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"current w1:"</span>, sess.run([w1, ema.average(w1)])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>运行程序，结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">current global_step: 0</span><br><span class="line">current w1 [0.0, 0.0]</span><br><span class="line">current global_step: 0</span><br><span class="line">current w1 [1.0, 0.9]</span><br><span class="line">current global_step: 100</span><br><span class="line">current w1: [10.0, 1.6445453]</span><br><span class="line">current global_step: 100</span><br><span class="line">current w1: [10.0, 2.3281732]</span><br><span class="line">current global_step: 100</span><br><span class="line">current w1: [10.0, 2.955868]</span><br><span class="line">current global_step: 100</span><br><span class="line">current w1: [10.0, 3.532206]</span><br><span class="line">current global_step: 100</span><br><span class="line">current w1: [10.0, 4.061389]</span><br><span class="line">current global_step: 100</span><br><span class="line">current w1: [10.0, 4.547275]</span><br><span class="line">current global_step: 100</span><br><span class="line">current w1: [10.0, 4.9934072]</span><br></pre></td></tr></table></figure>
<p>从运行结果可知，最初参数 <code>w1</code> 和滑动平均值都是 0；参数 <code>w1</code> 设定为 1 后，滑动平均值变为 0.9；当迭代轮数更新为 100 轮时，参数 <code>w1</code> 更新为 10 后，滑动平均值变为 1.644。随后每执行一次，参数</p>
<p><code>w1</code> 的滑动平均值都向参数 <code>w1</code> 靠近。可见，滑动平均追随参数的变化而变化。</p>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><ul>
<li>√ <strong>过拟合：神经网络模型在训练数据集上的准确率较高，在新的数据进行预测或分类时准确率较低，说明模型的泛化能力差。</strong></li>
<li>√ <strong>正则化：在损失函数中给每个参数 <code>w</code> 加上权重，引入模型复杂度指标，从而抑制模型噪声，减小过拟合。</strong></li>
</ul>
<p>使用正则化后，损失函数 <code>loss</code> 变为两项之和：</p>
<p><code>loss = loss(y 与 y_) + REGULARIZER*loss(w)</code></p>
<p>其中，第一项是预测结果与标准答案之间的差距，如之前讲过的交叉熵、均方误差等；第二项是正则化计算结果。</p>
<ul>
<li>√ <strong>正则化计算方法：</strong><ul>
<li>① L1 正则化：<img src="/2018/01/06/ai-dl/in-eq-05.svg" alt="&quot;$loss_{L1}=\sum_i{|w_i|}$&quot;"><br>用 Tesnsorflow 函数表示:<br><code>loss(w) = tf.contrib.layers.l1_regularizer(REGULARIZER)(w)</code></li>
<li>② L2 正则化：<img src="/2018/01/06/ai-dl/in-eq-06.svg" alt="&quot;$loss_{L2}=\sum_i{|w_i|^2}$&quot;"><br>用 Tesnsorflow 函数表示:<br><code>loss(w) = tf.contrib.layers.l2_regularizer(REGULARIZER)(w)</code></li>
</ul>
</li>
<li><p>√ 用 Tesnsorflow 函数实现正则化：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.add_to_collection(</span><br><span class="line">    <span class="string">'losses'</span>, </span><br><span class="line">    tf.contrib.layers.l2_regularizer(regularizer)(w)</span><br><span class="line">)</span><br><span class="line">loss = cem + tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br></pre></td></tr></table></figure>
<p>  <code>cem</code> 的计算已在 4.1 节中给出。 </p>
</li>
</ul>
<p><strong>【例如】</strong>：<br>用 300 个符合正态分布的点 <img src="/2018/01/06/ai-dl/in-eq-07.svg" alt="&quot;$X\left[ x_0, x_1 \right]$&quot;"> 作为数据集，根据点 <img src="/2018/01/06/ai-dl/in-eq-07.svg" alt="&quot;$X\left[ x_0, x_1 \right]$&quot;"> 计算生成标注 <code>Y_</code>，将数据集标注为红色点和蓝色点。</p>
<p>标注规则为：当 <img src="/2018/01/06/ai-dl/in-eq-08.svg" alt="&quot;$x_{0}^{2}+x_{1}^{2}&lt;2$&quot;"> 时，<code>y_=1</code>，标注为红色；当 <img src="/2018/01/06/ai-dl/in-eq-09.svg" alt="&quot;$x_{0}^{2}+x_{1}^{2}\ge 2$&quot;"> 时，<code>y_=0</code>，标注为蓝色。 我们分别用无正则化和有正则化两种方法，拟合曲线，把红色点和蓝色点分开。在实际分类时，</p>
<p>如果前向传播输出的预测值 <code>y</code> 接近 1 则为红色点概率越大，接近 0 则为蓝色点概率越大，输出的预测值 <code>y</code> 为 0.5 是红蓝点概率分界线。</p>
<p>在本例子中，我们使用了之前未用过的模块与函数：</p>
<ul>
<li>√ <strong><code>matplotlib</code> 模块：Python 中的可视化工具模块，实现函数可视化<br>  终端安装指令</strong>：<code>sudo pip install matplotlib</code></li>
<li><p>√ <strong>函数 <code>plt.scatter()</code>：利用指定颜色实现点 (x,y) 的可视化</strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x 坐标, y 坐标, c=<span class="string">"颜色"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>√ <strong>收集规定区域内所有的网格坐标点</strong>：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xx, yy = np.mgrid[起:止:步长, 起:止:步长]   <span class="comment"># 找到规定区域以步长为分辨率的行列网格坐标点 </span></span><br><span class="line">grid = np.c_[xx.ravel(), yy.ravel()]       <span class="comment"># 收集规定区域内所有的网格坐标点</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>√ <strong><code>plt.contour()</code> 函数：告知 x、y 坐标和各点高度，用 levels 指定高度的点描上颜色</strong> </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.contour(x 轴坐标值, y 轴坐标值, 该点的高度, levels=[等高线的高度]) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>本例 <a href="./opt4_7.py"><code>opt4_7.py</code></a> 代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 0 导入模块 ，生成模拟数据集</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">30</span> </span><br><span class="line">seed = <span class="number">2</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于 seed 产生随机数</span></span><br><span class="line">rdm = np.random.RandomState(seed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机数返回 300 行 2 列的矩阵，</span></span><br><span class="line"><span class="comment"># 表示 300 组坐标点 (x0, x1) 作为输入数据集</span></span><br><span class="line">X = rdm.randn(<span class="number">300</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从 X 这个 300 行 2 列的矩阵中取出一行,</span></span><br><span class="line"><span class="comment"># 判断如果两个坐标的平方和小于 2，给 Y 赋值 1，其余赋值 0</span></span><br><span class="line"><span class="comment"># 作为输入数据集的标签(正确答案)</span></span><br><span class="line">Y_ = [int(x0*x0 + x1*x1 &lt;<span class="number">2</span>) <span class="keyword">for</span> (x0,x1) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历 Y 中的每个元素，1 赋值 'red' 其余赋值 'blue'，</span></span><br><span class="line"><span class="comment"># 这样可视化显示时人可以直观区分</span></span><br><span class="line">Y_c = [[<span class="string">'red'</span> <span class="keyword">if</span> y <span class="keyword">else</span> <span class="string">'blue'</span>] <span class="keyword">for</span> y <span class="keyword">in</span> Y_]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数据集 X 和标签 Y 进行 shape 整理，</span></span><br><span class="line"><span class="comment"># 第一个元素为 -1 表示，随第二个参数计算得到，第二个元素表示多少列，</span></span><br><span class="line"><span class="comment"># 把 X 整理为 n 行 2 列，把 Y 整理为 n 行 1 列</span></span><br><span class="line">X = np.vstack(X).reshape(<span class="number">-1</span>,<span class="number">2</span>)</span><br><span class="line">Y_ = np.vstack(Y_).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="keyword">print</span> X</span><br><span class="line"><span class="keyword">print</span> Y_</span><br><span class="line"><span class="keyword">print</span> Y_c</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用 plt.scatter 画出数据集 X 各行中第 0 列元素和第 1 列元素的点即各行的 (x0, x1)，</span></span><br><span class="line"><span class="comment"># 用各行 Y_c 对应的值表示颜色(c 是 color 的缩写) </span></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=np.squeeze(Y_c)) </span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络的输入、参数和输出，定义前向传播过程 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, regularizer)</span>:</span></span><br><span class="line">    w = tf.Variable(tf.random_normal(shape), dtype=tf.float32)</span><br><span class="line">    tf.add_to_collection(<span class="string">'losses'</span>, tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span>:</span>  </span><br><span class="line">    b = tf.Variable(tf.constant(<span class="number">0.01</span>, shape=shape)) </span><br><span class="line">    <span class="keyword">return</span> b</span><br><span class="line">    </span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">w1 = get_weight([<span class="number">2</span>,<span class="number">11</span>], <span class="number">0.01</span>)    </span><br><span class="line">b1 = get_bias([<span class="number">11</span>])</span><br><span class="line">y1 = tf.nn.relu(tf.matmul(x, w1)+b1)</span><br><span class="line"></span><br><span class="line">w2 = get_weight([<span class="number">11</span>,<span class="number">1</span>], <span class="number">0.01</span>)</span><br><span class="line">b2 = get_bias([<span class="number">1</span>])</span><br><span class="line">y = tf.matmul(y1, w2)+b2 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line">loss_mse = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">loss_total = loss_mse + tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义反向传播方法：不含正则化</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">0.0001</span>).minimize(loss_mse)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = <span class="number">40000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE) % <span class="number">300</span></span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x:X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            loss_mse_v = sess.run(loss_mse, feed_dict=&#123;x:X, y_:Y_&#125;)</span><br><span class="line">            print(<span class="string">"After %d steps, loss is: %f"</span> %(i, loss_mse_v))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># xx 在 -3 到 3 之间以步长为 0.01，yy 在 -3 到 3 之间以步长 0.01,生成二维网格坐标点</span></span><br><span class="line">    xx, yy = np.mgrid[<span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>, <span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将 xx, yy 拉直，并合并成一个 2 列的矩阵，得到一个网格坐标点的集合</span></span><br><span class="line">    grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将网格坐标点喂入神经网络 ，probs 为输出</span></span><br><span class="line">    probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># probs 的 shape 调整成 xx 的样子</span></span><br><span class="line">    probs = probs.reshape(xx.shape)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w1:\n"</span>,sess.run(w1)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"b1:\n"</span>,sess.run(b1)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w2:\n"</span>,sess.run(w2)    </span><br><span class="line">    <span class="keyword">print</span> <span class="string">"b2:\n"</span>,sess.run(b2)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=np.squeeze(Y_c))</span><br><span class="line">plt.contour(xx, yy, probs, levels=[<span class="number">.5</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义反向传播方法：包含正则化</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">0.0001</span>).minimize(loss_total)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = <span class="number">40000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i*BATCH_SIZE) % <span class="number">300</span></span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x: X[start:end], y_:Y_[start:end]&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            loss_v = sess.run(loss_total, feed_dict=&#123;x:X,y_:Y_&#125;)</span><br><span class="line">            print(<span class="string">"After %d steps, loss is: %f"</span> %(i, loss_v))</span><br><span class="line"></span><br><span class="line">    xx, yy = np.mgrid[<span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>, <span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>]</span><br><span class="line">    grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">    probs = probs.reshape(xx.shape)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w1:\n"</span>,sess.run(w1)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"b1:\n"</span>,sess.run(b1)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w2:\n"</span>,sess.run(w2)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"b2:\n"</span>,sess.run(b2)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=np.squeeze(Y_c)) </span><br><span class="line">plt.contour(xx, yy, probs, levels=[<span class="number">.5</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>执行代码，效果如下：</p>
<p>首先，数据集实现可视化，<img src="/2018/01/06/ai-dl/in-eq-08.svg" alt="&quot;$x_{0}^{2}+x_{1}^{2}&lt;2$&quot;"> 的点显示红色， <img src="/2018/01/06/ai-dl/in-eq-09.svg" alt="&quot;$x_{0}^{2}+x_{1}^{2}\ge 2$&quot;"> 的点显示蓝色，如图所示：</p>
<p><img src="/2018/01/06/ai-dl/4.4-1.svg" alt=""></p>
<p>接着，执行无正则化的训练过程，把红色的点和蓝色的点分开，生成曲线如下图所示：</p>
<p><img src="/2018/01/06/ai-dl/4.4-2.svg" alt=""></p>
<p>最后，执行有正则化的训练过程，把红色的点和蓝色的点分开，生成曲线如下图所示：</p>
<p><img src="/2018/01/06/ai-dl/4.4-3.svg" alt=""></p>
<p>对比无正则化与有正则化模型的训练结果，可看出有正则化模型的拟合曲线平滑，模型具有更好的泛化能力。</p>
<h2 id="搭建模块化神经网络八股"><a href="#搭建模块化神经网络八股" class="headerlink" title="搭建模块化神经网络八股"></a>搭建模块化神经网络八股</h2><ul>
<li><p>√ <strong>前向传播：由输入到输出，搭建完整的网络结构描述前向传播的过程需要定义三个函数</strong>：</p>
<ul>
<li><p><strong>√ <code>forward()</code></strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, regularizer)</span>:</span> </span><br><span class="line">    w=</span><br><span class="line">    b= </span><br><span class="line">    y=</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<p>  第一个函数 <code>forward()</code> 完成网络结构的设计，从输入到输出搭建完整的网络结构，实现前向传播过程。该函数中，参数 x 为输入，<code>regularizer</code> 为正则化权重，返回值为预测或分类结果 y。</p>
</li>
<li><p><strong>√ <code>get_weight()</code></strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, regularizer)</span>:</span></span><br><span class="line">    w = tf.Variable( )</span><br><span class="line">    tf.add_to_collection(</span><br><span class="line">        <span class="string">'losses'</span>, </span><br><span class="line">        tf.contrib.layers.l2_regularizer(regularizer)(w)</span><br><span class="line">    ) </span><br><span class="line">    <span class="keyword">return</span> w</span><br></pre></td></tr></table></figure>
<p>  第二个函数 <code>get_weight()</code> 对参数 w 设定。该函数中，参数 <code>shape</code> 表示参数 w 的形状，<code>regularizer</code> 表示正则化权重，返回值为参数 w。<br>其中，<code>tf.variable()</code> 给 w 赋初值，<code>tf.add_to_collection()</code> 表示将参数 w 正则化损失加到总损失 <code>losses</code> 中。</p>
</li>
<li><p><strong>√ <code>get_bias()</code></strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span>:</span></span><br><span class="line">    b = tf.Variable( ) </span><br><span class="line">    <span class="keyword">return</span> b</span><br></pre></td></tr></table></figure>
<p>  第三个函数 <code>get_bias()</code> 对参数 b 进行设定。该函数中，参数 <code>shape</code> 表示参数 b 的形状,返回值为参数b。其中，<code>tf.variable()</code>表示给 b 赋初值。</p>
</li>
</ul>
</li>
<li><p>√ <strong>反向传播：训练网络，优化网络参数，提高模型准确性。</strong></p>
<ul>
<li><p><strong>√ <code>backward()</code></strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">( )</span>:</span></span><br><span class="line">    x = tf.placeholder( ) </span><br><span class="line">    y_ = tf.placeholder( )</span><br><span class="line">    y = forward.forward(x, REGULARIZER) </span><br><span class="line">    global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>) </span><br><span class="line">    loss =</span><br></pre></td></tr></table></figure>
<p>函数 <code>backward()</code> 中，<code>placeholder()</code> 实现对数据集 x 和标准答案 <code>y_</code> 占位，<code>forward.forward()</code> 实现前向传播的网络结构，参数 <code>global_step</code> 表示训练轮数，设置为不可训练型参数。</p>
</li>
</ul>
</li>
</ul>
<p>在训练网络模型时，常将正则化、指数衰减学习率和滑动平均这三个方法作为模型优化方法。</p>
<ul>
<li><p>√ 在 Tensorflow 中，<strong>正则化</strong> 表示为： </p>
<p>  首先，计算预测结果与标准答案的损失值</p>
<ul>
<li>① MSE：<code>y 与 y_ 的差距(loss_mse) = tf.reduce_mean(tf.square(y-y_))</code></li>
<li><p>② 交叉熵：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">y 与 y_的差距(cem) = tf.reduce_mean(ce)</span><br></pre></td></tr></table></figure>
</li>
<li><p>③ 自定义：<code>y</code> 与 <code>y_</code> 的差距  </p>
<p>其次，总损失值为预测结果与标准答案的损失值加上正则化项<br><code>loss = y 与 y_的差距 + tf.add_n(tf.get_collection(&#39;losses&#39;))</code></p>
</li>
</ul>
</li>
<li><p>√ 在 Tensorflow 中，<strong>指数衰减学习率</strong> 表示为： </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = tf.train.exponential_decay(</span><br><span class="line">    LEARNING_RATE_BASE,</span><br><span class="line">    global_step,</span><br><span class="line">    数据集总样本数 / BATCH_SIZE,</span><br><span class="line">    LEARNING_RATE_DECAY,</span><br><span class="line">    staircase=<span class="keyword">True</span></span><br><span class="line">) </span><br><span class="line">train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br></pre></td></tr></table></figure>
</li>
<li><p>√ 在 Tensorflow 中，<strong>滑动平均</strong> 表示为：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ema = tf.train.ExponentialMovingAverage(</span><br><span class="line">    MOVING_AVERAGE_DECAY, </span><br><span class="line">    global_step</span><br><span class="line">) </span><br><span class="line">ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">    train_op = tf.no_op(name=<span class="string">'train'</span>)</span><br></pre></td></tr></table></figure>
<p>  其中，滑动平均和指数衰减学习率中的 <code>global_step</code> 为同一个参数。</p>
</li>
<li><p>√ <strong>用 <code>with</code> 结构初始化所有参数</strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x: , y_: &#125;)</span><br><span class="line">        <span class="keyword">if</span> i % 轮数 == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span></span><br></pre></td></tr></table></figure>
<p>  其中，<code>with</code> 结构用于初始化所有参数信息以及实现调用训练过程，并打印出 <code>loss</code> 值。</p>
</li>
<li><p>√ <strong>判断 python 运行文件是否为主文件</strong><br>  <code>if name ==&#39; main__&#39;: backward()</code><br>  该部分用来判断 python 运行的文件是否为主文件。若是主文件，则执行 <code>backword()</code> 函数。 </p>
</li>
</ul>
<p><strong>【例如】</strong>：</p>
<p>用 300 个符合正态分布的点 <img src="/2018/01/06/ai-dl/in-eq-07.svg" alt="&quot;$X\left[ x_0, x_1 \right]$&quot;"> 作为数据集，根据点 <img src="/2018/01/06/ai-dl/in-eq-07.svg" alt="&quot;$X\left[ x_0, x_1 \right]$&quot;"> 计算生成标注 <code>Y_</code>，将数据集标注为红色点和蓝色点。</p>
<p>标注规则为：当 <img src="/2018/01/06/ai-dl/in-eq-08.svg" alt="&quot;$x_{0}^{2}+x_{1}^{2}&lt;2$&quot;"> 时，<code>y_=1</code>，点 <code>X</code> 标注为红色；当 <img src="/2018/01/06/ai-dl/in-eq-09.svg" alt="&quot;$x_{0}^{2}+x_{1}^{2}\ge 2$&quot;"> 时，<code>y_=0</code>，点 <code>X</code> 标注为蓝色。 </p>
<p>我们加入指数衰减学习率优化效率，加入正则化提高泛化性，并使用模块化设计方法，把红色点和蓝色点分开。</p>
<p>代码总共分为三个模块：<br>生成数据集(<a href="./opt4_8_generateds.py"><code>generateds.py</code></a>)、<br>前向传播(<a href="./opt4_8_forward.py"><code>forward.py</code></a>)、<br>反向传播(<a href="./opt4_8_backward.py"><code>backward.py</code></a>)。</p>
<ul>
<li><p>① 生成数据集的模块 (<a href="./opt4_8_generateds.py"><code>generateds.py</code></a>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 0 导入模块 ，生成模拟数据集</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">seed = <span class="number">2</span> </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateds</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 基于seed产生随机数</span></span><br><span class="line">    rdm = np.random.RandomState(seed)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机数返回 300 行 2 列的矩阵</span></span><br><span class="line">    <span class="comment"># 表示 300 组坐标点 (x0,x1)作为输入数据集</span></span><br><span class="line">    X = rdm.randn(<span class="number">300</span>,<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从 X 这个 300 行 2 列的矩阵中取出一行</span></span><br><span class="line">    <span class="comment"># 判断如果两个坐标的平方和小于 2，给 Y 赋值 1，其余赋值 0</span></span><br><span class="line">    <span class="comment"># 作为输入数据集的标签(正确答案)</span></span><br><span class="line">    Y_ = [int(x0*x0 + x1*x1 &lt;<span class="number">2</span>) <span class="keyword">for</span> (x0,x1) <span class="keyword">in</span> X]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 遍历Y中的每个元素，1 赋值 'red' 其余赋值 'blue'</span></span><br><span class="line">    <span class="comment"># 这样可视化显示时人可以直观区分</span></span><br><span class="line">    Y_c = [[<span class="string">'red'</span> <span class="keyword">if</span> y <span class="keyword">else</span> <span class="string">'blue'</span>] <span class="keyword">for</span> y <span class="keyword">in</span> Y_]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对数据集 X 和标签 Y 进行形状整理</span></span><br><span class="line">    <span class="comment"># 第一个元素为 -1 表示跟随第二列计算</span></span><br><span class="line">    <span class="comment"># 第二个元素表示多少列，可见 X 为两列，Y 为 1 列</span></span><br><span class="line">    X = np.vstack(X).reshape(<span class="number">-1</span>,<span class="number">2</span>)</span><br><span class="line">    Y_ = np.vstack(Y_).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, Y_, Y_c</span><br><span class="line">    </span><br><span class="line"><span class="comment"># print X</span></span><br><span class="line"><span class="comment"># print Y_</span></span><br><span class="line"><span class="comment"># print Y_c</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 用 plt.scatter 画出数据集 X 各行中</span></span><br><span class="line"><span class="comment"># # 第 0 列元素和第 1 列元素的点即各行的(x0，x1)，</span></span><br><span class="line"><span class="comment"># # 用各行 Y_c对应的值表示颜色(c 是 color 的缩写) </span></span><br><span class="line"><span class="comment"># plt.scatter(X[:,0], X[:,1], c=np.squeeze(Y_c)) </span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>② 前向传播模块 (<a href="./opt4_8_forward.py"><code>forward.py</code></a>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 0 导入模块 ，生成模拟数据集</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络的输入、参数和输出，定义前向传播过程 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, regularizer)</span>:</span></span><br><span class="line">    w = tf.Variable(tf.random_normal(shape), dtype=tf.float32)</span><br><span class="line">    tf.add_to_collection(</span><br><span class="line">        <span class="string">'losses'</span>, </span><br><span class="line">        tf.contrib.layers.l2_regularizer(regularizer)(w)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span>:</span>  </span><br><span class="line">    b = tf.Variable(tf.constant(<span class="number">0.01</span>, shape=shape)) </span><br><span class="line">    <span class="keyword">return</span> b</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, regularizer)</span>:</span></span><br><span class="line">    </span><br><span class="line">    w1 = get_weight([<span class="number">2</span>,<span class="number">11</span>], regularizer)    </span><br><span class="line">    b1 = get_bias([<span class="number">11</span>])</span><br><span class="line">    y1 = tf.nn.relu(tf.matmul(x, w1) + b1)</span><br><span class="line"></span><br><span class="line">    w2 = get_weight([<span class="number">11</span>,<span class="number">1</span>], regularizer)</span><br><span class="line">    b2 = get_bias([<span class="number">1</span>])</span><br><span class="line">    y = tf.matmul(y1, w2) + b2 </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
</li>
<li><p>③ 反向传播模块 (<a href="./opt4_8_backward.py"><code>backward.py</code></a>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 0 导入模块 ，生成模拟数据集</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> opt4_8_generateds</span><br><span class="line"><span class="keyword">import</span> opt4_8_forward</span><br><span class="line"></span><br><span class="line">STEPS = <span class="number">40000</span></span><br><span class="line">BATCH_SIZE = <span class="number">30</span> </span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.001</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.999</span></span><br><span class="line">REGULARIZER = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">()</span>:</span></span><br><span class="line">    x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>))</span><br><span class="line">    y_ = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    X, Y_, Y_c = opt4_8_generateds.generateds()</span><br><span class="line"></span><br><span class="line">    y = opt4_8_forward.forward(x, REGULARIZER)</span><br><span class="line">    </span><br><span class="line">    global_step = tf.Variable(<span class="number">0</span>,trainable=<span class="keyword">False</span>)    </span><br><span class="line"></span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE,</span><br><span class="line">        global_step,</span><br><span class="line">        <span class="number">300</span>/BATCH_SIZE,</span><br><span class="line">        LEARNING_RATE_DECAY,</span><br><span class="line">        staircase=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义损失函数</span></span><br><span class="line">    loss_mse = tf.reduce_mean(tf.square(y-y_))</span><br><span class="line">    loss_total = loss_mse + tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义反向传播方法：包含正则化</span></span><br><span class="line">    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss_total)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        init_op = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">            start = (i*BATCH_SIZE) % <span class="number">300</span></span><br><span class="line">            end = start + BATCH_SIZE</span><br><span class="line">            sess.run(</span><br><span class="line">                train_step, </span><br><span class="line">                feed_dict=&#123;x: X[start:end], y_:Y_[start:end]&#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">                loss_v = sess.run(loss_total, feed_dict=&#123;x:X,y_:Y_&#125;)</span><br><span class="line">                print(<span class="string">"After %d steps, loss is: %f"</span> %(i, loss_v))</span><br><span class="line"></span><br><span class="line">        xx, yy = np.mgrid[<span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>, <span class="number">-3</span>:<span class="number">3</span>:<span class="number">.01</span>]</span><br><span class="line">        grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">        probs = sess.run(y, feed_dict=&#123;x:grid&#125;)</span><br><span class="line">        probs = probs.reshape(xx.shape)</span><br><span class="line">    </span><br><span class="line">    plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=np.squeeze(Y_c)) </span><br><span class="line">    plt.contour(xx, yy, probs, levels=[<span class="number">.5</span>])</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    backward()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>运行代码，结果如下：</p>
<p><img src="/2018/01/06/ai-dl/4.8.svg" alt=""></p>
<p>由运行结果可见，程序使用模块化设计方法，加入指数衰减学习率，使用正则化后，红色点和蓝色点的分割曲线相对平滑，效果变好。</p>
<p><br><br><br></p>
<h1 id="搭建神经网络-mnist-数据集上训练模型-输出手写数字识别准确率"><a href="#搭建神经网络-mnist-数据集上训练模型-输出手写数字识别准确率" class="headerlink" title="搭建神经网络(mnist 数据集上训练模型,输出手写数字识别准确率)"></a>搭建神经网络(mnist 数据集上训练模型,输出手写数字识别准确率)</h1><h2 id="MNIST数据集"><a href="#MNIST数据集" class="headerlink" title="MNIST数据集"></a>MNIST数据集</h2><ul>
<li><p><strong>√ mnist 数据集</strong>：<br>  <strong>包含 <code>7</code> 万张黑底白字手写数字图片，其中 <code>55000</code> 张为训练集， <code>5000</code> <code>张为验证集，10000</code> 张为测试集。</strong><br>  每张图片大小为 <code>28*28</code> 像素，图片中纯黑色像素值为 <code>0</code>，纯白色像素值为 <code>1</code>。<br>  数据集的标签是长度为 <code>10</code> 的一维数组，数组中每个元素索引号表示对应数字出现的概率。   </p>
<p>  在将 mnist 数据集作为输入喂入神经网络时，需先将数据集中每张图片变为长度 <code>784</code> 一维数组，将该数组作为神经网络输入特征喂入神经网络。</p>
<p>  <strong>【例如】</strong>：<br>  一张数字手写体图片变成长度为 <code>784</code> 的一维数组 <code>[0. 0. 0. 0. 0.231 0.235 0.459 ... 0.219 0. 0. 0. 0.]</code> 输入神经网络。<br>  该图片对应的标签为 <code>[0. 0. 0. 0. 0. 0. 1. 0. 0. 0]</code>，标签中索引号为 6 的元素为 1，表示是数字 6 出现的概率为 <code>100%</code>，则该图片对应的识别结果是 6。</p>
</li>
<li><p><strong>√ 使用 <code>input_data</code> 模块中的 <code>read_data_sets()</code> 函数加载 mnist 数据集</strong>：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data </span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'./data/'</span>,one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>  在 <code>read_data_sets()</code> 函数中有两个参数，第一个参数表示数据集存放路径，第二个参数表示数据集的存取形式。<br>  当第二个参数为 <code>Ture</code> 时，表示以独热码形式存取数据集。<code>read_data_sets()</code> 函数运行时，会检查指定路径内是否已经有数据集，<br>  若指定路径中没有数据集，则自动下载，并将 mnist 数据集分为训练集 train、验证集 validation 和测试集 test 存放。在终端显示如下内容：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Extracting ./data/train-images-idx3-ubyte.gz </span><br><span class="line">Extracting ./data/train-labels-idx1-ubyte.gz</span><br><span class="line">Extracting ./data/tl0k-images-idx3-ubyte.gz </span><br><span class="line">Extracting ./data/ tl0k-labels-idx1-ubyte.gz</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>√ 返回 mnist 数据集中训练集 train、验证集 validation 和测试集 test 样本数</strong><br>  在 Tensorflow 中用以下函数返回子集样本数：  </p>
<ul>
<li><p><strong>①返回训练集 train 样本数</strong><br>  <code>print &quot;train data size:&quot;, mnist.train.mun_examples</code><br>  输出结果：<code>train data size:55000</code></p>
</li>
<li><p><strong>② 返回验证集 validation 样本数</strong><br>  <code>print &quot;validation data size:&quot;, mnist.validation.mun_examples</code><br>  输出结果：<code>validation data size:5000</code></p>
</li>
<li><p><strong>③ 返回测试集 test 样本数</strong><br>  <code>print &quot;test data size:&quot;, mnist.test.mun_examples</code><br>  输出结果：<code>test data size:10000</code></p>
</li>
</ul>
</li>
<li><p><strong>√ 使用 <code>train.labels</code> 函数返回 mnist 数据集标签</strong><br>  <strong>【例如】</strong>：<br>  在 mnist 数据集中，若想要查看训练集中第 0 张图片的标签，则使用如下函数 <code>mnist.train.labels[0]</code><br>  输出结果：<code>array([0.,0.,0.,0.,0.,0.,1.,0.,0.,0])</code></p>
</li>
<li><p><strong>√ 使用 <code>train.images</code> 函数返回 mnist 数据集图片像素值</strong><br>  <strong>【例如】</strong>：<br>  在 mnist 数据集中，若想要查看训练集中第 0 张图片像素值，则使用如下函数 <code>mnist.train.images[0]</code><br>  输出结果：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">array([0. ,0. ,0. ,</span><br><span class="line">       1. ,0. ,0. ,</span><br><span class="line">       2. ,0. ,0. ,</span><br><span class="line">       …   …   …])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>√ 使用 mnist.train.next_batch()函数将数据输入神经网络</strong><br>  <strong>【例如】</strong>：   </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">200</span></span><br><span class="line">xs,ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"xs shape:"</span>,xs.shape </span><br><span class="line"><span class="keyword">print</span> <span class="string">"ys shape:"</span>,ys.shape</span><br></pre></td></tr></table></figure>
<p>  输出结果：<code>xs.shape(200,784)</code><br>  输出结果：<code>ys.shape(200,10)</code></p>
<p>  其中，<code>mnist.train.next_batch()</code> 函数包含一个参数 <code>BATCH_SIZE</code>，表示随机从训练集中抽取 <code>BATCH_SIZE</code> 个样本输入神经网络，并将样本的像素值和标签分别赋给 <code>xs</code> 和 <code>ys</code>。</p>
<p>  在本例中，<code>BATCH_SIZE</code> 设置为 200，表示一次将 200 个样本的像素值和标签分别赋值给 <code>xs</code> 和 <code>ys</code>，故 <code>xs</code> 的形状为(200,784)，对应的 <code>ys</code> 的形状为 (200,10)。</p>
</li>
<li><p><strong>√ 实现”Mnist 数据集手写数字识别”的常用函数</strong>： </p>
<ul>
<li><p><strong>① <code>tf.get_collection(&quot; &quot;)</code> 函数表示从 <code>collection</code> 集合中取出全部变量生成一个列表</strong>。</p>
</li>
<li><p><strong>② <code>tf.add( )</code> 函数表示将参数列表中对应元素相加</strong>。<br>  <strong>【例如】</strong>：  </p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=tf.constant([[1,2],[1,2]])</span><br><span class="line">y=tf.constant([[1,1],[1,2]]) </span><br><span class="line">z=tf.add(x,y)</span><br><span class="line">print z</span><br></pre></td></tr></table></figure>
<p>  输出结果：<br>  <code>[[2,3],[2,4]]</code></p>
</li>
<li><p><strong>③ <code>tf.cast(x,dtype)</code> 函数表示将参数 <code>x</code> 转换为指定数据类型</strong>。<br>  <strong>【例如】</strong>：  </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A = tf.convert_to_tensor(np.array([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>], [<span class="number">3</span>,<span class="number">4</span>,<span class="number">8</span>,<span class="number">5</span>]])) <span class="keyword">print</span> A.dtype</span><br><span class="line">b = tf.cast(A, tf.float32) <span class="keyword">print</span> b.dtype</span><br></pre></td></tr></table></figure>
<p>  结果输出：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;dtype: &apos;int64&apos;&gt;</span><br><span class="line">&lt;dtype: &apos;float32&apos;&gt;</span><br></pre></td></tr></table></figure>
<p>  从输出结果看出，将矩阵 <code>A</code> 由整数型变为 32 位浮点型。</p>
</li>
<li><p><strong>④ <code>tf.equal( )</code> 函数表示对比两个矩阵或者向量的元素</strong>。<br>  若对应元素相等，则返回 <code>True</code>；若对应元素不相等，则返回 <code>False</code>。<br>  <strong>【例如】</strong>：  </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">A = [[<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line">B = [[<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session( ) <span class="keyword">as</span> sess: </span><br><span class="line">    print(sess.run(tf.equal(A, B)))</span><br></pre></td></tr></table></figure>
<p>  输出结果：<code>[[ True True True False False]]</code></p>
<p>  在矩阵 <code>A</code> 和 <code>B</code> 中，第 1、2、3 个元素相等，第 4、5 个元素不等，故输出结果中，第 1、2、3 个元素取值为 <code>True</code>，第 4、5 个元素取值为 <code>False</code>。</p>
</li>
<li><p><strong>⑤ <code>tf.reduce_mean(x,axis)</code> 函数表示求取矩阵或张量指定维度的平均值</strong>。<br>  若不指定第二个参数，则在所有元素中取平均值；<br>  若指定第二个参数为 0，则在第一维元素上取平均值，即每一列求平均值；<br>  若指定第二个参数为 1，则在第二维元素上取平均值，即每一行求平均值。</p>
<p>  <strong>【例如】</strong>：  </p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = [[1., 1.]</span><br><span class="line">    [2., 2.]]</span><br><span class="line"></span><br><span class="line">print(tf.reduce_mean(x))</span><br></pre></td></tr></table></figure>
<p>  输出结果：<code>1.5</code></p>
<p>  <code>print(tf.reduce_mean(x, 0))</code><br>  输出结果：<code>[1.5, 1.5]</code> </p>
<p>  <code>print(tf.reduce_mean(x, 1))</code><br>  输出结果：<code>[1., 1.]</code></p>
</li>
<li><p><strong>⑥ <code>tf.argmax(x,axis)</code> 函数表示返回指定维度 axis 下，参数 x 中最大值索引号</strong>。<br>  <strong>【例如】</strong>：<br>  在 <code>tf.argmax([1,0,0],1)</code> 函数中，axis 为 1，参数 x 为 <code>[1,0,0]</code>，表示在参数 x的第一个维度取最大值对应的索引号，故返回 0。</p>
</li>
<li><p><strong>⑦ <code>os.path.join()</code> 函数表示把参数字符串按照路径命名规则拼接</strong>。<br>  <strong>【例如】</strong>：     </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.path.join(<span class="string">'/hello/'</span>,<span class="string">'good/boy/'</span>,<span class="string">'doiido'</span>)</span><br></pre></td></tr></table></figure>
<p>  输出结果：<code>&#39;/hello/good/boy/doiido&#39;</code></p>
</li>
<li><p><strong>⑧ <code>字符串.split( )</code> 函数表示按照指定”拆分符”对字符串拆分，返回拆分列表</strong>。<br>  <strong>【例如】</strong>：<br>  <code>&#39;./model/mnist_model-1001&#39;.split(&#39;/&#39;)[-1].split(&#39;-&#39;)[-1]</code></p>
<p>  在该例子中，共进行两次拆分。<br>  第一个拆分符为 <code>/</code>，返回拆分列表，并提取 列表中索引为 -1 的元素即倒数第一个元素；<br>  第二个拆分符为 <code>-</code>，返回拆分列表，并提取列表中索引为 -1 的元素即倒数第一个元素，故函数返回值为 1001。</p>
</li>
<li><p><strong>⑨ <code>tf.Graph( ).as_default( )</code> 函数表示将当前图设置成为默认图，并返回一个上下文管理器。该函数一般与 with 关键字搭配使用，应用于将已经定义好的神经网络在计算图中复现</strong>。<br>  <strong>【例如】</strong>：<br>  <code>with tf.Graph().as_default() as g</code>，表示将在 <code>Graph()</code> 内定义的节点加入到计算图 g 中。</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>√ 神经网络模型的保存</strong><br>  在反向传播过程中，一般会间隔一定轮数保存一次神经网络模型，并产生三个文件<br>  <strong>(保存当前图结构的 <code>.meta</code> 文件、保存当前参数名的 <code>.index</code> 文件、保存当前参数的 <code>.data</code> 文件)</strong>，<br>  在 Tensorflow 中如下表示：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver() </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        <span class="keyword">if</span> i % 轮 数 == <span class="number">0</span>:</span><br><span class="line">            saver.save(</span><br><span class="line">                sess, </span><br><span class="line">                os.path.join(MODEL_SAVE_PATH, MODEL_NAME), </span><br><span class="line">                global_step=global_step</span><br><span class="line">            )</span><br></pre></td></tr></table></figure>
<p>  其中，<code>tf.train.Saver()</code> 用来实例化 <code>saver</code> 对象。上述代码表示，神经网络每循环规定的轮数，将神经网络模型中所有的参数等信息保存到指定的路径中，并在存放网络模型的文件夹名称中注明保存模型时的训练轮数。</p>
</li>
<li><p><strong>√ 神经网络模型的加载</strong><br>  <strong>在测试网络效果时，需要将训练好的神经网络模型加载</strong>，<br>  在 Tensorflow 中这样表示：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    ckpt = tf.train.get_checkpoint_state(存储路径)</span><br><span class="line">    <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">        saver.restore(sess, ckpt.model_checkpoint_path)</span><br></pre></td></tr></table></figure>
<p>  在 <code>with</code> 结构中进行加载保存的神经网络模型，若 <code>ckpt</code> 和保存的模型在指定路径中存在，则将保存的神经网络模型加载到当前会话中。</p>
</li>
<li><p><strong>√ 加载模型中参数的滑动平均值</strong><br>  <strong>在保存模型时，若模型中采用滑动平均，则参数的滑动平均值会保存在相应文件中。通过实例化 <code>saver</code> 对象，实现参数滑动平均值的加载</strong>，<br>  在 Tensorflow 中如下表示：  </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ema = tf.train.ExponentialMovingAverage(滑动平均基数) </span><br><span class="line">ema_restore = ema.variables_to_restore() </span><br><span class="line">saver = tf.train.Saver(ema_restore)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>√ 神经网络模型准确率评估方法</strong><br>  <strong>在网络评估时，一般通过计算在一组数据上的识别准确率，评估神经网络的效果。</strong><br>  在 Tensorflow 中这样表示：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>)) </span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br></pre></td></tr></table></figure>
<p>  在上述中，y 表示在一组数据(即 <code>batch_size</code> 个数据)上神经网络模型的预测结果，y 的形状为 <code>[batch_size,10]</code>，每一行表示一张图片的识别结果。</p>
<p>  通过 <code>tf.argmax()</code> 函数取出每张图片对应向量中最大值元素对应的索引值，组成长度为输入数据 <code>batch_size</code> 个的一维数组。<br>  通过 <code>tf.equal()</code> 函数判断预测结果张量和实际标签张量的每个维度是否相等，若相等则返回 <code>True</code>，不相等则返回 <code>False</code>。<br>  通过 <code>tf.cast()</code> 函数将得到的布尔型数值转化为实数型，<br>  再通过<code>tf.reduce_mean()</code> 函数求平均值，最终得到神经网络模型在本组数据上的准确率。</p>
</li>
</ul>
<h2 id="模块化搭建神经网络"><a href="#模块化搭建神经网络" class="headerlink" title="模块化搭建神经网络"></a>模块化搭建神经网络</h2><p><strong>神经网络八股包括前向传播过程、反向传播过程、反向传播过程中用到的正则化、指数衰减学习率、滑动平均方法的设置、以及测试模块。</strong></p>
<ul>
<li><p><strong>√ 前向传播过程(<code>forward.py</code>)</strong></p>
<p>  前向传播过程完成神经网络的搭建，结构如下： </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, regularizer)</span>:</span></span><br><span class="line">    w=</span><br><span class="line">    b= </span><br><span class="line">    y=</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, regularizer)</span>:</span> </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span>:</span></span><br></pre></td></tr></table></figure>
<p>  前向传播过程中，需要定义神经网络中的参数 w 和偏置 b，定义由输入到输出的网络结构。<br>  通过定义函数 <code>get_weight()</code> 实现对参数 w 的设置，包括参数 w 的形状和是否正则化的标志。同样，通过定义函数 <code>get_bias()</code> 实现对偏置 b 的设置。</p>
</li>
<li><p><strong>√ 反向传播过程(<code>backword.py</code>)</strong></p>
<p>  反向传播过程完成网络参数的训练，结构如下： </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">( mnist )</span>:</span></span><br><span class="line">    x  = tf.placeholder(dtype, shape )</span><br><span class="line">    y_ = tf.placeholder(dtype, shape )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向传播函数</span></span><br><span class="line">    y = forward( ) </span><br><span class="line">    global_step =</span><br><span class="line">    loss =</span><br><span class="line"></span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实例化 saver 对象</span></span><br><span class="line">    saver = tf.train.Saver() </span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># 初始化所有模型参数</span></span><br><span class="line">        tf.initialize_all_variables().run()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练模型</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">            sess.run(train_step, feed_dict=&#123;x: , y_: &#125;)</span><br><span class="line">            <span class="keyword">if</span> i % 轮数 == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">print</span></span><br><span class="line">                saver.save( )</span><br></pre></td></tr></table></figure>
<p>  反向传播过程中，<br>  用 <code>tf.placeholder(dtype, shape)</code> 函数实现训练样本 x 和样本标签 y_占位，<br>  函数参数 <code>dtype</code> <code>表示数据的类型，shape</code> 表示数据的形状；<br>  y 表示定义的前向传播函数 <code>forward</code>；<br>  <code>loss</code> 表示定义的损失函数，一般为预测值与样本标签的交叉熵(或均方误差)与正则化损失之和；<br>  <code>train_step</code> 表示利用优化算法对模型参数进行优化<br>  常用优化算法 <code>GradientDescentOptimizer</code>、<code>AdamOptimizer</code>、<code>MomentumOptimizer</code> 算法，<br>  在上述代码中使用的 <code>GradientDescentOptimizer</code> 优化算法。</p>
<p>  接着实例化 <code>saver</code> 对象，<br>  其中利用 <code>tf.initialize_all_variables().run()</code> 函数实例化所有参数模型，<br>  利用 <code>sess.run( )</code> 函数实现模型的训练优化过程，并每间隔一定轮数保存一次模型。</p>
</li>
<li><p><strong>√ 正则化、指数衰减学习率、滑动平均方法的设置</strong></p>
</li>
</ul>
<ul>
<li><strong>① 正则化项 <code>regularization</code></strong>  </li>
</ul>
<p>当在前向传播过程中即 <code>forward.py</code> 文件中，设置正则化参数 <code>regularization</code> 为 1 时，则表明在反向传播过程中优化模型参数时，需要在损失函数中加入正则化项。</p>
<p>结构如下：  </p>
<p><strong>首先，需要在前向传播过程即 <code>forward.py</code> 文件中加入</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> regularizer != <span class="keyword">None</span>: </span><br><span class="line">    tf.add_to_collection(<span class="string">'losses'</span>, tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br></pre></td></tr></table></figure>
<p><strong>其次，需要在反向传播过程即 <code>backword.py</code> 文件中加入</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))</span><br><span class="line">cem = tf.reduce_mean(ce)</span><br><span class="line">loss = cem + tf.add_n(tf.get_collection(&apos;losses&apos;))</span><br></pre></td></tr></table></figure>
<p>其中，<code>tf.nn.sparse_softmax_cross_entropy_with_logits()</code> 表示 <code>softmax()</code> 函数与交叉熵一起使用。</p>
<ul>
<li><strong>② 指数衰减学习率</strong><br>在训练模型时，使用指数衰减学习率可以使模型在训练的前期快速收敛接近较优解，又可以保证模型在训练后期不会有太大波动。</li>
</ul>
<p><strong>运用指数衰减学习率，需要在反向传播过程即 <code>backword.py</code> 文件中加入：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = tf.train.exponential_decay( </span><br><span class="line">    LEARNING_RATE_BASE,</span><br><span class="line">    global_step,</span><br><span class="line">    LEARNING_RATE_STEP, </span><br><span class="line">    LEARNING_RATE_DECAY,</span><br><span class="line">    staircase=<span class="keyword">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>③ 滑动平均</strong><br>在模型训练时引入滑动平均可以使模型在测试数据上表现的更加健壮。<br><strong>需要在反向传播过程即 <code>backword.py</code> 文件中加入</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ema = tf.train.ExponentialMovingAverage(</span><br><span class="line">    MOVING_AVERAGE_DECAY,</span><br><span class="line">    global_step</span><br><span class="line">)</span><br><span class="line">ema_op = ema.apply(tf.trainable_variables()) </span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">    train_op = tf.no_op(name=<span class="string">'train'</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p><strong>√ 测试过程(<code>test.py</code>)</strong></p>
<p>  当神经网络模型训练完成后，便可用于测试数据集，验证神经网络的性能。结构如下：</p>
<p>  <strong>首先，制定模型测试函数 <code>test()</code></strong>  </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">( mnist )</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.Graph( ).as_default( ) <span class="keyword">as</span> g:</span><br><span class="line">        <span class="comment"># 给 x y_占位</span></span><br><span class="line">        x  = tf.placeholder(dtype,shape)</span><br><span class="line">        y_ = tf.placeholder(dtype,shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#前向传播得到预测结果 y</span></span><br><span class="line">        y = mnist_forward.forward(x, <span class="keyword">None</span>) <span class="comment">#前向传播得到 y</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#实例化可还原滑动平均的 saver</span></span><br><span class="line">        ema = tf.train.ExponentialMovingAverage(滑动衰减率)</span><br><span class="line">        ema_restore = ema.variables_to_restore()</span><br><span class="line">        saver = tf.train.Saver(ema_restore)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#计算正确率</span></span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>),tf.argmax(y_,<span class="number">1</span>))</span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">            <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">            <span class="comment">#加载训练好的模型</span></span><br><span class="line">            ckpt = tf.train.get_checkpoint_state(存储路径)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#如果已有 ckpt 模型则恢复</span></span><br><span class="line">            <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">                <span class="comment">#恢复会话</span></span><br><span class="line">                saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line"></span><br><span class="line">                <span class="comment">#恢复轮数</span></span><br><span class="line">                global_ste = ckpt.model_checkpoint_path.split(<span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'-'</span>)[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">                <span class="comment">#计算准确率</span></span><br><span class="line">                accuracy_score = sess.run(accuracy, feed_dict=&#123;x:测试数据, y_:测试数据标签 &#125;)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 打印提示</span></span><br><span class="line">                print(<span class="string">"After %s training step(s), test accuracy=%g"</span> % (global_step, accuracy_score))</span><br><span class="line">            <span class="comment">#如果没有模型</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'No checkpoint file found'</span>) <span class="comment">#模型不存在提示</span></span><br><span class="line">                <span class="keyword">return</span></span><br></pre></td></tr></table></figure>
<p>  <strong>其次，制定 main()函数</strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#加载测试数据集</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#调用定义好的测试函数 test()</span></span><br><span class="line">    test(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>  通过对测试数据的预测得到准确率，从而判断出训练出的神经网络模型的性能好坏。当准确率低时，可能原因有模型需要改进，或者是训练数据量太少导致过拟合。</p>
</li>
</ul>
<h2 id="手写数字识别准确率输出"><a href="#手写数字识别准确率输出" class="headerlink" title="手写数字识别准确率输出"></a>手写数字识别准确率输出</h2><p><strong>实现手写体 mnist 数据集的识别任务，共分为三个模块文件，分别是描述网络结构的前向传播过程文件(<code>mnist_forward.py</code>)、描述网络参数优化方法的反向传播过程文件(<code>mnist_backward.py</code>)、验证模型准确率的测试过程文件(<code>mnist_test.py</code>)。</strong></p>
<ul>
<li><p><strong>√ 前向传播过程文件(<a href="./mnist_forward.py"><code>mnist_forward.py</code></a>)</strong><br>  在前向传播过程中，需要定义网络模型输入层个数、隐藏层节点数、输出层个数，定义网络参数 w、偏置 b，定义由输入到输出的神经网络架构。</p>
<p>  实现手写体 mnist 数据集的识别任务前向传播过程如下：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">INPUT_NODE = <span class="number">784</span></span><br><span class="line">OUTPUT_NODE = <span class="number">10</span></span><br><span class="line">LAYER1_NODE = <span class="number">500</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, regularizer)</span>:</span></span><br><span class="line">    w = tf.Variable(tf.truncated_normal(shape,stddev=<span class="number">0.1</span>))</span><br><span class="line">    <span class="keyword">if</span> regularizer != <span class="keyword">None</span>: </span><br><span class="line">        tf.add_to_collection(</span><br><span class="line">            <span class="string">'losses'</span>, </span><br><span class="line">            tf.contrib.layers.l2_regularizer(regularizer)(w)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span>:</span>  </span><br><span class="line">    b = tf.Variable(tf.zeros(shape))  </span><br><span class="line">    <span class="keyword">return</span> b</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, regularizer)</span>:</span></span><br><span class="line">    w1 = get_weight([INPUT_NODE, LAYER1_NODE], regularizer)</span><br><span class="line">    b1 = get_bias([LAYER1_NODE])</span><br><span class="line">    y1 = tf.nn.relu(tf.matmul(x, w1) + b1)</span><br><span class="line"></span><br><span class="line">    w2 = get_weight([LAYER1_NODE, OUTPUT_NODE], regularizer)</span><br><span class="line">    b2 = get_bias([OUTPUT_NODE])</span><br><span class="line">    y = tf.matmul(y1, w2) + b2</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<p>  由上述代码可知，在前向传播过程中，规定网络输入结点为 784 个(代表每张输入图片的像素个数)，隐藏层节点 500 个，输出节点 10 个(表示输出为数字 0-9 的十分类)。</p>
<p>  由输入层到隐藏层的参数 w1 形状为 <code>[784,500]</code>，由隐藏层到输出层的参数 w2 形状为 <code>[500,10]</code>，参数满足截断正态分布，并使用正则化，将每个参数的正则化损失加到总损失中。</p>
<p>  由输入层到隐藏层的偏置 b1 形状为长度为 500 的一维数组，由隐藏层到输出层的偏置 b2 形状为长度为 10 的一维数组，初始化值为全 0。</p>
<p>  前向传播结构第一层为输入 x 与参数 w1 矩阵相乘加上偏置 b1，再经过 <code>relu</code> 函数，得到隐藏层输出 y1。前向传播结构第二层为隐藏层输出 y1 与参数 w2 矩阵相乘加上偏置 b2，得到输出 y。<br>  由于输出 y 要经过 <code>softmax</code> 函数，使其符合概率分布，故输出 y 不经过 <code>relu</code> 函数。</p>
</li>
</ul>
<ul>
<li><p><strong>√ 反向传播过程文件(<a href="./mnist_backward.py"><code>mnist_backward.py</code></a>)</strong><br>  反向传播过程实现利用训练数据集对神经网络模型训练，通过降低损失函数值，实现网络模型参数的优化，从而得到准确率高且泛化能力强的神经网络模型。 实现手写体 mnist 数据集的识别任务反向传播过程如下：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> mnist_forward</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">200</span></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.1</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span></span><br><span class="line">REGULARIZER = <span class="number">0.0001</span></span><br><span class="line">STEPS = <span class="number">50000</span></span><br><span class="line">MOVING_AVERAGE_DECAY = <span class="number">0.99</span></span><br><span class="line">MODEL_SAVE_PATH=<span class="string">"./model/"</span></span><br><span class="line">MODEL_NAME=<span class="string">"mnist_model"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(mnist)</span>:</span></span><br><span class="line"></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, mnist_forward.INPUT_NODE])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, mnist_forward.OUTPUT_NODE])</span><br><span class="line">    y = mnist_forward.forward(x, REGULARIZER)</span><br><span class="line">    global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    cem = tf.reduce_mean(ce)</span><br><span class="line">    loss = cem + tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br><span class="line"></span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE,</span><br><span class="line">        global_step,</span><br><span class="line">        mnist.train.num_examples / BATCH_SIZE, </span><br><span class="line">        LEARNING_RATE_DECAY,</span><br><span class="line">        staircase=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br><span class="line"></span><br><span class="line">    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">        train_op = tf.no_op(name=<span class="string">'train'</span>)</span><br><span class="line"></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        init_op = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">            xs, ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line">            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict=&#123;x: xs, y_: ys&#125;)</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"After %d training step(s), loss on training batch is %g."</span> % (step, loss_value))</span><br><span class="line">                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">    backward(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>  由上述代码可知，在反向传播过程中，首先引入 <code>tensorflow</code>、<code>input_data</code>、前向传播 <code>mnist_forward</code> 和 <code>os</code> 模块，定义每轮喂入神经网络的图片数、初始学习率、学习率衰减率、正则化系数、训练轮数、模型保存路径以及模型保存名称等相关信息。</p>
<p>  在反向传播函数 <code>backword</code> 中，首先读入 mnist，用 <code>placeholder</code> 给训练数据 <code>x</code> 和标签 <code>y_</code> 占位，调用 <code>mnist_forward</code> 文件中的前向传播过程 <code>forword()</code> 函数，并设置正则化，计算训练数据集上的预测结果 <code>y</code>，并给当前计算轮数计数器赋值，设定为不可训练类型。</p>
<p>  接着，调用包含所有参数正则化损失的损失函数 <code>loss</code>，并设定指数衰减学习率 <code>learning_rate</code>。<br>  然后，使用梯度衰减算法对模型优化，降低损失函数，并定义参数的滑动平均。<br>  最后，在 <code>with</code> 结构中，实现所有参数初始化，每次喂入 <code>batch_size</code> 组(即 200 组)训练数据和对应标签，循环迭代 <code>steps</code> 轮，并每隔 1000 轮打印出一次损失函数值信息，并将当前会话加载到指定路径。<br>  最后，通过主函数 <code>main()</code>，加载指定路径下的训练数据集，并调用规定的 <code>backward()</code> 函数训练模型。</p>
</li>
</ul>
<ul>
<li><p><strong>√ 测试过程文件(<a href="./mnist_test.py"><code>mnist_test.py</code></a>)</strong><br>  当训练完模型后，给神经网络模型输入测试集验证网络的准确性和泛化性。注意，所用的测试集和训练集是相互独立的。</p>
<p>  实现手写体 mnist 数据集的识别任务测试传播过程如下：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> mnist_forward</span><br><span class="line"><span class="keyword">import</span> mnist_backward</span><br><span class="line"></span><br><span class="line">TEST_INTERVAL_SECS = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(mnist)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g:</span><br><span class="line">        x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, mnist_forward.INPUT_NODE])</span><br><span class="line">        y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, mnist_forward.OUTPUT_NODE])</span><br><span class="line">        y = mnist_forward.forward(x, <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">        ema = tf.train.ExponentialMovingAverage(mnist_backward.MOVING_AVERAGE_DECAY)</span><br><span class="line">        ema_restore = ema.variables_to_restore()</span><br><span class="line">        saver = tf.train.Saver(ema_restore)</span><br><span class="line">        </span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">            <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">                ckpt = tf.train.get_checkpoint_state(mnist_backward.MODEL_SAVE_PATH)</span><br><span class="line">                <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">                    saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line">                    global_step = ckpt.model_checkpoint_path.split(<span class="string">'/'</span>)[<span class="number">-1</span>].split(<span class="string">'-'</span>)[<span class="number">-1</span>]</span><br><span class="line">                    accuracy_score = sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)</span><br><span class="line">                    print(<span class="string">"After %s training step(s), test accuracy = %g"</span> % (global_step, accuracy_score))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    print(<span class="string">'No checkpoint file found'</span>)</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">            time.sleep(TEST_INTERVAL_SECS)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">    test(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>  在上述代码中，首先需要引入 <code>time</code> 模块、<code>tensorflow</code>、<code>input_data</code>、前向传播 <code>mnist_forward</code>、反向传播 <code>mnist_backward</code> 模块和 <code>os</code> 模块，并规定程序 5 秒的循环间隔时间。</p>
<p>  接着，定义测试函数 <code>test()</code>,读入 mnist 数据集，利用 <code>tf.Graph()</code> 复现之前定义的计算图，利用 <code>placeholder</code> 给训练数据 <code>x</code> 和标签 <code>y_</code> 占位，调用 <code>mnist_forward</code> 文件中的前向传播过程 <code>forword()</code> 函数，计算训练数据集上的预测结果 <code>y</code>。</p>
<p>  接着，实例化具有滑动平均的 <code>saver</code> 对象，从而在会话被加载时模型中的所有参数被赋值为各自的滑动平均值，增强模型的稳定性，然后计算模型在测试集上的准确率。</p>
<p>  在 <code>with</code> 结构中，加载指定路径下的 <code>ckpt</code>，若模型存在，则加载出模型到当前对话，在测试数据集上进行准确率验证，并打印出当前轮数下的准确率，若模型不存在，则打印出模型不存在的提示，从而 <code>test()</code> 函数完成。 </p>
<p>  通过主函数 <code>main()</code>，加载指定路径下的测试数据集，并调用规定的 <code>test</code> 函数，进行模型在测试集上的准确率验证。</p>
</li>
</ul>
<p>运行以上三个文件，可得到手写体 mnist 数据集的识别任务的运行结果：</p>
<p><em>mnist_backward.py</em><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Extracting ./data/train-images-idx3-ubyte.gz</span><br><span class="line">Extracting ./data/train-labels-idx1-ubyte.gz</span><br><span class="line">Extracting ./data/t10k-images-idx3-ubyte.gz</span><br><span class="line">Extracting ./data/t10k-labels-idx1-ubyte.gz</span><br><span class="line">After 1 training step(s), loss on training batch is 3.35724.</span><br><span class="line">After 1001 training step(s), loss on training batch is 0.300589.</span><br><span class="line">After 2001 training step(s), loss on training batch is 0.265261.</span><br><span class="line">已杀死</span><br><span class="line">ailab@ailab-virtual-machine:~/tf-notes/lec5$ </span><br><span class="line">ailab@ailab-virtual-machine:~/tf-notes/lec5$ </span><br><span class="line">ailab@ailab-virtual-machine:~/tf-notes/lec5$ python mnist_backward.py</span><br><span class="line">Extracting ./data/train-images-idx3-ubyte.gz</span><br><span class="line">Extracting ./data/train-labels-idx1-ubyte.gz</span><br><span class="line">Extracting ./data/t10k-images-idx3-ubyte.gz</span><br><span class="line">Extracting ./data/t10k-labels-idx1-ubyte.gz</span><br><span class="line">After 1 training step(s), loss on training batch is 2.82503.</span><br><span class="line">After 1001 training step(s), loss on training batch is 0.302797.</span><br><span class="line">After 2001 training step(s), loss on training batch is 0.282333.</span><br><span class="line">After 3001 training step(s), loss on training batch is 0.347805.</span><br><span class="line">After 4001 training step(s), loss on training batch is 0.206898.</span><br><span class="line">After 5001 training step(s), loss on training batch is 0.198906.</span><br><span class="line">After 6001 training step(s), loss on training batch is 0.19496.</span><br><span class="line">After 7001 training step(s), loss on training batch is 0.21611.</span><br><span class="line">After 8001 training step(s), loss on training batch is 0.190037.</span><br><span class="line">After 9001 training step(s), loss on training batch is 0.180762.</span><br><span class="line">After 10001 training step(s), loss on training batch is 0.186933.</span><br><span class="line">After 11001 training step(s), loss on training batch is 0.194215.</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p><em>mnist_test.py</em><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Extracting ./data/train-images-idx3-ubyte.gz</span><br><span class="line">Extracting ./data/train-labels-idx1-ubyte.gz</span><br><span class="line">Extracting ./data/t10k-images-idx3-ubyte.gz</span><br><span class="line">Extracting ./data/t10k-labels-idx1-ubyte.gz</span><br><span class="line">After 1 training step(s), test accuracy = 0.0793</span><br><span class="line">After 1001 training step(s), test accuracy = 0.9465</span><br><span class="line">After 1001 training step(s), test accuracy = 0.9465</span><br><span class="line">After 2001 training step(s), test accuracy = 0.9608</span><br><span class="line">After 2001 training step(s), test accuracy = 0.9608</span><br><span class="line">After 3001 training step(s), test accuracy = 0.9674</span><br><span class="line">After 3001 training step(s), test accuracy = 0.9674</span><br><span class="line">After 4001 training step(s), test accuracy = 0.9708</span><br><span class="line">After 4001 training step(s), test accuracy = 0.9708</span><br><span class="line">After 5001 training step(s), test accuracy = 0.9727</span><br><span class="line">After 5001 training step(s), test accuracy = 0.9727</span><br><span class="line">After 6001 training step(s), test accuracy = 0.9737</span><br><span class="line">After 6001 training step(s), test accuracy = 0.9737</span><br><span class="line">After 7001 training step(s), test accuracy = 0.975</span><br><span class="line">After 7001 training step(s), test accuracy = 0.975</span><br><span class="line">After 8001 training step(s), test accuracy = 0.9763</span><br><span class="line">After 8001 training step(s), test accuracy = 0.9763</span><br><span class="line">After 9001 training step(s), test accuracy = 0.9767</span><br><span class="line">After 9001 training step(s), test accuracy = 0.9767</span><br><span class="line">After 10001 training step(s), test accuracy = 0.9769</span><br><span class="line">After 10001 training step(s), test accuracy = 0.9769</span><br><span class="line">After 11001 training step(s), test accuracy = 0.9772</span><br><span class="line">After 11001 training step(s), test accuracy = 0.9772</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>从终端显示的运行结果可以看出，随着训练轮数的增加，网络模型的损失函数值在不断降低，并且在测试集上的准确率在不断提升，有较好的泛化能力。</p>

        
    </section>
</article>



<div class="comments">
    <div id="disqus_thread">
        <p class="comment-tips">国内查看评论需要代理~</p>
    </div>
    <script>
    window.disqus_config = function () {
        this.language = 'zh';
        this.page.url = 'http://www.coderss.cn/2018/01/06/ai-dl/';
        this.page.title = 'AI-深度学习';
        this.page.identifier = '2018/01/06/ai-dl/';
    };
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://name.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</div>
        <footer class="footer">
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, Theme by <a href="https://github.com/sanonz/hexo-theme-concise" target="_blank">Concise</a>
</footer>

<script type="text/javascript" src="//s13.cnzz.com/z_stat.php?id=1234567890&amp;web_id=1234567890"></script>


    </div>

    <script type="text/javascript" src="https://cdn.bootcss.com/jquery/1.9.0/jquery.min.js"></script>
    
    <script type="text/javascript" src="/js/scrollspy.min.js"></script>
    
    <script type="text/javascript">
        $(function() {
            var nodes = {
                nav: $('#nav'),
                aside: $('#aside'),
                navTags: $('#nav-tags')
            };

            $('#open-panel, #aside-mask').on('click', function() {
                nodes.aside.toggleClass('panel-show');
            });
            $('#nav-tag').on('click', function(event) {
                event.preventDefault();console.log(nodes.navTags.attr('class'))
                nodes.navTags.toggleClass('tag-show');console.log(nodes.navTags.attr('class'))
            })/*.hover(function() {
                nodes.navTags.addClass('tag-show');
            }, function() {
                nodes.navTags.removeClass('tag-show');
            });*/

            
            $(document.body).scrollspy({target: '#aside-inner'});
            
        });
    </script>

</body>
</html>
