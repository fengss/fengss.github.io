<!DOCTYPE html>
<html lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="UTF-8">

    

    

    <title>Kubernetes-运维工作 | Coderss</title>
    <meta name="author" content="coder">
    <meta name="version" content="1.0.0">
    <meta name="keywords" content="">
    <meta name="description" content="公司Kubernetes运维体系K8sDevOps解析笔记
现存资源


集群名称
地域
节点数量
节点池




联调
华东1
5
2核16GCentOS 7.6 


生产k8s
华东1
7
4核32GCentos7.6 


测试
华东1
6
4核16GCentos7.6 



资源申请
一阶段需申请资源 




集群名称
地域
节点数量
节点池




实验
华东1
3
2核8GCentOS 7.6 




专用网络VPC ?
虚拟交换机 
网络插件：Flannel、Terway ">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    <meta name="baidu-site-verification" content="F0CXvmUgA9">

    
    
    <link rel="icon" href="/favicon.png">
    

    <link rel="stylesheet" href="/css/style.css">
</head>
<body>

    <div class="app">
        <header class="header clearfix">
    <div id="nav" class="nav">
    <button id="open-panel" class="open-panel"><i class="icon-library"></i></button>

    <nav class="nav-inner">

        
        
        <li class="nav-item">
            <a class="nav-link" href="/">首页</a>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/categories/back-end">Java后端</a>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/categories/cpp">C嵌入式</a>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/categories/go">Go云原生</a>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/categories/cloud">Linux安全</a>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/categories/reverse">Win安全</a>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/categories/data">大数据处理</a>
        </li>
        
        
        
        <li class="nav-item nav-item-tag">
            <a id="nav-tag" class="nav-link" href="#">标签</a>
            <div id="nav-tags" class="nav-tag-wrap">
                <i class="nav-tag-arrow"></i>
                
  <div class="widget-wrap">
    <h3 class="widget-title">
        <i class="icon-tag vm"></i>
        <span class="vm">Tags</span>
    </h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/">AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Boost库/">Boost库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Collection/">Collection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cpp编程/">Cpp编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Fescar/">Fescar</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gc/">Gc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/K8s/">K8s</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MapReduce/">MapReduce</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/">Math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Net/">Net</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nosql/">Nosql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python计算库/">Python计算库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Rust/">Rust</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sharding-jdbc/">Sharding-jdbc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SkyWalking/">SkyWalking</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TensorFlow/">TensorFlow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Turi/">Turi</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Windows系统/">Windows系统</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Windows驱动/">Windows驱动</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Yarn/">Yarn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/assembly/">assembly</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/c-cpp语言/">c/cpp语言</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/debug/">debug</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/design/">design</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/">docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dubbo/">dubbo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/eth/">eth</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/go/">go</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/go-kernel/">go-kernel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/io/">io</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/juc/">juc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kubernetes/">kubernetes</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/map/">map</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mfc/">mfc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/microservice/">microservice</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mybatis/">mybatis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/netty/">netty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python-book/">python-book</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/qt/">qt</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sentinel/">sentinel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/skycoin/">skycoin</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spring/">spring</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spring-cloud/">spring-cloud</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/stl/">stl</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tomcat/">tomcat</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/x86-Windows系统总结/">x86 Windows系统总结</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/中台/">中台</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/分布式文件系统/">分布式文件系统</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/多线程编程/">多线程编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/嵌入式/">嵌入式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/架构/">架构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/消息队列/">消息队列</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/网络编程/">网络编程</a></li></ul>
    </div>
  </div>


            </div>
        </li>
        
        
        
        <li class="nav-item">
            <a class="nav-link" href="/archives">归档</a>
        </li>
        
        
        

    </nav>
</div>

    <aside id="aside" class="aside">
    <div id="aside-mask" class="aside-mask"></div>
    <div id="aside-inner" class="aside-inner">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit"><i class="icon-search-stroke"></i></button><input type="hidden" name="sitesearch" value="http://www.coderss.cn"></form>

        
        
        
        
        

        
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#现存资源"><span class="toc-number">1.</span> <span class="toc-text">现存资源</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#资源申请"><span class="toc-number">2.</span> <span class="toc-text">资源申请</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#实验内容"><span class="toc-number">3.</span> <span class="toc-text">实验内容</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#集群的操作"><span class="toc-number">3.1.</span> <span class="toc-text">集群的操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#节点的操作"><span class="toc-number">3.2.</span> <span class="toc-text">节点的操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#集群网络"><span class="toc-number">3.3.</span> <span class="toc-text">集群网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#应用"><span class="toc-number">3.4.</span> <span class="toc-text">应用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#存储"><span class="toc-number">3.5.</span> <span class="toc-text">存储</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CSI"><span class="toc-number">3.5.1.</span> <span class="toc-text">CSI</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#FlexVolume"><span class="toc-number">3.5.2.</span> <span class="toc-text">FlexVolume</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#自动伸缩"><span class="toc-number">3.6.</span> <span class="toc-text">自动伸缩</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#灰度发布"><span class="toc-number">3.7.</span> <span class="toc-text">灰度发布</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#知识点"><span class="toc-number">4.</span> <span class="toc-text">知识点</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Node的管理"><span class="toc-number">4.1.</span> <span class="toc-text">Node的管理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Node的隔离与恢复"><span class="toc-number">4.1.1.</span> <span class="toc-text">Node的隔离与恢复</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Node的扩容"><span class="toc-number">4.1.2.</span> <span class="toc-text">Node的扩容</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kubernetes资源管理"><span class="toc-number">4.2.</span> <span class="toc-text">Kubernetes资源管理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#计算资源管理"><span class="toc-number">4.2.1.</span> <span class="toc-text">计算资源管理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#基于Requests和Limits的Pod调度机制"><span class="toc-number">4.2.1.1.</span> <span class="toc-text">基于Requests和Limits的Pod调度机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Requests和Limits的背后机制"><span class="toc-number">4.2.1.2.</span> <span class="toc-text">Requests和Limits的背后机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#计算资源相关常见问题分析"><span class="toc-number">4.2.1.3.</span> <span class="toc-text">计算资源相关常见问题分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#资源配置范围管理-LimitRange"><span class="toc-number">4.2.1.4.</span> <span class="toc-text">资源配置范围管理(LimitRange)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#资源服务质量管理-Resource-QoS"><span class="toc-number">4.2.1.5.</span> <span class="toc-text">资源服务质量管理(Resource QoS)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Requests和Limits对不同计算资源类型的限制机制"><span class="toc-number">4.2.1.5.1.</span> <span class="toc-text">Requests和Limits对不同计算资源类型的限制机制</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#对调度策略的影响"><span class="toc-number">4.2.1.5.2.</span> <span class="toc-text">对调度策略的影响</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#服务质量等级-QoS-Classes"><span class="toc-number">4.2.1.5.3.</span> <span class="toc-text">服务质量等级(QoS Classes)</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Kubernetes-QoS的工作特点"><span class="toc-number">4.2.1.5.3.1.</span> <span class="toc-text">Kubernetes QoS的工作特点</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#OOM计分系统"><span class="toc-number">4.2.1.5.3.2.</span> <span class="toc-text">OOM计分系统</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#QoS的需要解决的问题"><span class="toc-number">4.2.1.5.3.3.</span> <span class="toc-text">QoS的需要解决的问题</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pod驱逐机制"><span class="toc-number">4.3.</span> <span class="toc-text">Pod驱逐机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#驱逐阈值"><span class="toc-number">4.3.1.</span> <span class="toc-text">驱逐阈值</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#软驱逐"><span class="toc-number">4.3.1.1.</span> <span class="toc-text">软驱逐</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#硬驱逐"><span class="toc-number">4.3.1.2.</span> <span class="toc-text">硬驱逐</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#驱逐频率"><span class="toc-number">4.3.2.</span> <span class="toc-text">驱逐频率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#节点的状况"><span class="toc-number">4.3.3.</span> <span class="toc-text">节点的状况</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#节点状况的抖动"><span class="toc-number">4.3.4.</span> <span class="toc-text">节点状况的抖动</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#回收Node级别的资源"><span class="toc-number">4.3.5.</span> <span class="toc-text">回收Node级别的资源</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#有Imagefs的情况"><span class="toc-number">4.3.5.1.</span> <span class="toc-text">有Imagefs的情况</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#没有Imagefs的情况"><span class="toc-number">4.3.5.2.</span> <span class="toc-text">没有Imagefs的情况</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#驱逐用户的Pod"><span class="toc-number">4.3.6.</span> <span class="toc-text">驱逐用户的Pod</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#资源最少回收量"><span class="toc-number">4.3.7.</span> <span class="toc-text">资源最少回收量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#节点资源紧缺情况下的系统行为"><span class="toc-number">4.3.8.</span> <span class="toc-text">节点资源紧缺情况下的系统行为</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#调度器的行为"><span class="toc-number">4.3.8.1.</span> <span class="toc-text">调度器的行为</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Node的OOM行为"><span class="toc-number">4.3.8.2.</span> <span class="toc-text">Node的OOM行为</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#现阶段的问题"><span class="toc-number">4.3.9.</span> <span class="toc-text">现阶段的问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pod-Disruption-Budget-主动驱逐保护"><span class="toc-number">4.4.</span> <span class="toc-text">Pod Disruption Budget(主动驱逐保护)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#实践操作"><span class="toc-number">4.4.1.</span> <span class="toc-text">实践操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#资源配额管理-Resource-Quotas"><span class="toc-number">4.5.</span> <span class="toc-text">资源配额管理(Resource Quotas)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#在Master中开启资源配额选型"><span class="toc-number">4.5.1.</span> <span class="toc-text">在Master中开启资源配额选型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#计算资源配额-Compute-Resource-Quota"><span class="toc-number">4.5.1.1.</span> <span class="toc-text">计算资源配额(Compute Resource Quota)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#存储资源配额-Volume-Count-Quota"><span class="toc-number">4.5.1.2.</span> <span class="toc-text">存储资源配额(Volume Count Quota)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#对象数量配额-Object-Count-Quota"><span class="toc-number">4.5.1.3.</span> <span class="toc-text">对象数量配额(Object Count Quota)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#配额的作用域-Quota-Scopes"><span class="toc-number">4.5.2.</span> <span class="toc-text">配额的作用域(Quota Scopes)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#在资源配额-ResourceQuota-中设置Requests和Limits"><span class="toc-number">4.5.3.</span> <span class="toc-text">在资源配额(ResourceQuota)中设置Requests和Limits</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#资源配额的定义"><span class="toc-number">4.5.4.</span> <span class="toc-text">资源配额的定义</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#共享存储机制"><span class="toc-number">4.6.</span> <span class="toc-text">共享存储机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#PV详解"><span class="toc-number">4.6.1.</span> <span class="toc-text">PV详解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PVC详解"><span class="toc-number">4.6.2.</span> <span class="toc-text">PVC详解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PV和PVC的生命周期"><span class="toc-number">4.6.3.</span> <span class="toc-text">PV和PVC的生命周期</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#资源供应"><span class="toc-number">4.6.3.1.</span> <span class="toc-text">资源供应</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#资源绑定"><span class="toc-number">4.6.3.2.</span> <span class="toc-text">资源绑定</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#资源使用"><span class="toc-number">4.6.3.3.</span> <span class="toc-text">资源使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#资源释放"><span class="toc-number">4.6.3.4.</span> <span class="toc-text">资源释放</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#资源回收"><span class="toc-number">4.6.3.5.</span> <span class="toc-text">资源回收</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#StorageClass详解"><span class="toc-number">4.6.4.</span> <span class="toc-text">StorageClass详解</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#StorageClass的关键配置参数"><span class="toc-number">4.6.4.1.</span> <span class="toc-text">StorageClass的关键配置参数</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kubesphere迁移阿里Ack"><span class="toc-number">5.</span> <span class="toc-text">Kubesphere迁移阿里Ack</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Node情况"><span class="toc-number">5.1.</span> <span class="toc-text">Node情况</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#业务Namespaces情况"><span class="toc-number">5.2.</span> <span class="toc-text">业务Namespaces情况</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#核心Replica、Stateful、Daemon"><span class="toc-number">5.3.</span> <span class="toc-text">核心Replica、Stateful、Daemon</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#配置新用户及证书"><span class="toc-number">6.</span> <span class="toc-text">配置新用户及证书</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Sealos-使用问题"><span class="toc-number">7.</span> <span class="toc-text">Sealos 使用问题</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#kube-ovn"><span class="toc-number">8.</span> <span class="toc-text">kube-ovn</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#子网"><span class="toc-number">8.1.</span> <span class="toc-text">子网</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#网关"><span class="toc-number">8.2.</span> <span class="toc-text">网关</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Es"><span class="toc-number">9.</span> <span class="toc-text">Es</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SCA"><span class="toc-number">10.</span> <span class="toc-text">SCA</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SAST"><span class="toc-number">11.</span> <span class="toc-text">SAST</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#证书"><span class="toc-number">12.</span> <span class="toc-text">证书</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Debug"><span class="toc-number">13.</span> <span class="toc-text">Debug</span></a></li></ol>
        
    </div>
</aside>

</header>

        <div id="content" class="content"><article class="article" itemscope="" itemprop="blogPost">
    
    <header class="article-header">
        
        <h1 itemprop="name">
            Kubernetes-运维工作
        </h1>
        
        <div class="article-meta clearfix">
            <a class="article-date" href="/2021/06/15/k8s-devops/">
    
    <i class="icon-calendar"></i>
    
    <time datetime="2021-06-15T12:17:18.000Z" itemprop="datePublished">2021-06-15</time>
</a>

            
<div class="article-tag-list">
    <i class="icon-tag"></i>
    <a class="article-tag-link" href="/tags/kubernetes/">kubernetes</a>
</div>


        </div>
    </header>
    
    <section class="article-body markdown-body">
        
        <p>公司Kubernetes运维体系<br>K8sDevOps解析笔记<br><a id="more"></a></p>
<h1 id="现存资源"><a href="#现存资源" class="headerlink" title="现存资源"></a>现存资源</h1><table>
<thead>
<tr>
<th>集群名称</th>
<th>地域</th>
<th>节点数量</th>
<th>节点池</th>
</tr>
</thead>
<tbody>
<tr>
<td>联调</td>
<td>华东1</td>
<td>5</td>
<td>2核16G<br>CentOS 7.6 </td>
</tr>
<tr>
<td>生产k8s</td>
<td>华东1</td>
<td>7</td>
<td>4核32G<br>Centos7.6 </td>
</tr>
<tr>
<td>测试</td>
<td>华东1</td>
<td>6</td>
<td>4核16G<br>Centos7.6 </td>
</tr>
</tbody>
</table>
<h1 id="资源申请"><a href="#资源申请" class="headerlink" title="资源申请"></a>资源申请</h1><blockquote>
<p>一阶段需申请资源 </p>
</blockquote>
<table>
<thead>
<tr>
<th>集群名称</th>
<th>地域</th>
<th>节点数量</th>
<th>节点池</th>
</tr>
</thead>
<tbody>
<tr>
<td>实验</td>
<td>华东1</td>
<td>3</td>
<td>2核8G<br>CentOS 7.6 </td>
</tr>
</tbody>
</table>
<ul>
<li>专用网络VPC ?</li>
<li>虚拟交换机 </li>
<li>网络插件：Flannel、Terway (免费提供选择)</li>
<li><p>容器运行时:Container、Docker、安全沙箱 (免费提供选择)</p>
</li>
<li><p>Ack套件使用费</p>
</li>
<li>EIP 费用(公网访问): api server给公网ip</li>
<li>NAT 费用(公网访问): 让vpc拥有公网访问能力</li>
<li>SLB 费用</li>
</ul>
<p>2核4G<br>SLB 费用：￥ 0.561 /时 、￥ 0.680 /GB<br>EIP 费用：￥ 0.680 /GB<br>ECS 费用：￥ 1.21 /时</p>
<blockquote>
<p>二阶段需申请资源 </p>
</blockquote>
<table>
<thead>
<tr>
<th>集群名称</th>
<th>地域</th>
<th>节点数量</th>
<th>节点池</th>
</tr>
</thead>
<tbody>
<tr>
<td>实验</td>
<td>华东1</td>
<td>5</td>
<td>待定</td>
</tr>
</tbody>
</table>
<h1 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h1><h2 id="集群的操作"><a href="#集群的操作" class="headerlink" title="集群的操作"></a>集群的操作</h2><ul>
<li>创建k8s普通集群、Ack托管版(无Master)集群、Ask集群(无Master、Node)</li>
<li>升级集群</li>
<li>连接操控&amp;join扩容集群</li>
</ul>
<h2 id="节点的操作"><a href="#节点的操作" class="headerlink" title="节点的操作"></a>节点的操作</h2><ul>
<li>Ack节点池管理</li>
<li>节点的管理</li>
<li>节点的调度和运维(亲和、污点)</li>
</ul>
<h2 id="集群网络"><a href="#集群网络" class="headerlink" title="集群网络"></a>集群网络</h2><ul>
<li>理解Terway、Flannel、Calico</li>
<li>理解Service、Ingress</li>
<li>阿里云容器网络CNI<ul>
<li>如何给Pod挂载弹性公网IP</li>
<li>修改Pod镜像保持IP不变</li>
</ul>
</li>
<li>Service管理<ul>
<li>创建服务</li>
<li>管理服务</li>
<li>Service的负载均衡配置注意事项</li>
<li>通过Annotation配置负载均衡</li>
<li>通过使用已有SLB的服务公开应用</li>
<li></li>
</ul>
</li>
<li>SLB Ingress管理<ul>
<li>SLB Ingress Controller组件管理</li>
<li>使用默认生成的SLB实例</li>
<li>使用指定的SLB实例</li>
<li>通过Secret配置TLS证书实现HTTPS访问</li>
<li></li>
</ul>
</li>
<li>Nginx Ingress管理<ul>
<li>安装Nginx Ingress Controller</li>
<li>Ingress基本操作</li>
<li>Ingress高级用法</li>
</ul>
</li>
<li>网络管理最佳实践<ul>
<li>安全组的使用</li>
<li>IPv6的使用</li>
<li>部署高可靠Ingress Controller</li>
<li>优化集群DNS</li>
<li>使用Ambassador Edge Stack管理Ingress资源</li>
<li>Serverless集群基于与解析PrivateZone的服务发现</li>
</ul>
</li>
</ul>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><ul>
<li>工作负载<ul>
<li>无状态Deployment</li>
<li>有状态StatefulSet</li>
<li>守护DaemonSet</li>
<li>任务Job</li>
<li>定时任务CronJob</li>
<li>容器组Pod</li>
</ul>
</li>
</ul>
<h2 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h2><h3 id="CSI"><a href="#CSI" class="headerlink" title="CSI"></a>CSI</h3><ul>
<li>了解 CSI</li>
<li>安装插件</li>
<li>OSS<ul>
<li>使用云盘静态存储卷</li>
<li>通过控制台使用云盘静态存储卷</li>
<li>使用动态云盘卷</li>
<li>存储类(StorageClass)</li>
</ul>
</li>
<li>NAS<ul>
<li>使用NAS静态存储卷</li>
<li>通过控制台使用NAS静态存储卷</li>
</ul>
</li>
</ul>
<h3 id="FlexVolume"><a href="#FlexVolume" class="headerlink" title="FlexVolume"></a>FlexVolume</h3><ul>
<li>了解 FlexVolume</li>
<li>安装插件</li>
<li>OSS<ul>
<li>使用云盘静态存储卷</li>
<li>通过控制台使用云盘静态存储卷</li>
<li>使用动态云盘卷</li>
<li>存储类(StorageClass)</li>
</ul>
</li>
<li>NAS<ul>
<li>使用NAS静态存储卷</li>
<li>通过控制台使用NAS静态存储卷</li>
</ul>
</li>
</ul>
<h2 id="自动伸缩"><a href="#自动伸缩" class="headerlink" title="自动伸缩"></a>自动伸缩</h2><ul>
<li>资源层自动伸缩<ul>
<li>托管节点自动伸缩</li>
<li>ACK节点和ASK的配合</li>
</ul>
</li>
<li>调度层自动伸缩<ul>
<li>HPA实验</li>
<li>VPA实验</li>
<li>CronHPA实验</li>
</ul>
</li>
</ul>
<h2 id="灰度发布"><a href="#灰度发布" class="headerlink" title="灰度发布"></a>灰度发布</h2><ul>
<li>使用阿里云的自动模式/手动模式创建灰度发布</li>
<li>Ingress实现法度恢复和蓝绿发布</li>
</ul>
<blockquote>
<p>总结</p>
</blockquote>
<ul>
<li>Kubelet通过Eviction Signal来记录监控到的Node节点使用情况。</li>
<li>Eviction Signal支持：memory.available, nodefs.available, nodefs.inodesFree, imagefs.available, imagefs.inodesFree。</li>
<li>通过设置Hard Eviction Thresholds和Soft Eviction Thresholds相关参数来触发Kubelet进行Evict Pods的操作。</li>
<li>Evict Pods的时候根据Pod QoS和资源使用情况挑选Pods进行Kill。</li>
<li>Kubelet通过eviction-pressure-transition-period防止Node Condition来回切换引起scheduler做出错误的调度决定。</li>
<li>Kubelet通过–eviction-minimum-reclaim来保证每次进行资源回收后，Node的最少可用资源，以避免频繁被触发Evict Pods操作。</li>
<li>当Node Condition为MemoryPressure时，Scheduler不会调度新的QoS Class为BestEffort的Pods到该Node。</li>
<li>当Node Condition为DiskPressure时，Scheduler不会调度任何新的Pods到该Node。</li>
</ul>
<h1 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h1><h2 id="Node的管理"><a href="#Node的管理" class="headerlink" title="Node的管理"></a>Node的管理</h2><h3 id="Node的隔离与恢复"><a href="#Node的隔离与恢复" class="headerlink" title="Node的隔离与恢复"></a>Node的隔离与恢复</h3><p>Kubernetes提供了一种机制，既可以将Node纳入调度范围，也可以将Node脱离调度范围。</p>
<p>在spec部分指定unschedulable为true;<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apVersion:</span><span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span><span class="string">Node</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">	<span class="attr">name:</span> <span class="string">k8s-node</span></span><br><span class="line">	<span class="attr">labels:</span></span><br><span class="line">		<span class="string">kubernetes.io/hostname:</span> <span class="string">k8s-node</span></span><br><span class="line">	<span class="attr">spec:</span></span><br><span class="line">		<span class="attr">unschedulable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl patch node k8s-node '&#123;"spec":&#123;“unschedulable:true”&#125;&#125;'</span><br><span class="line">kubectl replace -f unscheduling_node.yaml</span><br></pre></td></tr></table></figure>
<p>如果需要将某个Node重新纳入集群调度范围，则将unschedulable设置为false，再次执行kubectl replace或kubectl patch命令就能恢复系统对该Node的调度。</p>
<h3 id="Node的扩容"><a href="#Node的扩容" class="headerlink" title="Node的扩容"></a>Node的扩容</h3><p>Kubernetes集群中，一个新Node的加入是非常简单的。<br>在新的Node上安装Docker、kubelet和kube-proxy服务，然后配置kubelet和kube-proxy的启动参数，<br>将Master URL指定为当前Kubernetes集群Master的地址，最后启动这些服务。<br>通过kubelet默认的自动注册机制，新的Node将会自动加入现有的Kubernetes集群中</p>
<p><br></p>
<h2 id="Kubernetes资源管理"><a href="#Kubernetes资源管理" class="headerlink" title="Kubernetes资源管理"></a>Kubernetes资源管理</h2><p>Pod的两个重要参数：CPU Request与Memory Request<br>即使系统资源严重不足，也需要保障这些Pod的存活，Kubernetes中该保障机制</p>
<ul>
<li>通过资源限额来确保不同的Pod只能占用指定的资源。</li>
<li>允许集群的资源被超额分配，以提高集群的资源利用率。</li>
<li>为Pod划分等级，确保不同等级的Pod有不同的服务质量(QoS)，资源不足时，低等级的Pod会被清理，以确保高等级的Pod稳定运行。</li>
</ul>
<p>CPU与Memory是被Pod使用的，因此在配置Pod时可以通过参数CPU Request及Memory Request为其中的每个容器指定所需使用的CPU与Memory量<br>Kubernetes会根据Request的值去查找有足够资源的Node来调度此Pod，如果没有则调度失败。</p>
<p>一个程序所使用的CPU与Memory是一个动态的量，确切地说是一个范围，跟它的负载密切相关<br>负载增加时，CPU和Memory的使用量也会增加。因此最准确的说法是某个进程的CPU使用量为0.1个CPU～1个CPU，内存占用则为500MB～1GB。</p>
<p>对应到Kubernetes的Pod容器上，就是下面这4个参数：</p>
<ul>
<li><code>spec.container[].resources.requests.cpu</code>；</li>
<li><code>spec.container[].resources.limits.cpu</code>；</li>
<li><code>spec.container[].resources.requests.memory</code>；</li>
<li><code>spec.container[].resources.limits.memory</code>。</li>
</ul>
<p>limits对应资源量的上限，即最多允许使用这个上限的资源量</p>
<p>对于Memory这种不可压缩资源来说，它的Limit设置就是一个问题了<br>如果设置得小了当进程在业务繁忙期试图请求超过Limit限制的Memory时，此进程就会被Kubernetes杀掉。<br>因此Memory的Request与Limit的值需要结合进程的实际需求谨慎设置。</p>
<blockquote>
<p>如果不设置CPU或Memory的Limit值，会怎样呢？<br>在这种情况下该Pod的资源使用量有一个弹性范围，我们不用绞尽脑汁去思考这两个Limit的合理值，但问题也来了，考虑下面的例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Pod A的Memory Request被设置为1GB，Node A当时空闲的Memory为1.2GB，符合Pod A的需求，因此Pod A被调度到Node A上。</span><br><span class="line">运行3天后，Pod A的访问请求大增，内存需要增加到1.5GB，此时Node A的剩余内存只有200MB，由于Pod A新增的内存已经超出系统资源</span><br><span class="line">所以在这种情况下，Pod A就会被Kubernetes杀掉。</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>Kubernetes提供了另外两个相关对象：LimitRange及ResourceQuota，前者解决request与limit参数的默认值和合法取值范围等问题，后者则解决约束租户的资源配额问题。</p>
<h3 id="计算资源管理"><a href="#计算资源管理" class="headerlink" title="计算资源管理"></a>计算资源管理</h3><h4 id="基于Requests和Limits的Pod调度机制"><a href="#基于Requests和Limits的Pod调度机制" class="headerlink" title="基于Requests和Limits的Pod调度机制"></a>基于Requests和Limits的Pod调度机制</h4><p>Pod创建成功时，Kubernetes调度器(Scheduler)会为该Pod选择一个节点来执行。<br>对于每种计算资源(CPU和Memory)而言，每个节点都有一个能用于运行Pod的最大容量值。<br>调度器在调度时，首先要确保调度后该节点上所有Pod的CPU和内存的Requests总和，不超过该节点能提供给Pod使用的CPU和Memory的最大容量值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">例如，某个节点上的CPU资源充足，而内存为4GB，其中3GB可以运行Pod，而某Pod的MemoryRequests为1GB、Limits为2GB，那么在这个节点上最多可以运行3个这样的Pod。</span><br><span class="line"></span><br><span class="line">这里需要注意：可能某节点上的实际资源使用量非常低，但是已运行Pod配置的Requests值的总和非常高，再加上需要调度的Pod的Requests值，会超过该节点提供给Pod的资源容量上限</span><br><span class="line">这时Kubernetes仍然不会将Pod调度到该节点上。</span><br><span class="line">如果Kubernetes将Pod调度到该节点上，之后该节点上运行的Pod又面临服务峰值等情况，就可能导致Pod资源短缺。</span><br><span class="line"></span><br><span class="line">接着上面的例子，假设该节点已经启动3个Pod实例，而这3个Pod的实际内存使用都不足500MB，那么理论上该节点的可用内存应该大于1.5GB。</span><br><span class="line">但是由于该节点的Pod Requests总和已经达到节点的可用内存上限</span><br><span class="line">因此Kubernetes不会再将任何Pod实例调度到该节点上。</span><br></pre></td></tr></table></figure>
<h4 id="Requests和Limits的背后机制"><a href="#Requests和Limits的背后机制" class="headerlink" title="Requests和Limits的背后机制"></a>Requests和Limits的背后机制</h4><p>kubelet在启动Pod的某个容器时，会将容器的Requests和Limits值转化为相应的容器启动参数传递给容器执行器(Docker或者rkt)<br>如果容器的执行环境是Docker，那么容器的如下4个参数是这样传递给Docker的。</p>
<blockquote>
<p>spec.container[].resources.requests.cpu<br>spec.container[].resources.limits.cpu<br>spec.container[].resources.requests.memory<br>spec.container[].resources.limits.memory</p>
</blockquote>
<p>如果一个容器在运行过程中使用了超出了其内存Limits配置的内存限制值，那么它可能会被杀掉，如果这个容器是一个可重启的容器，那么之后它会被kubelet重新启动。<br>因此对容器的Limits配置需要进行准确测试和评估。与内存Limits不同的是，CPU在容器技术中属于可压缩资源<br>因此对CPU的Limits配置一般不会因为偶然超标使用而导致容器被系统杀掉。</p>
<h4 id="计算资源相关常见问题分析"><a href="#计算资源相关常见问题分析" class="headerlink" title="计算资源相关常见问题分析"></a>计算资源相关常见问题分析</h4><ul>
<li>Pod状态为Pending，错误信息为FailedScheduling。如果Kubernetes调度器在集群中找不到合适的节点来运行Pod，那么这个Pod会一直处于未调度状态<ul>
<li>添加更多的节点到集群中。</li>
<li>停止一些不必要的运行中的Pod，释放资源。</li>
<li>检查Pod的配置</li>
</ul>
</li>
<li>容器被强行终止(Terminated):如果容器使用的资源超过了它配置的Limits，那么该容器可能会被强制终止。</li>
</ul>
<h4 id="资源配置范围管理-LimitRange"><a href="#资源配置范围管理-LimitRange" class="headerlink" title="资源配置范围管理(LimitRange)"></a>资源配置范围管理(LimitRange)</h4><p>默认情况下Kubernetes不会对Pod加上CPU和内存限制，这意味着Kubernetes系统中任何Pod都可以使用其所在节点的所有可用的CPU和内存。</p>
<p>通过配置Pod的计算资源Requests和Limits，我们可以限制Pod的资源使用，<br>但对于Kubernetes集群管理员而言，配置每一个Pod的Requests和Limits是烦琐的，而且很受限制</p>
<ul>
<li>集群中的每个节点都有2GB内存，集群管理员不希望任何Pod申请超过2GB的内存</li>
<li>集群由同一个组织中的两个团队共享，分别运行生产环境和开发环境。生产环境最多可以使用8GB内存，而开发环境最多可以使用512MB内存。这两个环境创建不同的命名空间，并为每个命名空间设置不同的限制来满足这个需求。</li>
<li>用户创建Pod时使用的资源可能会刚好比整个机器资源的上限稍小，而恰好剩下的资源大小非常尴尬,不足以运行其他任务但整个集群加起来又非常浪费</li>
</ul>
<p>针对这些需求，Kubernetes提供了LimitRange机制对Pod和容器的Requests和Limits配置进一步做出限制。</p>
<ul>
<li>创建一个Namespace</li>
<li>为Namespace设置LimitRange</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">LimitRange</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">mylimits</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  limits:</span></span><br><span class="line"><span class="attr">  - max:</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="string">"4"</span></span><br><span class="line"><span class="attr">      memory:</span> <span class="number">2</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">    min:</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="number">200</span><span class="string">m</span></span><br><span class="line"><span class="attr">      memory:</span> <span class="number">6</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">    maxLimitRequestRatio:</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">      memory:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">  - default:</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="number">300</span><span class="string">m</span></span><br><span class="line"><span class="attr">      memory:</span> <span class="number">200</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">    defaultRequest:</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="number">200</span><span class="string">m</span></span><br><span class="line"><span class="attr">      memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">    max:</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="string">"2"</span></span><br><span class="line"><span class="attr">      memory:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">    min:</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">      memory:</span> <span class="number">3</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">    maxLimitRequestRatio:</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">      memory:</span> <span class="number">4</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">Container</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>不论是CPU还是内存，在LimitRange中，Pod和Container都可以设置Min、Max和MaxLimit/Requests Ratio参数<br>Container还可以设置Default Request和Default Limit参数，而Pod不能设置Default Request和Default Limit参数。</p>
</blockquote>
<p>对Pod和Container的参数解释如下</p>
<ul>
<li>Container的Min(上面的100m和3Mi)是Pod中所有容器的Requests值下限；Container的Max(上面的2和1Gi)是Pod中所有容器的Limits值上限;</li>
<li>Container的Default Request(上面的200m和100Mi)是Pod中所有未指定Requests值的容器的默认Requests值;Container的Default Limit(上面的300m和200Mi)是Pod中所有未指定Limits值的容器的默认Limits值。</li>
<li>对于同一资源类型，这4个参数必须满足以下关系：Min ≤ Default Request ≤ Default Limit ≤Max。</li>
<li>Pod的Min(上面的200m和6Mi)是Pod中所有容器的Requests值的总和下限；Pod的Max(上面的4和2Gi)是Pod中所有容器的Limits值的总和上限。当容器未指定Requests值或者Limits值时，将使用Container的Default Request值或者Default Limit值。</li>
<li>Container的Max Limit/Requests Ratio(上面的5和4)限制了Pod中所有容器的Limits值与Requests值的比例上限；而Pod的Max Limit/Requests Ratio(上面的3和2)限制了Pod中所有容器的Limits值总和与Requests值总和的比例上限。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">如果设置了Container的Max，那么对于该类资源而言，整个集群中的所有容器都必须设置Limits，否则无法成功创建。</span><br><span class="line">Pod内的容器未配置Limits时将使用Default Limit的值(本例中的300m CPU和200MiB内存),如果也未配置Default，则无法成功创建。</span><br><span class="line"></span><br><span class="line">如果设置了Container的Min，那么对于该类资源而言，整个集群中的所有容器都必须设置Requests。</span><br><span class="line">如果创建Pod的容器时未配置该类资源的Requests，那么在创建过程中会报验证错误。</span><br><span class="line">Pod里容器的Requests在未配置时，可以使用默认值defaultRequest(本例中的200m CPU和100MiB内存)；</span><br><span class="line">如果未配置而又没有使用默认值defaultRequest，那么会默认等于该容器的Limits；如果此时Limits也未定义，就会报错。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">对于任意一个Pod而言，该Pod中所有容器的Requests总和必须大于或等于6MiB，而且所有容器的Limits总和必须小于或等于1GiB；</span><br><span class="line">同样所有容器的CPU Requests总和必须大于或等于200m，而且所有容器的CPU Limits总和必须小于或等于2。</span><br><span class="line"></span><br><span class="line">Pod里任何容器的Limits与Requests的比例都不能超过Container的Max Limit/RequestsRatio;</span><br><span class="line">Pod里所有容器的Limits总和与Requests的总和的比例不能超过Pod的Max Limit/RequestsRatio。</span><br></pre></td></tr></table></figure>
<h4 id="资源服务质量管理-Resource-QoS"><a href="#资源服务质量管理-Resource-QoS" class="headerlink" title="资源服务质量管理(Resource QoS)"></a>资源服务质量管理(Resource QoS)</h4><p>在Kubernetes的资源QoS体系中，需要保证高可靠性的Pod可以申请可靠资源，而一些不需要高可靠性的Pod可以申请可靠性较低或者不可靠的资源</p>
<p>Kubernetes中Pod的Requests和Limits资源配置有如下特点。</p>
<ul>
<li>如果Pod配置的Requests值等于Limits值，那么该Pod可以获得的资源是完全可靠的。</li>
<li>如果Pod的Requests值小于Limits值，那么该Pod获得的资源可分成两部分：<ul>
<li>完全可靠的资源，资源量的大小等于Requests值；</li>
<li>不可靠的资源，资源量最大等于Limits与Requests的差额,这份不可靠的资源能够申请到多少，取决于当时主机上容器可用资源的余量。</li>
</ul>
</li>
</ul>
<p>通过这种机制Kubernetes可以实现节点资源的超售(Over Subscription)<br>比如在CPU完全充足的情况下，某机器共有32GiB内存可提供给容器使用，容器配置为Requests值1GiB，Limits值为2GiB<br>那么在该机器上最多可以同时运行32个容器，每个容器最多可以使用2GiB内存<br>如果这些容器的内存使用峰值能错开，那么所有容器都可以正常运行。</p>
<h5 id="Requests和Limits对不同计算资源类型的限制机制"><a href="#Requests和Limits对不同计算资源类型的限制机制" class="headerlink" title="Requests和Limits对不同计算资源类型的限制机制"></a>Requests和Limits对不同计算资源类型的限制机制</h5><p>容器的资源配置满足以下两个条件:</p>
<ul>
<li>Requests&lt;=节点可用资源</li>
<li>Requests&lt;=Limits。</li>
</ul>
<blockquote>
<p>可压缩资源(CPU)</p>
<ul>
<li>Pod可以得到Pod的Requests配置的CPU使用量，而能否使用超过Requests值的部分取决于系统的负载和调度以及Limit。</li>
<li>空闲CPU资源按照容器Requests值的比例分配。  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">容器A的CPU配置为Requests 1Limits 10，容器B的CPU配置为Request 2 Limits 8，A和B同时运行在一个节点上，初始状态下容器的可用CPU为3cores</span><br><span class="line">那么A和B恰好得到在它们的Requests中定义的CPU用量:即1CPU和2CPU。</span><br><span class="line">如果A和B都需要更多的CPU资源，而恰好此时系统的其他任务释放出1.5CPU</span><br><span class="line">那么这1.5CPU将按照A和B的Requests值的比例1∶2分配给A和B，即最终A可使用1.5CPU，B可使用3CPU。</span><br></pre></td></tr></table></figure>
</li>
</ul>
</blockquote>
<ul>
<li>如果Pod使用了超过在Limits 10中配置的CPU用量，那么cgroups会对Pod中的容器的CPU使用进行限流(Throttled);如果Pod没有配置Limits 10，那么Pod会尝试抢占所有空闲的CPU资源</li>
</ul>
<blockquote>
<p>不可压缩资源(内存)</p>
<ul>
<li>Pod可以得到在Requests中配置的内存。如果Pod使用的内存量小于它的Requests的配置，那么这个Pod可以正常运行(除非出现操作系统级别的内存不足等严重问题);如果Pod使用的内存量超过了它的Requests的配置，那么这个Pod有可能被Kubernetes杀掉;  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">比如Pod A使用了超过Requests而不到Limits的内存量，此时同一机器上另外一个Pod B之前只使用了远少于自己的Requests值的内存</span><br><span class="line">此时程序压力增大Pod B向系统申请的总量不超过自己的Requests值的内存，那么Kubernetes可能会直接杀掉Pod A;</span><br><span class="line">另外一种情况是Pod A使用了超过Requests而不到Limits的内存量，此时Kubernetes将一个新的Pod调度到这台机器上，新的Pod需要使用内存</span><br><span class="line">而只有Pod A使用了超过了自己的Requests值的内存，那么Kubernetes也可能会杀掉Pod A来释放内存资源。</span><br></pre></td></tr></table></figure>
</li>
</ul>
</blockquote>
<ul>
<li>如果Pod使用的内存量超过了它的Limits设置，那么操作系统内核会杀掉Pod所有容器的所有进程中使用内存最多的一个，直到内存不超过Limits为止</li>
</ul>
<h5 id="对调度策略的影响"><a href="#对调度策略的影响" class="headerlink" title="对调度策略的影响"></a>对调度策略的影响</h5><ul>
<li>Kubernetes的kubelet通过计算Pod中所有容器的Requests的总和来决定对Pod的调度。</li>
<li>不管是CPU还是内存，Kubernetes调度器和kubelet都会确保节点上所有Pod的Requests的总和不会超过在该节点上可分配给容器使用的资源容量上限。</li>
</ul>
<h5 id="服务质量等级-QoS-Classes"><a href="#服务质量等级-QoS-Classes" class="headerlink" title="服务质量等级(QoS Classes)"></a>服务质量等级(QoS Classes)</h5><p>在一个超用(Over Committed，容器Limits总和大于系统容量上限)系统中，由于容器负载的波动可能导致操作系统的资源不足，最终可能导致部分容器被杀掉。</p>
<blockquote>
<p>在这种情况下，我们当然会希望优先杀掉那些不太重要的容器，那么如何衡量重要程度呢？<br>Kubernetes将容器划分成3个QoS等级：Guaranteed(完全可靠的)、Burstable(弹性波动、较可靠的)和BestEffort(尽力而为、不太可靠的)</p>
</blockquote>
<p>这三种优先级依次递减</p>
<p><img src="/2021/06/15/k8s-devops/image-02.png" width="500px"></p>
<p>为了简化模式及避免引入太多的复杂性，QoS级别直接由Requests和Limits来定义</p>
<blockquote>
<p>Guaranteed</p>
</blockquote>
<p>Pod中的所有容器对所有资源类型都定义了Limits和Requests，并且所有容器的Limits值都和Requests值全部相等(且都不为0)，那么该Pod的QoS级别就是Guaranteed。</p>
<blockquote>
<p>BestEffort</p>
</blockquote>
<p>如果Pod中所有容器都未定义资源配置(Requests和Limits都未定义)，那么该Pod的QoS级别就是BestEffort。</p>
<blockquote>
<p>Burstable</p>
</blockquote>
<p>当一个Pod既不为Guaranteed级别，也不为BestEffort级别时，该Pod的QoS级别就是Burstable。</p>
<p>Burstable级别的Pod包括两种情况。</p>
<ul>
<li>第1种情况：Pod中的一部分容器在一种或多种资源类型的资源配置中定义了Requests值和Limits值(都不为0)，且Requests值小于Limits值；</li>
<li>第2种情况：Pod中的一部分容器未定义资源配置(Requests和Limits都未定义)。</li>
</ul>
<p>注意:在容器未定义Limits时，Limits值默认等于节点资源容量的上限。</p>
<h6 id="Kubernetes-QoS的工作特点"><a href="#Kubernetes-QoS的工作特点" class="headerlink" title="Kubernetes QoS的工作特点"></a>Kubernetes QoS的工作特点</h6><p>由于内存是不可压缩的资源，所以针对内存资源紧缺的情况，会按照以下逻辑进行处理。</p>
<ul>
<li>BestEffort Pod的优先级最低，在这类Pod中运行的进程会在系统内存紧缺时被第一优先杀掉。</li>
<li>Burstable Pod的优先级居中，这类Pod初始时会分配较少的可靠资源，但可以按需申请更多的资源。</li>
<li>Guaranteed Pod的优先级最高，而且一般情况下这类Pod只要不超过其资源Limits的限制就不会被杀掉。</li>
</ul>
<h6 id="OOM计分系统"><a href="#OOM计分系统" class="headerlink" title="OOM计分系统"></a>OOM计分系统</h6><p>OOM(Out Of Memory)计分规则包括如下内容。</p>
<ul>
<li>OOM计分的计算方法为：计算进程使用内存在系统中占的百分比，取其中不含百分号的数值，再乘以10的结果，这个结果是进程OOM的基础分；<br>  将进程OOM基础分的分值再加上这个进程的OOM分数调整值OOM_SCORE_ADJ的值，作为进程OOM的最终分值(除root启动的进程外)。<br>  在系统发生OOM时，OOM Killer会优先杀掉OOM计分更高的进程。</li>
<li>进程的OOM计分的基本分数值范围是0～1000，如果A进程的调整值OOM_SCORE_ADJ减去B进程的调整值的结果大于1000，那么A进程的OOM计分最终值必然大于B进程，会优先杀掉A进程。</li>
<li>不论调整OOM_SCORE_ADJ值为多少，任何进程的最终分值范围也是0～1000。</li>
</ul>
<table>
<thead>
<tr>
<th>Service 质量</th>
<th>oom_score_adj</th>
</tr>
</thead>
<tbody>
<tr>
<td>Guaranteed</td>
<td>-998</td>
</tr>
<tr>
<td>Burstable</td>
<td>min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)</td>
</tr>
<tr>
<td>BestEffort</td>
<td>1000</td>
</tr>
</tbody>
</table>
<ul>
<li>BestEffort Pod设置OOM_SCORE_ADJ调整值为1000，因此BestEffort Pod中容器里所有进程的OOM最终分肯定是1000。</li>
<li>Guaranteed Pod设置OOM_SCORE_ADJ调整值为-998，因此Guaranteed Pod中容器里所有进程的OOM最终分一般是0或者1(因为基础分不可能是1000)。</li>
<li>Burstable Pod规则分情况说明  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">如果Burstable Pod的内存Requests超过了系统可用内存的99.8%，那么这个Pod的OOM_SCORE_ADJ调整值固定为2；</span><br><span class="line">否则设置OOM_SCORE_ADJ调整值为1000-10×(% of memory requested);</span><br><span class="line">如果内存Requests为0，那么OOM_SCORE_ADJ调整值固定为999。</span><br><span class="line">这样的规则能确保OOM_SCORE_ADJ调整值的范围为2～999，而Burstable Pod中所有进程的OOM最终分数范围为2～1000。</span><br><span class="line">Burstable Pod进程的OOM最终分数始终大于Guaranteed Pod的进程得分，因此它们会被优先杀掉。</span><br><span class="line">如果一个Burstable Pod使用的内存比它的内存Requests少，那么可以肯定的是它的所有进程的OOM最终分数会小于1000，此时能确保它的优先级高于BestEffort Pod。</span><br><span class="line">如果在一个Burstable Pod的某个容器中某个进程使用的内存比容器的Requests值高，那么这个进程的OOM最终分数会是1000，否则它的OOM最终分会小于1000。</span><br><span class="line">假设在下面的容器中有一个占用内存非常大的进程，那么当一个使用内存超过其Requests的Burstable Pod与另外一个使用内存少于其Requests的Burstable Pod发生内存竞争冲突时，前者的进程会被系统杀掉。</span><br><span class="line">如果在一个Burstable Pod内部有多个进程的多个容器发生内存竞争冲突，那么此时OOM评分只能作为参考，不能保证完全按照资源配置的定义来执行OOM Kill。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>OOM还有一些特殊的计分规则，如下所述。</p>
<ul>
<li>kubelet进程和Docker进程的调整值OOM_SCORE_ADJ为-998。</li>
<li>如果配置进程调整值OOM_SCORE_ADJ为-999，那么这类进程不会被OOM Killer杀掉。</li>
</ul>
<h6 id="QoS的需要解决的问题"><a href="#QoS的需要解决的问题" class="headerlink" title="QoS的需要解决的问题"></a>QoS的需要解决的问题</h6><ul>
<li>当前的QoS策略都是假定主机不启用内存Swap。如果主机启用了Swap，那么上面的QoS策略可能会失效<br>  由于Kubernetes和Docker尚不支持内存Swap空间的隔离机制，所以这一功能暂时还未实现。</li>
<li>更丰富的QoS策略。当前的QoS策略都是基于Pod的资源配置(Requests和Limits)来定义的，而资源配置本身又承担着对Pod资源管理和限制的功能。两种不同维度的功能使用同一个参数来配置，可能会导致某些复杂需求无法满足<br>  比如当前Kubernetes无法支持弹性的、高优先级的Pod。自定义QoS优先级能提供更大的灵活性，完美地实现各类需求，但同时会引入更高的复杂性，而且过于灵活的设置会给予用户过高的权限，对系统管理也提出了更大的挑战。</li>
</ul>
<p><br><br><br></p>
<h2 id="Pod驱逐机制"><a href="#Pod驱逐机制" class="headerlink" title="Pod驱逐机制"></a>Pod驱逐机制</h2><p>kubelet需要解决硬件资源紧缺保证Node的稳定</p>
<p><img src="/2021/06/15/k8s-devops/image-01.png" width="600px"></p>
<p><code>memory.available</code>的值取自cgroupfs</p>
<p>kubelet支持以下两种文件系统。</p>
<ul>
<li>(1)nodefs：保存kubelet的卷和守护进程日志等。</li>
<li>(2)imagefs：在容器运行时保存镜像及可写入层。</li>
</ul>
<h3 id="驱逐阈值"><a href="#驱逐阈值" class="headerlink" title="驱逐阈值"></a>驱逐阈值</h3><h4 id="软驱逐"><a href="#软驱逐" class="headerlink" title="软驱逐"></a>软驱逐</h4><p><code>--eviction-soft</code>：描述驱逐阈值(例如memory.available&lt;1.5GiB)，如果满足这一条件的持续时间超过宽限期，就会触发对Pod的驱逐动作。</p>
<p><code>--eviction-soft-grace-period</code>：驱逐宽限期(例如memory.available=1m30s)，用于定义达到软阈值之后持续时间超过多久才进行驱逐。</p>
<p><code>--eviction-max-pod-grace-period</code>：在达到软阈值后，终止Pod的最大宽限时间(单位为s)。</p>
<h4 id="硬驱逐"><a href="#硬驱逐" class="headerlink" title="硬驱逐"></a>硬驱逐</h4><p><code>--evction-hard=memory.available&lt;100Mi</code>:<code>--eviction-hard</code>驱逐硬阈值，一旦达到阈值，就会触发对Pod的驱逐操作</p>
<h3 id="驱逐频率"><a href="#驱逐频率" class="headerlink" title="驱逐频率"></a>驱逐频率</h3><p><code>kubelet的--housekeeping-interval</code>参数定义了一个时间间隔，kubelet每隔一个这样的时间间隔就会对驱逐阈值进行评估。</p>
<h3 id="节点的状况"><a href="#节点的状况" class="headerlink" title="节点的状况"></a>节点的状况</h3><p>kubelet会持续向Master报告节点状态的更新过程，这一频率由参数<code>--node-status-update-frequency</code>指定，默认为10s。</p>
<h3 id="节点状况的抖动"><a href="#节点状况的抖动" class="headerlink" title="节点状况的抖动"></a>节点状况的抖动</h3><p>如果一个节点的状况在软阈值的上下抖动，但是又没有超过宽限期，则会导致该节点的相应状态在True和False之间不断变换，可能会对调度的决策过程产生负面影响。</p>
<p>使用参数<code>--eviction-pressure-transition-period</code>(在脱离压力状态前需要等待的时间，默认值为5m0s)，为kubelet设置在脱离压力状态之前需要等待的时间。</p>
<h3 id="回收Node级别的资源"><a href="#回收Node级别的资源" class="headerlink" title="回收Node级别的资源"></a>回收Node级别的资源</h3><p>kubelet在驱逐用户Pod之前，会尝试回收Node级别的资源。在观测到磁盘压力的情况下，基于服务器是否为容器运行时定义了独立的imagefs，会导致不同的资源回收过程。</p>
<h4 id="有Imagefs的情况"><a href="#有Imagefs的情况" class="headerlink" title="有Imagefs的情况"></a>有Imagefs的情况</h4><ul>
<li>(1)如果nodefs文件系统达到了驱逐阈值，则kubelet会删掉死掉的Pod、容器来清理空间。</li>
<li>(2)如果imagefs文件系统达到了驱逐阈值，则kubelet会删掉所有无用的镜像来清理空间。</li>
</ul>
<h4 id="没有Imagefs的情况"><a href="#没有Imagefs的情况" class="headerlink" title="没有Imagefs的情况"></a>没有Imagefs的情况</h4><p>如果nodefs文件系统达到了驱逐阈值，则kubelet会按照下面的顺序来清理空间。</p>
<ul>
<li>(1)删除死掉的Pod、容器。</li>
<li>(2)删除所有无用的镜像。</li>
</ul>
<h3 id="驱逐用户的Pod"><a href="#驱逐用户的Pod" class="headerlink" title="驱逐用户的Pod"></a>驱逐用户的Pod</h3><p>kubelet会按照下面的标准对Pod的驱逐行为进行判断。</p>
<ul>
<li>◎ Pod要求的服务质量。</li>
<li>◎ 根据Pod调度请求的被耗尽资源的消耗量。</li>
</ul>
<p>接下来kubelet按照下面的顺序驱逐Pod。</p>
<ul>
<li>◎ BestEffort：紧缺资源消耗最多的Pod最先被驱逐。</li>
<li>◎ Burstable：根据相对请求来判断，紧缺资源消耗最多的Pod最先被驱逐，如果没有Pod超出它们的请求，则策略会瞄准紧缺资源消耗量最大的Pod。</li>
<li>◎ Guaranteed：根据相对请求来判断，紧缺资源消耗最多的Pod最先被驱逐，如果没有Pod超出它们的请求，策略会瞄准紧缺资源消耗量最大的Pod。</li>
</ul>
<h3 id="资源最少回收量"><a href="#资源最少回收量" class="headerlink" title="资源最少回收量"></a>资源最少回收量</h3><p>驱逐Pod可能只回收了很少的资源，这就导致了kubelet反复触发驱逐阈值。另外，回收磁盘这样的资源，是需要消耗时间的。</p>
<p>kubelet可以对每种资源都定义minimum-reclaim。<br>kubelet一旦监测到了资源压力，就会试着回收不少于minimum-reclaim的资源数量，使得资源消耗量回到期望的范围</p>
<h3 id="节点资源紧缺情况下的系统行为"><a href="#节点资源紧缺情况下的系统行为" class="headerlink" title="节点资源紧缺情况下的系统行为"></a>节点资源紧缺情况下的系统行为</h3><h4 id="调度器的行为"><a href="#调度器的行为" class="headerlink" title="调度器的行为"></a>调度器的行为</h4><p>在节点资源紧缺的情况下(MemoryPressure、DiskPressure)，节点会向Master报告这一状况。在Master上运行的调度器(Scheduler)以此为信号，不再继续向该节点调度新的Pod</p>
<h4 id="Node的OOM行为"><a href="#Node的OOM行为" class="headerlink" title="Node的OOM行为"></a>Node的OOM行为</h4><table>
<thead>
<tr>
<th>Service 质量</th>
<th>oom_score_adj</th>
</tr>
</thead>
<tbody>
<tr>
<td>Guaranteed</td>
<td>-998</td>
</tr>
<tr>
<td>Burstable</td>
<td>min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)</td>
</tr>
<tr>
<td>BestEffort</td>
<td>1000</td>
</tr>
</tbody>
</table>
<p>kubelet根据Pod的QoS为每个容器都设置了一个oom_score_adj值</p>
<p>如果节点在kubelet能够回收内存之前遭遇了系统的OOM(内存不足)，节点则依赖oom_killer的设置进行响应</p>
<p>如果kubelet无法在系统OOM之前回收足够的内存，则oom_killer会根据内存使用比率来计算oom_score，将得出的结果和oom_score_adj相加，得分最高的Pod首先被驱逐。</p>
<blockquote>
<p>预期的行为应该是拥有最低服务质量并消耗和调度请求相关内存量最多的容器第一个被结束，以回收内存。</p>
</blockquote>
<h3 id="现阶段的问题"><a href="#现阶段的问题" class="headerlink" title="现阶段的问题"></a>现阶段的问题</h3><blockquote>
<p>1．kubelet无法及时观测到内存压力kubelet<br>目前从cAdvisor定时获取内存使用状况的统计情况。如果内存使用在这个时间段内发生了快速增长，且kubelet无法观察到MemoryPressure，则可能会触发OOMKiller</p>
</blockquote>
<blockquote>
<p>2．kubelet可能会错误地驱逐更多的Pod</p>
</blockquote>
<p>这也是状态搜集存在时间差导致的。未来可能会通过按需获取根容器的统计信息来减少计算偏差(<a href="https://github.com/google/cadvisor/issues/1247)。" target="_blank" rel="noopener">https://github.com/google/cadvisor/issues/1247)。</a></p>
<p><br><br><br></p>
<h2 id="Pod-Disruption-Budget-主动驱逐保护"><a href="#Pod-Disruption-Budget-主动驱逐保护" class="headerlink" title="Pod Disruption Budget(主动驱逐保护)"></a>Pod Disruption Budget(主动驱逐保护)</h2><ul>
<li>节点的维护或升级时（kubectl drain）。</li>
<li>对应用的自动缩容操作（autoscaling down）。</li>
</ul>
<p>由于节点不可用（Not Ready）导致的Pod驱逐就不能被称为主动了</p>
<p>对PodDisruptionBudget的定义包括如下两部分。</p>
<ul>
<li>Label Selector：用于筛选被管理的Pod。</li>
<li>minAvailable：指定驱逐过程需要保障的最少Pod数量。</li>
</ul>
<p>minAvailable可以是一个数字，也可以是一个百分比，例如100%就表示不允许进行主动驱逐。</p>
<h3 id="实践操作"><a href="#实践操作" class="headerlink" title="实践操作"></a>实践操作</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy/v1beta1</span>  </span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodDisruptionBudget</span>  </span><br><span class="line"><span class="attr">metadata:</span>  </span><br><span class="line"><span class="attr"> name:</span> <span class="string">zk-pdb</span>  </span><br><span class="line"><span class="attr">spec:</span>  </span><br><span class="line"><span class="attr"> minAvailable:</span> <span class="number">2</span>  </span><br><span class="line"><span class="attr"> selector:</span>  </span><br><span class="line"><span class="attr">   matchLabels:</span>  </span><br><span class="line"><span class="attr">     app:</span> <span class="string">zookeeper</span></span><br></pre></td></tr></table></figure>
<p><br><br><br></p>
<h2 id="资源配额管理-Resource-Quotas"><a href="#资源配额管理-Resource-Quotas" class="headerlink" title="资源配额管理(Resource Quotas)"></a>资源配额管理(Resource Quotas)</h2><p>如果一个Kubernetes集群被多个用户或者多个团队共享，就需要考虑资源公平使用的问题，因为某个用户可能会使用超过基于公平原则分配给其的资源量。</p>
<p>我们可以定义资源配额，这个资源配额可以为每个命名空间都提供一个总体的资源使用的限制：它可以限制命名空间中某种类型的对象的总数目上限，也可以设置命名空间中Pod可以使用的计算资源的总上限。</p>
<blockquote>
<p>下面的例子展示了一个非常适合使用资源配额来做资源控制管理的场景。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- 集群共有32GB内存和16 CPU，两个小组。A小组使用20GB内存和10 CPU，B小组使用10GB内存和2 CPU，剩下的2GB内存和2 CPU作为预留。</span><br><span class="line">- 在名为testing的命名空间中，限制使用1 CPU和1GB内存; 在名为production的命名空间中，资源使用不受限制。</span><br></pre></td></tr></table></figure></p>
</blockquote>
<h3 id="在Master中开启资源配额选型"><a href="#在Master中开启资源配额选型" class="headerlink" title="在Master中开启资源配额选型"></a>在Master中开启资源配额选型</h3><p>kube-apiserver的–admission-control参数值中添加ResourceQuota参数进行开启</p>
<h4 id="计算资源配额-Compute-Resource-Quota"><a href="#计算资源配额-Compute-Resource-Quota" class="headerlink" title="计算资源配额(Compute Resource Quota)"></a>计算资源配额(Compute Resource Quota)</h4><p>资源配额可以限制一个命名空间中所有Pod的计算资源的总和</p>
<p><img src="/2021/06/15/k8s-devops/image-03.png" width="800px"></p>
<h4 id="存储资源配额-Volume-Count-Quota"><a href="#存储资源配额-Volume-Count-Quota" class="headerlink" title="存储资源配额(Volume Count Quota)"></a>存储资源配额(Volume Count Quota)</h4><p>可以在给定的命名空间中限制所使用的存储资源(Storage Resources)的总量</p>
<p><img src="/2021/06/15/k8s-devops/image-04.png" width="800px"></p>
<h4 id="对象数量配额-Object-Count-Quota"><a href="#对象数量配额-Object-Count-Quota" class="headerlink" title="对象数量配额(Object Count Quota)"></a>对象数量配额(Object Count Quota)</h4><p><img src="/2021/06/15/k8s-devops/image-05.png" width="800px"></p>
<p>指定类型的对象数量可以被限制</p>
<h3 id="配额的作用域-Quota-Scopes"><a href="#配额的作用域-Quota-Scopes" class="headerlink" title="配额的作用域(Quota Scopes)"></a>配额的作用域(Quota Scopes)</h3><p>每项资源配额都可以单独配置一组作用域，配置了作用域的资源配额只会对符合其作用域的资源使用情况进行计量和限制，作用域范围内超过了资源配额的请求都会报验证错误。</p>
<p><img src="/2021/06/15/k8s-devops/image-06.png" width="700px"></p>
<p>BestEffort作用域可以限定资源配额来追踪pods资源的使用;<br>Terminating、NotTerminating和NotBestEffort这三种作用域可以限定资源配额来追踪以下资源的使用</p>
<ul>
<li>cpu</li>
<li>limits.cpu</li>
<li>limits.memory</li>
<li>memory</li>
<li>pods</li>
<li>requests.cpu</li>
<li>requests.memory</li>
</ul>
<h3 id="在资源配额-ResourceQuota-中设置Requests和Limits"><a href="#在资源配额-ResourceQuota-中设置Requests和Limits" class="headerlink" title="在资源配额(ResourceQuota)中设置Requests和Limits"></a>在资源配额(ResourceQuota)中设置Requests和Limits</h3><p>如果在资源配额中指定了requests.cpu或requests.memory，那么它会强制要求每个容器都配置自己的CPU Requests或CPU Limits(可使用LimitRange提供的默认值)。<br>同理如果在资源配额中指定了limits.cpu或limits.memory，那么它也会强制要求每个容器都配置自己的内存Requests或内存Limits(可使用LimitRange提供的默认值)。</p>
<h3 id="资源配额的定义"><a href="#资源配额的定义" class="headerlink" title="资源配额的定义"></a>资源配额的定义</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Namespace</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">quota-example</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#################################</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ResourceQuota</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">object-counts</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  hard:</span></span><br><span class="line"><span class="attr">    persistentvolumeclaims:</span> <span class="string">"2"</span> <span class="comment"># 持久存储卷</span></span><br><span class="line">    <span class="string">services.loadbalancers:</span> <span class="string">"2"</span> <span class="comment"># 负载均衡器</span></span><br><span class="line">    <span class="string">services.nodeports:</span> <span class="string">"0"</span> <span class="comment"># NodePort</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#################################</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ResourceQuota</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">compute-resources</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  hard:</span></span><br><span class="line"><span class="attr">    pods:</span> <span class="string">"4"</span></span><br><span class="line">    <span class="string">requests.cpu:</span> <span class="string">"1"</span></span><br><span class="line">    <span class="string">requests.memory:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line">    <span class="string">limits.cpu:</span> <span class="string">"2"</span></span><br><span class="line">    <span class="string">limits.memory:</span> <span class="number">2</span><span class="string">Gi</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#################################</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">LimitRange</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">limits</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  limits:</span></span><br><span class="line"><span class="attr">  - default:</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="number">200</span><span class="string">m</span></span><br><span class="line"><span class="attr">      memory:</span> <span class="number">512</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">    defaultRequest:</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">      memory:</span> <span class="number">256</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">Container</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#################################</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ResourceQuota</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">best-effort</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  hard:</span></span><br><span class="line"><span class="attr">    pods:</span> <span class="string">"10"</span></span><br><span class="line"><span class="attr">  scopes:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">BestEffort</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#################################</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ResourceQuota</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">not-best-effort</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  hard:</span></span><br><span class="line"><span class="attr">    pods:</span> <span class="string">"4"</span></span><br><span class="line">    <span class="string">requests.cpu:</span> <span class="string">"1"</span></span><br><span class="line">    <span class="string">requests.memory:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line">    <span class="string">limits.cpu:</span> <span class="string">"2"</span></span><br><span class="line">    <span class="string">limits.memory:</span> <span class="number">2</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  scopes:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">NotBestEffort</span></span><br></pre></td></tr></table></figure>
<p><br><br><br></p>
<h2 id="共享存储机制"><a href="#共享存储机制" class="headerlink" title="共享存储机制"></a>共享存储机制</h2><ul>
<li>StorageClass的定义，管理员可以将存储资源定义为某种类别（Class），正如存储设备对于自身的配置描述（Profile），例如<code>快速存储</code>,<code>慢速存储</code>,<code>有数据冗余</code>,<code>无数据冗余</code>等。</li>
<li>PVC则是用户对存储资源的一个”申请”</li>
<li>PV是对底层网络共享存储的抽象, 将共享存储定义为一种”资源”</li>
</ul>
<h3 id="PV详解"><a href="#PV详解" class="headerlink" title="PV详解"></a>PV详解</h3><p>PV作为存储资源，主要包括存储能力、访问模式、存储类型、回收策略、后端存储类型等关键信息的设置</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">pv001</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">pv001</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  nfs:</span></span><br><span class="line"><span class="attr">    path:</span> <span class="string">/data/volumes/v1</span></span><br><span class="line"><span class="attr">    server:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">  accessModes:</span> <span class="string">["ReadWriteMany","ReadWriteOnce"]</span></span><br><span class="line"><span class="attr">  capacity:</span></span><br><span class="line"><span class="attr">    storage:</span> <span class="number">2</span><span class="string">Gi</span></span><br></pre></td></tr></table></figure>
<ul>
<li>存储能力（Capacity）</li>
<li>存储卷模式（Volume Mode）<br>  可选项包括Filesystem（文件系统）和Block（块设备）</li>
<li>访问模式（Access Modes）<ul>
<li>ReadWriteOnce（RWO）：读写权限，并且只能被单个Node挂载。</li>
<li>ReadOnlyMany（ROX）：只读权限，允许被多个Node挂载。</li>
<li>ReadWriteMany（RWX）：读写权限，允许被多个Node挂载。</li>
</ul>
</li>
<li>存储类别（Class）<br>  通过storageClassName参数指定一个StorageClass资源对象的名称</li>
<li>回收策略（Reclaim Policy）<ul>
<li>保留：保留数据，需要手工处理。</li>
<li>回收空间：简单清除文件的操作<code>（例如执行rm -rf /thevolume/*命令）</code>。</li>
<li>删除：与PV相连的后端存储完成Volume的删除操作（如AWS EBS、GCE PD、Azure Disk、OpenStack Cinder等设备的内部Volume清理）。</li>
</ul>
</li>
<li>挂载参数（Mount Options）</li>
<li>节点亲和性（Node Affinity）</li>
</ul>
<h3 id="PVC详解"><a href="#PVC详解" class="headerlink" title="PVC详解"></a>PVC详解</h3><p>PVC作为用户对存储资源的需求申请，主要包括存储空间请求、访问模式、PV选择条件和存储类别等信息的设置。</p>
<blockquote>
<p>PVC的关键配置参数说明如下。</p>
</blockquote>
<ul>
<li>资源请求（Resources）：描述对存储资源的请求，目前仅支持request.storage的设置，即存储空间大小。<ul>
<li>访问模式（Access Modes）：PVC也可以设置访问模式，用于描述用户应用对存储资源的访问权限。其三种访问模式的设置与PV的设置相同。</li>
<li>存储卷模式（Volume Modes）：PVC也可以设置存储卷模式，用于描述希望使用的PV存储卷模式，包括文件系统和块设备。</li>
<li>PV选择条件（Selector）：通过对Label Selector的设置，可使PVC对于系统中已存在的各种PV进行筛选。系统将根据标签选出合适的PV与该PVC进行绑定。选择条件可以使用matchLabels和matchExpressions进行设置，如果两个字段都设置了，则Selector的逻辑将是两组条件同时满足才能完成匹配。</li>
<li>存储类别（Class）： PVC在定义时可以设定需要的后端存储的类别（通过storageClassName字段指定），以减少对后端存储特性的详细信息的依赖。只有设置了该Class的PV才能被系统选出，并与该PVC进行绑定。</li>
</ul>
</li>
</ul>
<h3 id="PV和PVC的生命周期"><a href="#PV和PVC的生命周期" class="headerlink" title="PV和PVC的生命周期"></a>PV和PVC的生命周期</h3><p><img src="/2021/06/15/k8s-devops/image-07.png" width="700px"></p>
<h4 id="资源供应"><a href="#资源供应" class="headerlink" title="资源供应"></a>资源供应</h4><p>Kubernetes支持两种资源的供应模式：静态模式（Static）和动态模式（Dynamic）</p>
<ul>
<li>静态模式：集群管理员手工创建许多PV，在定义PV时需要将后端存储的特性进行设置。</li>
<li>动态模式：集群管理员无须手工创建PV，而是通过StorageClass的设置对后端存储进行描述，标记为某种类型。</li>
</ul>
<h4 id="资源绑定"><a href="#资源绑定" class="headerlink" title="资源绑定"></a>资源绑定</h4><p>用户定义好PVC之后，系统将根据PVC对存储资源的请求（存储空间和访问模式）在已存在的PV中选择一个满足PVC要求的PV，一旦找到，就将该PV与用户定义的PVC进行绑定，用户的应用就可以使用这个PVC了。<br>如果在系统中没有满足PVC要求的PV，PVC则会无限期处于Pending状态，直到等到系统管理员创建了一个符合其要求的PV。<br>PV一旦绑定到某个PVC上，就会被这个PVC独占，不能再与其他PVC进行绑定了。</p>
<p>在这种情况下，当PVC申请的存储空间比PV的少时，整个PV的空间就都能够为PVC所用，可能会造成资源的浪费。<br>如果资源供应使用的是动态模式，则系统在为PVC找到合适的StorageClass后，将自动创建一个PV并完成与PVC的绑定。</p>
<h4 id="资源使用"><a href="#资源使用" class="headerlink" title="资源使用"></a>资源使用</h4><p>Pod使用Volume的定义，将PVC挂载到容器内的某个路径进行使用。Volume的类型为persistentVolumeClaim。<br>在容器应用挂载了一个PVC后，就能被持续独占使用。不过多个Pod可以挂载同一个PVC，应用程序需要考虑多个实例共同访问一块存储</p>
<h4 id="资源释放"><a href="#资源释放" class="headerlink" title="资源释放"></a>资源释放</h4><p>当用户对存储资源使用完毕后，用户可以删除PVC，与该PVC绑定的PV将会被标记为“已释放”，但还不能立刻与其他PVC进行绑定。<br>通过之前PVC写入的数据可能还被留在存储设备上，只有在清除之后该PV才能再次使用。</p>
<h4 id="资源回收"><a href="#资源回收" class="headerlink" title="资源回收"></a>资源回收</h4><p>对于PV，管理员可以设定回收策略，用于设置与之绑定的PVC释放资源之后如何处理遗留数据的问题。<br>只有PV的存储空间完成回收，才能供新的PVC绑定和使用。</p>
<p><img src="/2021/06/15/k8s-devops/image-08.png" width="700px"></p>
<p>在静态资源供应模式下，通过PV和PVC完成绑定，并供Pod使用的存储管理机制</p>
<p><img src="/2021/06/15/k8s-devops/image-09.png" width="700px"></p>
<p>在动态资源供应模式下，通过StorageClass和PVC完成资源动态绑定（系统自动生成PV），并供Pod使用的存储管理机制。</p>
<h3 id="StorageClass详解"><a href="#StorageClass详解" class="headerlink" title="StorageClass详解"></a>StorageClass详解</h3><p>StorageClass作为对存储资源的抽象定义，对用户设置的PVC申请屏蔽后端存储的细节，一方面减少了用户对于存储资源细节的关注，另一方面减轻了管理员手工管理PV的工作，由系统自动完成PV的创建和绑定，实现了动态的资源供应。<br>基于StorageClass的动态资源供应模式将逐步成为云平台的标准存储配置模式。</p>
<p>StorageClass的定义主要包括名称、后端存储的提供者（provisioner）和后端存储的相关参数配置。<br>StorageClass一旦被创建出来，则将无法修改。如需更改，则只能删除原StorageClass的定义重建。</p>
<h4 id="StorageClass的关键配置参数"><a href="#StorageClass的关键配置参数" class="headerlink" title="StorageClass的关键配置参数"></a>StorageClass的关键配置参数</h4><ul>
<li>提供者（Provisioner）</li>
<li>参数（Parameters）</li>
</ul>
<p><br><br><br></p>
<h1 id="Kubesphere迁移阿里Ack"><a href="#Kubesphere迁移阿里Ack" class="headerlink" title="Kubesphere迁移阿里Ack"></a>Kubesphere迁移阿里Ack</h1><h2 id="Node情况"><a href="#Node情况" class="headerlink" title="Node情况"></a>Node情况</h2><table>
<thead>
<tr>
<th>机器Ip</th>
<th>配额</th>
<th>角色</th>
<th>污点</th>
<th>容器CountLimit</th>
</tr>
</thead>
<tbody>
<tr>
<td>10.0.0.11</td>
<td>2核16g</td>
<td>work</td>
<td>无</td>
<td>110</td>
</tr>
<tr>
<td>10.0.0.12</td>
<td>4核16g</td>
<td>work</td>
<td>无</td>
<td>110</td>
</tr>
<tr>
<td>10.209.0.234</td>
<td>4核8g</td>
<td>work</td>
<td>无</td>
<td>110</td>
</tr>
<tr>
<td>10.0.0.48</td>
<td>8核16g</td>
<td>work</td>
<td>node.kubernetes.io/not-ready=:PreferNoSchedule</td>
<td>110</td>
</tr>
<tr>
<td>10.0.0.46</td>
<td>8核16g</td>
<td>work</td>
<td>trino=:PreferNoSchedule</td>
<td>110</td>
</tr>
<tr>
<td>10.0.0.47</td>
<td>8核16g</td>
<td>master,control-plan</td>
<td>无</td>
<td>110</td>
</tr>
</tbody>
</table>
<h2 id="业务Namespaces情况"><a href="#业务Namespaces情况" class="headerlink" title="业务Namespaces情况"></a>业务Namespaces情况</h2><table>
<thead>
<tr>
<th>名称</th>
<th>容器数量</th>
<th>Cpu用量</th>
<th>内存用量</th>
<th>资源配额(ResourceQuota)</th>
<th>LimitRange(Pod、Container)</th>
</tr>
</thead>
<tbody>
<tr>
<td>mia</td>
<td>21</td>
<td>1.93core</td>
<td>35g</td>
<td>Cpu无限制、内存无限制、count限制暂无(Pod、Service、Deployment、Job)</td>
<td>暂无 </td>
</tr>
<tr>
<td>online</td>
<td>1(maliang-v1:statefulSet)</td>
<td>0m</td>
<td>466m</td>
<td>Cpu无限制、内存无限制、、count限制暂无(Pod、Service、Deployment、Job)</td>
<td>暂无</td>
</tr>
</tbody>
</table>
<h2 id="核心Replica、Stateful、Daemon"><a href="#核心Replica、Stateful、Daemon" class="headerlink" title="核心Replica、Stateful、Daemon"></a>核心Replica、Stateful、Daemon</h2><table>
<thead>
<tr>
<th>名称</th>
<th>namespace</th>
<th>Kind</th>
<th>replicas</th>
<th>resources</th>
<th>Qos</th>
<th>其他 </th>
</tr>
</thead>
<tbody>
<tr>
<td>reports-v1(旧大表哥，在宋小福首页九宫格中使用)</td>
<td>mia</td>
<td>Deployment</td>
<td>1</td>
<td>暂无资源限制</td>
<td>BestEffort</td>
<td>暂无其他设置注意</td>
</tr>
<tr>
<td>trino-coordinator(计算引擎Master节点)</td>
<td>mia</td>
<td>Deployment</td>
<td>1</td>
<td>4G/8G,1/6C</td>
<td>Burstable</td>
<td>设置有点多,具体看yaml吧</td>
</tr>
<tr>
<td>trino-worker(计算引擎工作节点)</td>
<td>mia</td>
<td>Deployment</td>
<td>4</td>
<td>4G/8G,1/6C</td>
<td>Burstable</td>
<td>这个设置了很多nodeSelector、read及live探针,affinity<br>启动command,configMap,hostAliases</td>
</tr>
<tr>
<td>exporter-v1(大表哥导出服务)</td>
<td>mia</td>
<td>Deployment</td>
<td>2</td>
<td>无限制</td>
<td>BestEffort</td>
<td>这个有env,有python的一端command脚本</td>
</tr>
<tr>
<td>pan-backend-v1(大盘子服务端)</td>
<td>mia</td>
<td>Deployment</td>
<td>1</td>
<td>无限制</td>
<td>BestEffort</td>
<td>这个没啥特殊要注意的点</td>
</tr>
<tr>
<td>pan-client-v1(大盘子前端web服务)</td>
<td>mia</td>
<td>Deployment</td>
<td>3</td>
<td>无限制</td>
<td>BestEffort</td>
<td>这个没啥特殊要注意的点</td>
</tr>
<tr>
<td>app-search(全文搜索服务)</td>
<td>mia</td>
<td>Deployment</td>
<td>1</td>
<td>无限制</td>
<td>BestEffort</td>
<td>这个env设置了很多es的参数,还放了sessionkey</td>
</tr>
<tr>
<td>miaaa-2-es(大表哥数据同步至 app-search)</td>
<td>mia</td>
<td>Deployment</td>
<td>1</td>
<td>无限制</td>
<td>BestEffort</td>
<td>我觉得没啥特殊的</td>
</tr>
<tr>
<td>miaaa-v4(未知)</td>
<td>mia</td>
<td>Deployment</td>
<td>1</td>
<td>无限制</td>
<td>BestEffort</td>
<td>做了很多的hostAlias,设置了一些env</td>
</tr>
<tr>
<td>miaaa-client-pre(大表哥预发环境)</td>
<td>mia</td>
<td>Deployment</td>
<td>1</td>
<td>无限制</td>
<td>BestEffort</td>
<td>放了configMap, env, command , hostAliases</td>
</tr>
<tr>
<td>miaaa-client-v4(大表哥前端正式环境)</td>
<td>mia</td>
<td>Deployment</td>
<td>3</td>
<td>无限制</td>
<td>BestEffort</td>
<td>没啥说的</td>
</tr>
<tr>
<td>logstash(大表哥日志搬运)</td>
<td>mia</td>
<td>Deployment</td>
<td>1</td>
<td>无限制</td>
<td>BestEffort</td>
<td>没啥说的</td>
</tr>
</tbody>
</table>
<blockquote>
<p>集群情况 </p>
</blockquote>
<p>单独做出一个集群, Ack托管, 无需管理Master<br>Jenkins的托管如何处理<br>镜像仓库的处理</p>
<h1 id="配置新用户及证书"><a href="#配置新用户及证书" class="headerlink" title="配置新用户及证书"></a>配置新用户及证书</h1><blockquote>
<p>进入目录</p>
</blockquote>
<p>cd /etc/kubernetes/pki</p>
<blockquote>
<p>生产密钥 .key 文件 (pki目录下执行)</p>
</blockquote>
<p>umask 077;openssl genrsa -out fanjj.key 2048<br>openssl req -new -key fanjj.key -out fanjj.csr -subj “/O=k8s/CN=fanjj”<br>O=组织信息，CN=用户名</p>
<blockquote>
<p>根据 .key 和 .csr 文件生成了 .crt 文件 (pki目录下执行)</p>
</blockquote>
<p>openssl  x509 -req -in fanjj.csr -CA ca.crt -CAkey ca.key \<br>-CAcreateserial -out fanjj.crt -days 365</p>
<blockquote>
<p>新建集群配置 (pki目录下执行)</p>
</blockquote>
<p>kubectl config set-cluster k8s –server=<a href="https://192.168.2.152:6443" target="_blank" rel="noopener">https://192.168.2.152:6443</a> \<br>–certificate-authority=ca.crt –embed-certs=true –kubeconfig=/root/fanjj.conf</p>
<blockquote>
<p>查看集群配置 (pki目录下执行)</p>
</blockquote>
<p>kubectl config view –kubeconfig=/root/fanjj.conf</p>
<blockquote>
<p>新建用户配置 (pki目录下执行)</p>
</blockquote>
<p>kubectl config set-credentials fanjj –client-certificate=fanjj.crt \<br>–client-key=fanjj.key –embed-certs=true –kubeconfig=/root/fanjj.conf</p>
<blockquote>
<p>新建上下文配置 (pki目录下执行)</p>
</blockquote>
<p>kubectl config set-context fanjj@k8s –cluster=k8s –user=fanjj \<br>–kubeconfig=/root/fanjj.conf</p>
<blockquote>
<p>切换上下文配置 (pki目录下执行)</p>
</blockquote>
<p>kubectl config use-context fanjj@k8s –kubeconfig=/root/fanjj.conf</p>
<blockquote>
<p>绑定角色 clusterrole可以自己建立一个</p>
</blockquote>
<p>kubectl create  clusterrolebinding fanjj –clusterrole=cluster-admin –user=fanjj</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST https://oapi.dingtalk.com/robot/send?access_token=c9d95fbd615adcabc8805204f35d21b8811cb5809a6b2397a60cf6b7bd742941 -H &quot;Content-Type: application/json&quot; -d &apos;&#123;msgtype:&quot;markdown&quot;,&quot;markdown&quot;:&#123;&quot;title&quot;:&quot;部署成功&quot;,&quot;text&quot;:&quot;## 部署成功\n - 构建人：沈伟-@15216821371\n - 工程：gateway\n - 稳定运行&quot;&#125;,&quot;at&quot;:&#123;&quot;atMobiles&quot;:[15216821371],&quot;isAtAll&quot;: false&#125;&#125;&apos;</span><br></pre></td></tr></table></figure>
<h1 id="Sealos-使用问题"><a href="#Sealos-使用问题" class="headerlink" title="Sealos 使用问题"></a>Sealos 使用问题</h1><blockquote>
<p>遇到Schedule或者Controller挂了</p>
</blockquote>
<p>请到master节点的/etc/kubernetes/manifests目录下, kube-scheduler.yaml/kube-controller-manager.yaml设置以下这两个参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- --leader-elect-lease-duration=15s</span><br><span class="line">- --leader-elect-renew-deadline=10s</span><br></pre></td></tr></table></figure>
<blockquote>
<p>LibC</p>
</blockquote>
<p><a href="https://ftp.gnu.org/gnu/libc/" target="_blank" rel="noopener">https://ftp.gnu.org/gnu/libc/</a></p>
<blockquote>
<p>编译内核</p>
</blockquote>
<p>镜像下载:<a href="https://cdn.kernel.org/pub/linux/kernel/v5.x/" target="_blank" rel="noopener">https://cdn.kernel.org/pub/linux/kernel/v5.x/</a><br>修复内核配置:<a href="https://unix.stackexchange.com/questions/680261/preparing-the-installer-to-update-linux-kernel-in-a-virtualbox-causes-errors-con" target="_blank" rel="noopener">https://unix.stackexchange.com/questions/680261/preparing-the-installer-to-update-linux-kernel-in-a-virtualbox-causes-errors-con</a><br>证书相关:<a href="https://blog.csdn.net/m0_47696151/article/details/121574718" target="_blank" rel="noopener">https://blog.csdn.net/m0_47696151/article/details/121574718</a></p>
<blockquote>
<p>Kafka命令大全</p>
</blockquote>
<p>创建一个主题：./bin/kafka-topics.sh –bootstrap-server localhost:9092 –create –replication-factor 1 –partitions 1 –topic test</p>
<p>查看所有主题：./bin/kafka-topics.sh –bootstrap-server localhost:9092 –list</p>
<p>查看某个主题的详细信息：./bin/kafka-topics.sh –bootstrap-server localhost:9092 –describe –topic dev_apiInfo_A_low</p>
<p>删除一个主题：./bin/kafka-topics.sh –bootstrap-server localhost:9092 –delete –topic dev_apiInfo_A_low</p>
<p>发送消息到一个主题：./bin/kafka-console-producer.sh –broker-list localhost:9092 –topic test</p>
<p>从一个主题消费消息：./bin/kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic test –from-beginning</p>
<p>在一个消费者组中消费消息：./bin/kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic test –group test-group</p>
<p>// 查看消费偏移量<br>./bin/kafka-consumer-groups.sh –bootstrap-server localhost:9092 –describe –group dataService_consumer<br>./bin/kafka-consumer-groups.sh –bootstrap-server localhost:9092 –describe –group proxyService_consumer</p>
<p>// 更改消费偏移量<br>./bin/kafka-consumer-groups.sh –bootstrap-server localhost:9092 –group dataService_consumer –reset-offsets –to-offset 165000 –topic dev_apiInfo_A_low  –execute<br>./bin/kafka-consumer-groups.sh –bootstrap-server localhost:9092 –group dataService_consumer –topic dev_raspUpload_A_low –reset-offsets –to-latest –execute</p>
<p>// 更改topic的分区数量<br>./bin/kafka-topics.sh –bootstrap-server localhost:9092 –alter –topic dev_flowUpload_A_low –partitions 3</p>
<p>1、DaemonAgent, 最开始产品覆盖面, DevOps构建SCA、开发测试Iast, 缺运行时环境的应用侧安全,朝着运行时的应用资产安全+Rasp插件做; 提取软件成分、Api等应用资产及外围安全也有Rasp防护, 于是DaemonAgent一季度造出搜集器和扫描器、搜集器参考Faclo(Sysdiag 小佑科技的参考商)、扫描器引用来源SCA客户端(用同一套分析器, 引出Scanner的Command命令解析器区别于WhaleFall, 结果包装输出SBOM和自带漏洞库); 这一套Q1上了之后没有做产品化等产品设计<br>Q2 Falco本地化规则引擎,本地化一是抄思想不抄代码,二是c++工程组员接受度、三是和Scanner融合考虑,开始推进Golang规则引擎; 期间也在等产品设计, 等到后来还是先往DaemonAgent堆功能,输出了技术的原理给产品, 造3.20;3.21这些系列</p>
<p>2、新架构, 分两期; 理论上优化代码为主,然后重点往横向扩容方向堆机器解决性能痛点<br>二期的时间跨度拉的是长, 拉长是任务的清晰度规划和估时模糊, 一些细节段做的时候不理方案, 评估380多个口子做<br>方案梳理落档没有这个习惯, 造成技术实现是一个个小孤岛, 协调的人要各个孤岛上来回跳</p>
<p>3、能力或者说认知上面, 做事情两个人一个小团队去推进, 对别的事情感知度不大的、做好自己份内事; 还是经常分享为主吧  </p>
<blockquote>
<p>切换内核 </p>
</blockquote>
<p>sudo vi /etc/default/grub<br>GRUB_DEFAULT=0<br>0代表第一项系统内核镜像</p>
<p>最后生成一下配置<br>sudo grub2-mkconfig -o /boot/grub2/grub.cfg</p>
<blockquote>
<p>升级gcc</p>
</blockquote>
<p>sudo yum install devtoolset-7-gcc*</p>
<blockquote>
<p>Bpf常用函数</p>
</blockquote>
<p>以下是一些常用的eBPF函数列表：</p>
<ul>
<li>bpf_map_lookup_elem: 在BPF Map中查找指定的元素，并返回其值。</li>
<li>bpf_map_update_elem: 向BPF Map中插入或更新指定的元素。</li>
<li>bpf_trace_printk: 输出调试信息到内核日志中，方便调试eBPF程序。</li>
<li>bpf_get_current_comm: 获取当前进程的名称，用于调试和监控。</li>
<li>bpf_get_smp_processor_id: 获取当前CPU的ID，用于实现CPU亲和性。</li>
<li>bpf_skb_load_bytes: 加载网络数据包中指定偏移量和长度的字节数据，用于网络包过滤和分析。</li>
<li>bpf_skb_store_bytes: 在网络数据包中指定偏移量处写入指定长度的字节数据，用于网络包修改。</li>
<li>bpf_perf_event_output: 将指定的数据写入性能事件流，用于实现性能分析和监控。</li>
<li>bpf_ktime_get_ns: 获取当前系统时间戳，用于实现时间相关的功能。</li>
<li>bpf_xdp_adjust_head: 调整网络数据包的头部偏移量，用于实现网络数据包修改和处理。</li>
<li>bpf_skb_get_tunnel_key: 获取网络数据包中的隧道键值，用于网络虚拟化和隧道协议处理。</li>
<li>bpf_getsockopt: 获取套接字选项的值，用于套接字编程和网络协议处理。</li>
<li>bpf_setsockopt: 设置套接字选项的值，用于套接字编程和网络协议处理。</li>
<li>bpf_clone_redirect: 创建一个进程副本，并重定向其网络流量，用于实现网络流量控制和隔离。</li>
<li>bpf_redirect: 重定向网络流量到指定的网络接口或套接字，用于实现网络流量控制和重定向。</li>
</ul>
<blockquote>
<p>增加k8s WebHook</p>
</blockquote>
<p>–audit-webhook-config-file=/etc/kubernetes/audit-webhook-config.yaml<br>–audit-webhook-batch-max-wait=1s<br>–audit-webhook-batch-max-size=100</p>
<blockquote>
<p>audit-webhook-config.yaml</p>
</blockquote>
<p>apiVersion: v1<br>clusters:</p>
<ul>
<li>cluster:<br>  server: <a href="http://192.168.2.174:9765/k8s-audit" target="_blank" rel="noopener">http://192.168.2.174:9765/k8s-audit</a><br>name: metric<br>contexts:</li>
<li>context:<br>  cluster: metric<br>  user: “”<br>name: default-context<br>current-context: default-context<br>kind: Config<br>preferences: {}<br>users: []</li>
</ul>
<blockquote>
<p>挂载设置 </p>
</blockquote>
<p>echo 1 &gt; /proc/sys/fs/may_detach_mounts</p>
<blockquote>
<p>windows设置静态路由</p>
</blockquote>
<p>route add 100.64.2.27 mask 255.255.255.255 192.168.2.172<br>route add 192.168.2.172 mask 255.255.255.255 192.168.66.1</p>
<p>route CHANGE 100.64.2.27 MASK 255.255.255.255 192.168.66.1   IF 16</p>
<blockquote>
<p>Linux设置静态路由</p>
</blockquote>
<p>sudo ip route add 10.244.1.5/32 via 192.168.2.x</p>
<p>端口转发：使用端口转发技术，将从开发机发出的数据包转发到 k8s 集群中的服务器。具体的操作方式可以使用工具如 socat、netcat、ssh 等实现，例如可以在开发机上执行以下命令：<br>$ socat tcp-listen:8080,fork tcp:192.168.2.172:80<br>该命令将在开发机的 8080 端口上启动一个 TCP 监听器，当有数据包到达时，将数据包转发到 k8s 集群中的服务器的 80 端口。</p>
<p>SSH 隧道：使用 SSH 隧道技术，将从开发机发出的数据包通过 SSH 连接隧道转发到 k8s 集群中的服务器。具体的操作方式可以使用工具如 ssh、autossh 等实现，例如可以在开发机上执行以下命令：<br>$ ssh -L 8080:192.168.2.172:80 <a href="mailto:user@192.168.2.172" target="_blank" rel="noopener">user@192.168.2.172</a><br>该命令将在开发机上启动一个 SSH 连接隧道，将本地的 8080 端口转发到 k8s 集群中的服务器的 80 端口。在隧道建立后，您可以在开发机上通过访问 <a href="http://localhost:8080" target="_blank" rel="noopener">http://localhost:8080</a> 来访问 k8s 集群中的 Pod。</p>
<blockquote>
<p>启动集群</p>
</blockquote>
<p>同步时间<br>yum -y install ntpdate<br>ntpdate ntp1.aliyun.com </p>
<p>astpos run localhost/labring/kubernetes-docker:v1.24.10-4.1.6-amd64 \<br>localhost/labring/flannel:v0.20.2-amd64 \<br>localhost/labring/helm:v3.8.2  –masters 192.168.2.177 –nodes 192.168.2.178,192.168.2.179 –passwd TcsecK8s@2023test</p>
<p>astpos run 192.168.2.171:5000/astp/infra:1.0.1 –env PGSQL_NODE=192.168.2.178 –env NFS_IP=192.168.2.178 –env WEB_NODE=192.168.2.179</p>
<p>astpos run localhost/kubernetes-docker:v1.24.10-4.1.6-amd64 localhost/labring/kube-ovn:v1.10.5 localhost/helm:v3.8.2 –masters=192.168.2.171 –nodes=192.168.2.172,192.168.2.173,192.168.2.174 -p TcsecK8s@2023nw</p>
<p>astpos run labring/kubernetes:v1.24.0 labring/calico:v3.22.1 labring/helm:v3.8.2 –masters=192.168.2.171 –nodes=192.168.2.172,192.168.2.173,192.168.2.174 -p TcsecK8s@2023nw</p>
<blockquote>
<p>es相关命令</p>
</blockquote>
<p>调整副本数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">curl -XPUT &apos;http://192.168.2.152:9200/test_index/_settings&apos; -d &apos;&#123;</span><br><span class="line">    &quot;index&quot;: &#123;</span><br><span class="line">       &quot;number_of_replicas&quot;: &quot;0&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure>
<p>调整分片数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">curl -XPUT &apos;http://localhost:9200/wwh_test2/&apos; -d &apos;&#123;</span><br><span class="line">    &quot;settings&quot; : &#123;</span><br><span class="line">        &quot;index&quot; : &#123;</span><br><span class="line">            &quot;number_of_shards&quot; : 2, </span><br><span class="line">            &quot;number_of_replicas&quot; : 2 </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST  -H &apos;Content-Type: application/json&apos;  &apos;http://100.64.0.5:9200/app_hole_log_list_v3-2023051118380989/_search&apos; -d &apos;&#123;&quot;query&quot;:&#123;&quot;bool&quot;:&#123;&quot;must&quot;:[&#123;&quot;match_all&quot;:&#123;&#125;&#125;],&quot;must_not&quot;:[],&quot;should&quot;:[]&#125;&#125;,&quot;from&quot;:0,&quot;size&quot;:10,&quot;sort&quot;:[],&quot;aggs&quot;:&#123;&#125;&#125;&apos;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>网络相关命令</p>
</blockquote>
<p>sudo route add default gw 172.19.66.2<br>sudo ifconfig ens33 172.19.66.190/24 up</p>
<blockquote>
<p>qume启动麒麟机器命令</p>
</blockquote>
<p>qemu-system-aarch64.exe -m 8192 -cpu cortex-a72 -smp 8,sockets=4,cores=2 -M virt -bios E:\Kylin\Kylin\QEMU_EFI.fd -device VGA -device nec-usb-xhci -device usb-mouse -device usb-kbd -drive if=none,file=E:\Kylin\Kylin\kylindisk.qcow2,id=hd0 -device virtio-blk-device,drive=hd0 -drive if=none,file=,id=cdrom,media=cdrom -device virtio-scsi-device -device scsi-cd,drive=cdrom -net nic -net user,hostfwd=tcp::2222-:22</p>
<p>astp/Tcsec@2019<br>root/Tcsec@1947</p>
<p>./python_loader –script xxx.py </p>
<blockquote>
<p>conda </p>
</blockquote>
<p>conda info -e<br>conda activate base</p>
<blockquote>
<p>k8s密码</p>
</blockquote>
<p>192.168.2.171<br>TcsecK8s@2022nw</p>
<p>192.168.2.179<br>TcsecK8s@2023test</p>
<p>192.168.2.152<br>TcsecK8s@2022</p>
<p>麒麟服务器<br>外网：192.168.2.205  tcsec /Ngcdc@2014<br>内网：192.168.101.151  tcsec /Ngcdc@2014   </p>
<h1 id="kube-ovn"><a href="#kube-ovn" class="headerlink" title="kube-ovn"></a>kube-ovn</h1><p>kubect get node<br>kubect get subnet   查看子网情况<br>kubect get ip  查看ip分配情况</p>
<p>kubectl ko nbctl show 查看北向数据库</p>
<h2 id="子网"><a href="#子网" class="headerlink" title="子网"></a>子网</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kubeovn.io/v1</span><br><span class="line">kind: Subnet</span><br><span class="line">metadata:</span><br><span class="line">  name: ovn-default</span><br><span class="line">spec:</span><br><span class="line">  cidrBlock: 10.16.0.0/16</span><br><span class="line">  default: false</span><br><span class="line">  excludeIps:</span><br><span class="line">  - 10.16.0.1</span><br><span class="line">  - 10.16.0.5..10.160.0.10</span><br><span class="line">  gateway: 10.16.0.1</span><br><span class="line">  gatewayType: distributed</span><br><span class="line">  namespaces:</span><br><span class="line">    - ns1</span><br><span class="line">  natOutgoing: true</span><br><span class="line">  private: false</span><br><span class="line">  protocol: IPv4</span><br></pre></td></tr></table></figure>
<p>kube-ovn的service是用了ovn的loadbalance<br>而不是用了kubeproxy的iptables</p>
<h2 id="网关"><a href="#网关" class="headerlink" title="网关"></a>网关</h2><p>查看路由表<br>kubectl ko nbctl lr-route-list ovn-cluster</p>
<p>编辑子网<br>kubect edit subnets.kubeovn.io subnet1</p>
<h1 id="Es"><a href="#Es" class="headerlink" title="Es"></a>Es</h1><ul>
<li>查看占用排序: curl -XGET “<a href="http://192.168.2.152:9200/_cat/indices?v&amp;s=store.size:desc&quot;" target="_blank" rel="noopener">http://192.168.2.152:9200/_cat/indices?v&amp;s=store.size:desc&quot;</a></li>
<li>删除索引: curl -X DELETE “<a href="http://192.168.2.152:9200/sw*&quot;" target="_blank" rel="noopener">http://192.168.2.152:9200/sw*&quot;</a></li>
</ul>
<h1 id="SCA"><a href="#SCA" class="headerlink" title="SCA"></a>SCA</h1><p><a href="https://192.168.2.140:10000/login" target="_blank" rel="noopener">https://192.168.2.140:10000/login</a><br><a href="https://192.168.2.78:10000/login" target="_blank" rel="noopener">https://192.168.2.78:10000/login</a></p>
<p>Tcsec Tcsec1947</p>
<h1 id="SAST"><a href="#SAST" class="headerlink" title="SAST"></a>SAST</h1><p>dataFlowEngine来源于shiftLeft的开源产品</p>
<p>antlr4另一个编译解释器、替代gcc和javac、语言支持</p>
<p>javasrc cpg的导出<br>jimple class的cpg soot解析类,生成cpg<br>semanticcpg 语法的cpg<br>x2cpg </p>
<p>fortify</p>
<h1 id="证书"><a href="#证书" class="headerlink" title="证书"></a>证书</h1><blockquote>
<p>查看证书到期时间</p>
</blockquote>
<p>openssl x509 -noout -in your_certificate.crt -dates</p>
<h1 id="Debug"><a href="#Debug" class="headerlink" title="Debug"></a>Debug</h1><p>kube-debug -container “88e50e289640” -debugport 38080 </p>
<p>kube-debug -localhost </p>
<p>kube-debug -node “192.168.1.13” -debugport 38081 </p>
<p>kube-debug -pod “astp-core-84d99f5478-ng8wh” -namespace “daily” -kubeconfig “/etc/kubernetes/admin.conf” -debugport 38082</p>
<p>kube-debug -pod “postgres-5979894b7c-b6qhz” -namespace “infra” -kubeconfig “/etc/kubernetes/admin.conf” -debugport 38082<br>./kube-debug -pod “postgres-5979894b7c-b6qhz” -namespace “infra” -kubeconfig “/etc/kubernetes/admin.conf” -debugport 28080</p>
<p>vmtool –action getInstances –className  java.util.concurrent.ConcurrentHashMap –limit 1000 –express ‘instances[*].size()’</p>

        
    </section>
</article>



<div class="comments">
    <div id="disqus_thread">
        <p class="comment-tips">国内查看评论需要代理~</p>
    </div>
    <script>
    window.disqus_config = function () {
        this.language = 'zh';
        this.page.url = 'http://www.coderss.cn/2021/06/15/k8s-devops/';
        this.page.title = 'Kubernetes-运维工作';
        this.page.identifier = '2021/06/15/k8s-devops/';
    };
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://name.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</div>
        <footer class="footer">
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, Theme by <a href="https://github.com/sanonz/hexo-theme-concise" target="_blank">Concise</a>
</footer>

<script type="text/javascript" src="//s13.cnzz.com/z_stat.php?id=1234567890&amp;web_id=1234567890"></script>


    </div>

    <script type="text/javascript" src="https://cdn.bootcss.com/jquery/1.9.0/jquery.min.js"></script>
    
    <script type="text/javascript" src="/js/scrollspy.min.js"></script>
    
    <script type="text/javascript">
        $(function() {
            var nodes = {
                nav: $('#nav'),
                aside: $('#aside'),
                navTags: $('#nav-tags')
            };

            $('#open-panel, #aside-mask').on('click', function() {
                nodes.aside.toggleClass('panel-show');
            });
            $('#nav-tag').on('click', function(event) {
                event.preventDefault();console.log(nodes.navTags.attr('class'))
                nodes.navTags.toggleClass('tag-show');console.log(nodes.navTags.attr('class'))
            })/*.hover(function() {
                nodes.navTags.addClass('tag-show');
            }, function() {
                nodes.navTags.removeClass('tag-show');
            });*/

            
            $(document.body).scrollspy({target: '#aside-inner'});
            
        });
    </script>

</body>
</html>
